{"config":{"lang":["es"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Seminario Microsoft Fabric","text":""},{"location":"#arquitectura-moderna-ia-generativa-y-casos-reales-antonio-soto","title":"Arquitectura moderna, IA generativa y casos reales \u2014 Antonio Soto","text":"<p>En este seminario aprender\u00e1s c\u00f3mo aplicar Microsoft Fabric para construir arquitecturas reales de datos y c\u00f3mo integrar IA generativa y agentes. Incluye ejemplos, casos de uso y mejores pr\u00e1cticas.</p>"},{"location":"#introduccion","title":"\ud83d\udccc Introducci\u00f3n","text":"<p>Microsoft Fabric es una plataforma unificada de datos y an\u00e1lisis que permite gestionar, transformar y analizar datos de forma eficiente. Este curso est\u00e1 dise\u00f1ado para proporcionar un enfoque pr\u00e1ctico en la implementaci\u00f3n de soluciones avanzadas con Lakehouse, Data Warehouse, Arquitectura Medall\u00f3n, Ciencia de Datos y An\u00e1lisis en Tiempo Real dentro de Microsoft Fabric.</p> <p>Durante el curso, se realizar\u00e1 un recorrido a trav\u00e9s de las diferentes caracter\u00edsticas y funcionalidades que nos proporcionar Microsoft Fabric. </p>"},{"location":"#agenda-del-curso-21-horas","title":"\ud83d\udcc5 Agenda del Curso (21 horas)","text":""},{"location":"#modulo-1-introduccion-a-microsoft-fabric","title":"\ud83d\udfe0 M\u00f3dulo 1: Introducci\u00f3n a Microsoft Fabric","text":"<p>\u2705 \u00bfQu\u00e9 es Microsoft Fabric? Diferencias con Power BI e integraci\u00f3n con otras herramientas. Que es Microsoft Fabric </p> <p>\u2705 El Concepto de OneLake: Arquitectura y ventajas de un Data Lake unificado.  El Concepto de Onelake</p> <p>\u2705 Escenarios End to End: Implementaci\u00f3n de cargas ETL, almacenamiento y an\u00e1lisis.Escenarios End-to-End en Microsoft Fabric</p>"},{"location":"#modulo-2-carga-de-datos-en-microsoft-fabric","title":"\ud83d\udfe0 M\u00f3dulo 2: Carga de Datos en Microsoft Fabric","text":"<p>\u2705 ETL con Flujos de Datos de Gen2: Creaci\u00f3n, transformaci\u00f3n y optimizaci\u00f3n de datos.  ELT con flujos de datos de Gen2</p> <p>\u2705 Orquestaci\u00f3n con Pipelines de Data Factory: Automatizaci\u00f3n de cargas y dependencias. Orquestando Cargas con Pipelines de Data Factory </p> <p>\u2705 Desarrollo con Apache Spark: Uso de Notebooks y PySpark para Ingenier\u00eda de Datos.Desarrollo con Apache Spark</p>"},{"location":"#modulo-3-ingenieria-de-datos-con-microsoft-fabric","title":"\ud83d\udfe0 M\u00f3dulo 3: Ingenier\u00eda de Datos con Microsoft Fabric","text":"<p>\u2705 Arquitectura Medall\u00f3n: Implementaci\u00f3n de un modelo Bronce-Plata-Oro en Fabric. Introducci\u00f3n a la Arquitectura Medall\u00f3n </p> <p>\u2705 Lakehouse en Fabric: Creaci\u00f3n y gesti\u00f3n de un Lakehouse, almacenamiento y consultas SQL. Creaci\u00f3n y Gesti\u00f3n de un lakehouse </p> <p>\u2705 Data Warehouses en Fabric: Optimizaci\u00f3n de modelos de datos para an\u00e1lisis eficientes.Trabajo con Datawarehouse</p>"},{"location":"#modulo-4-analisis-avanzado-y-automatizacion-en-fabric","title":"\ud83d\udfe0 M\u00f3dulo 4: An\u00e1lisis Avanzado y Automatizaci\u00f3n en Fabric","text":"<p>\u2705 Plataforma para Ciencia de Datos: Fabric como plataforma para el desarrollo de proyectos de Ciencia de Datos. Fabric como plataforma de Data Science </p> <p>\u2705 An\u00e1lisis en Tiempo Real: Anal\u00edtica de datos en streaming y KQL. An\u00e1lisis de Datos en Tiempo Real con Microsoft Fabric </p> <p>\u2705 Data Activator: Automatizaci\u00f3n de procesos basada en eventos e integraci\u00f3n con Power Automate.Introducci\u00f3n a Data Activator</p>"},{"location":"#modulo-5-fabric-databases","title":"\ud83d\udfe0 M\u00f3dulo 5: Fabric Databases","text":"<p>\u2705 Introducci\u00f3n a Fabric Databases: Datos relacionales en Microsoft Fabric Fabric Databases</p> <p>\u2705 Escenarios: Escenarios de utilizaci\u00f3n de Fabric Databases Escenarios de Fabric Databases</p>"},{"location":"#objetivos-del-curso","title":"\ud83c\udfaf Objetivos del Curso","text":"<p>\ud83d\udd39 Comprender la arquitectura y funcionalidades de Microsoft Fabric. \ud83d\udd39 Implementar ETL con Dataflows Gen2 y Pipelines de Data Factory. \ud83d\udd39 Construir soluciones de datos usando Arquitectura Medall\u00f3n, Lakehouse y Data Warehouse. \ud83d\udd39 Aplicar ciencia de datos con PySpark ** en Fabric. \ud83d\udd39 Analizar datos en tiempo real con EventStream y Power BI. \ud83d\udd39 Automatizar procesos con Data Activator e integraci\u00f3n con Power Automate**.</p> <p>Este curso es ideal para ingenieros de datos, analistas y cient\u00edficos de datos que desean aprender a gestionar, transformar y analizar datos en Fabric de manera eficiente. \ud83d\ude80</p>"},{"location":"01%20Introduccion%20a%20Microsoft%20Fabric/01-1%20Que%20es%20Microsoft%20Fabric/","title":"\u00bfQu\u00e9 es Microsoft Fabric?","text":""},{"location":"01%20Introduccion%20a%20Microsoft%20Fabric/01-1%20Que%20es%20Microsoft%20Fabric/#1-introduccion-a-microsoft-fabric","title":"1. Introducci\u00f3n a Microsoft Fabric","text":""},{"location":"01%20Introduccion%20a%20Microsoft%20Fabric/01-1%20Que%20es%20Microsoft%20Fabric/#que-es-microsoft-fabric_1","title":"\ud83d\udd39 \u00bfQu\u00e9 es Microsoft Fabric?","text":"<p>Microsoft Fabric es una plataforma unificada de datos y an\u00e1lisis que permite a las organizaciones integrar, procesar y analizar datos en un entorno centralizado. Dise\u00f1ada como una soluci\u00f3n SaaS (Software as a Service), Fabric simplifica la gesti\u00f3n de datos eliminando la necesidad de combinar m\u00faltiples servicios y herramientas.</p> <p></p> <p>Microsoft Fabric ofrece un conjunto de servicios que incluyen:</p> <ul> <li>Data Engineering: Procesamiento de datos a gran escala con Spark.</li> <li>Data Factory: Orquestaci\u00f3n de cargas de datos con Dataflows Gen2 y Pipelines.</li> <li>Data Science: Creaci\u00f3n y despliegue de modelos de Machine Learning con integraci\u00f3n en Azure AI.</li> <li>Data Warehouse: Un servicio de almacenamiento y an\u00e1lisis optimizado para SQL.</li> <li>Real-Time Analytics: Procesamiento de eventos en tiempo real.</li> <li>Power BI: Visualizaci\u00f3n y exploraci\u00f3n de datos con modelos sem\u00e1nticos.</li> </ul> <p>\ud83d\udca1 Diferencia clave: Mientras que otras soluciones requieren ensamblar m\u00faltiples servicios de diferentes proveedores, Microsoft Fabric proporciona un ecosistema completo y unificado bajo una \u00fanica interfaz.</p>"},{"location":"01%20Introduccion%20a%20Microsoft%20Fabric/01-1%20Que%20es%20Microsoft%20Fabric/#principales-ventajas-de-microsoft-fabric","title":"\ud83d\udd39 Principales ventajas de Microsoft Fabric","text":"<p>\u2705 Unificaci\u00f3n de almacenamiento con OneLake: Todos los datos en un solo lago de datos, sin necesidad de replicarlos en diferentes sistemas. \u2705 Simplicidad operativa: Eliminaci\u00f3n de la complejidad de integraci\u00f3n entre herramientas y plataformas. \u2705 Capacidad de an\u00e1lisis en tiempo real: Permite tomar decisiones informadas con datos en streaming. \u2705 Inteligencia Artificial integrada: Fabric cuenta con capacidades de Copilot para asistir a los usuarios en la exploraci\u00f3n y an\u00e1lisis de datos.</p>"},{"location":"01%20Introduccion%20a%20Microsoft%20Fabric/01-1%20Que%20es%20Microsoft%20Fabric/#2-comparacion-entre-microsoft-fabric-y-power-bi-cuando-usar-cada-uno","title":"2. Comparaci\u00f3n entre Microsoft Fabric y Power BI: \u00bfCu\u00e1ndo usar cada uno?","text":"<p>Tanto Microsoft Fabric como Power BI forman parte del ecosistema de datos de Microsoft, pero tienen prop\u00f3sitos distintos.</p> Caracter\u00edstica Microsoft Fabric Power BI Prop\u00f3sito Plataforma completa de datos que abarca ingesta, almacenamiento, transformaci\u00f3n, modelado y visualizaci\u00f3n. Herramienta de Business Intelligence (BI) centrada en modelado y visualizaci\u00f3n. Almacenamiento Utiliza OneLake, un lago de datos unificado basado en Azure Data Lake Storage Gen2. Depende de datasets importados o conexiones en vivo. Procesamiento Soporta ETL, Machine Learning, Streaming, Data Warehousing y m\u00e1s. Se enfoca en modelado de datos y visualizaci\u00f3n. Usuarios clave Ingenieros de datos, cient\u00edficos de datos, arquitectos de soluciones. Analistas de negocio, directivos, usuarios de negocio. Integraci\u00f3n con Fabric Nativo, todos los datos est\u00e1n en OneLake y pueden ser usados sin movimiento de datos. Consume datos de Fabric pero depende de datasets creados externamente. <p>\ud83d\udccc \u00bfCu\u00e1ndo usar cada uno? \u2705 Si se necesita una soluci\u00f3n completa de datos, desde ingesta hasta visualizaci\u00f3n \u2192 Microsoft Fabric. \u2705 Si solo se requiere an\u00e1lisis y visualizaci\u00f3n de datos sin procesamiento complejo \u2192 Power BI. \u2705 Si ya se usa Power BI y se quiere integrar con una plataforma m\u00e1s robusta, Fabric proporciona un backend potente para centralizar los datos y los procesos de ingenier\u00eda y ciencia de datos necesarios.</p>"},{"location":"01%20Introduccion%20a%20Microsoft%20Fabric/01-1%20Que%20es%20Microsoft%20Fabric/#3-integracion-de-microsoft-fabric-con-otras-herramientas-de-microsoft","title":"3. Integraci\u00f3n de Microsoft Fabric con otras herramientas de Microsoft","text":"<p>Microsoft Fabric est\u00e1 dise\u00f1ado para trabajar de manera nativa con el ecosistema de Microsoft, incluyendo:</p>"},{"location":"01%20Introduccion%20a%20Microsoft%20Fabric/01-1%20Que%20es%20Microsoft%20Fabric/#onelake-y-azure-data-services","title":"\ud83d\udd39 OneLake y Azure Data Services","text":"<ul> <li>OneLake es el lago de datos unificado que elimina la fragmentaci\u00f3n de datos en la organizaci\u00f3n.</li> <li>Se integra con Azure Data Factory y Azure Synapse Analytics, facilitando la migraci\u00f3n desde estos servicios hacia Fabric.</li> </ul>"},{"location":"01%20Introduccion%20a%20Microsoft%20Fabric/01-1%20Que%20es%20Microsoft%20Fabric/#power-bi-y-microsoft-fabric","title":"\ud83d\udd39 Power BI y Microsoft Fabric","text":"<ul> <li>Power BI consume directamente los datos de Fabric sin necesidad de duplicaciones.</li> <li>Los modelos sem\u00e1nticos en Power BI pueden basarse en Data Warehouses y Lakehouses dentro de Fabric.</li> </ul>"},{"location":"01%20Introduccion%20a%20Microsoft%20Fabric/01-1%20Que%20es%20Microsoft%20Fabric/#inteligencia-artificial-y-machine-learning","title":"\ud83d\udd39 Inteligencia Artificial y Machine Learning","text":"<ul> <li>Azure AI permite entrenar y desplegar modelos de IA sobre los datos almacenados en Fabric.</li> <li>Soporte para Spark y MLlib dentro de los Notebooks de Fabric.</li> </ul>"},{"location":"01%20Introduccion%20a%20Microsoft%20Fabric/01-1%20Que%20es%20Microsoft%20Fabric/#orquestacion-de-datos","title":"\ud83d\udd39 Orquestaci\u00f3n de Datos","text":"<ul> <li>Dataflows Gen2 y Pipelines permiten el dise\u00f1o de ETL de manera eficiente.</li> <li>Fabric soporta conectores para Snowflake, Google BigQuery y Databricks, facilitando la integraci\u00f3n con otras plataformas.</li> </ul>"},{"location":"01%20Introduccion%20a%20Microsoft%20Fabric/01-1%20Que%20es%20Microsoft%20Fabric/#4-licenciamiento-y-unidades-de-capacidad-en-microsoft-fabric","title":"4. Licenciamiento y Unidades de Capacidad en Microsoft Fabric","text":"<p>Microsoft Fabric se ofrece bajo un modelo de Capacidad (Capacity-based licensing). Esto significa que los recursos est\u00e1n disponibles en funci\u00f3n del nivel de capacidad contratado.</p>"},{"location":"01%20Introduccion%20a%20Microsoft%20Fabric/01-1%20Que%20es%20Microsoft%20Fabric/#capacidades-y-skus-disponibles","title":"\ud83d\udd39 Capacidades y SKUs disponibles","text":"<p>Fabric utiliza unidades de capacidad (CU), que definen la cantidad de recursos disponibles en una suscripci\u00f3n.</p> Capacidad Unidades de Capacidad (CU) Casos de Uso F2 2 CU Pruebas y peque\u00f1os proyectos. F4 4 CU Modelado de datos y cargas de trabajo ligeras. F8 8 CU Proyectos medianos con ETL y an\u00e1lisis en Power BI. F16 16 CU Workloads m\u00e1s exigentes con integraci\u00f3n de ML y big data. F32 y superiores 32 CU o m\u00e1s Implementaciones a gran escala con cargas de trabajo distribuidas. <p>\ud83d\udccc Facturaci\u00f3n basada en Capacidad</p> <ul> <li>La capacidad se crea a nivel de tenant en Entra ID (anteriormente Azure AD).</li> <li>Se puede compartir entre varios workspaces dentro del mismo tenant</li> <li>Podemos tener tantas capacidades como queramos, asignadas a diferentes espacios de trabajo</li> <li>Los precios var\u00edan seg\u00fan el nivel de capacidad y el consumo de recursos.</li> </ul> <p>Existen algunas funcionalidades, como las de Copilot, que solo est\u00e1n disponibles a partir de la F64. A partir de esta SKU F64, no es necesario disponer de licenciamiento de Power BI para poder visualizar informes, solo los usuarios que necesiten publicar contenido, necesitar\u00edan licenciamiento de Power BI Pro</p> <p></p> <pre><code>                *Precios a fecha 10 de Febrero de 2025*\n</code></pre> <p>Realmente el coste se establece por CU, que es la unidad m\u00ednima que se puede reservar, y pueden reservarse unidades impares y el coste se establece a d\u00eda de hoy (Febrero 2025) en 150,33\u20ac por CU, sin reserva y para todo el mes. Con este c\u00e1lculo, podemos tambi\u00e9n saber cuando nos costar\u00eda cada CU por hora, tomando unas 720 horas mensuales, tendr\u00edamos un coste hora de CU de 0,21\u20ac/hora</p>"},{"location":"01%20Introduccion%20a%20Microsoft%20Fabric/01-1%20Que%20es%20Microsoft%20Fabric/#fabric-trial-evaluacion-gratuita","title":"\ud83d\udd39 Fabric Trial: Evaluaci\u00f3n Gratuita","text":"<p>Microsoft ofrece una versi\u00f3n de prueba gratuita de Fabric con 64 CU durante 60 d\u00edas. Esta trial permite experimentar con todas las funcionalidades antes de adquirir una suscripci\u00f3n, y nos permite hacer las pruebas necesarias, para ver que capacidad es la que necesitamos realmente. </p> <p>\ud83d\udca1 \u00bfC\u00f3mo elegir la capacidad correcta? - Si tu empresa solo usa Fabric para Power BI, una capacidad baja (F2-F4) puede ser suficiente. - Si necesitas procesamiento de grandes vol\u00famenes de datos, se recomienda F16 en adelante. - Para an\u00e1lisis en tiempo real y machine learning, es mejor optar por F32 o superior.</p>"},{"location":"01%20Introduccion%20a%20Microsoft%20Fabric/01-1%20Que%20es%20Microsoft%20Fabric/#pero-entonces-que-licenciamos","title":"\ud83d\udd39Pero entonces, \u00bfQu\u00e9 licenciamos?","text":"<p>Realmente, es la intersecci\u00f3n entre el espacio de trabajo y el usuario, es decir, un usuario sin licenciamiento de Power BI Pro, podr\u00eda, por ejemplo, visualizar informes que estuviesen publicados en un espacio de trabajo que tiene asignada una capacidad F64 o superior, pero no podr\u00eda acceder a informes que estuviesen desplegados en espacios de trabajo con capacidades inferiores o con licenciamiento Power BI Pro.  Es importante mencionar tambi\u00e9n, que los costes mostrados, son de pago por uso, sin ning\u00fan tipo de compromiso, es decir, podemos parar e iniciar la capacidad de Fabric, y pagar \u00fanicamente por el tiempo en el que est\u00e9 activa. El coste mostrado, es el que tendr\u00edan esas capacidades en caso de que estuviesen iniciadas continuamente. Si optamos por tener un descuento por compromiso de uno o tres a\u00f1os, entonces se nos cobrar\u00e1 el importe acordado al compromiso, independientemente de que est\u00e9 la capacidad iniciada o pausada. </p> <p>El detalle de los costes podemos verlo en este enlace https://azure.microsoft.com/es-es/pricing/details/microsoft-fabric/ </p>"},{"location":"01%20Introduccion%20a%20Microsoft%20Fabric/01-1%20Que%20es%20Microsoft%20Fabric/#capacidades-y-consumos","title":"\ud83d\udd39 Capacidades y Consumos","text":""},{"location":"01%20Introduccion%20a%20Microsoft%20Fabric/01-1%20Que%20es%20Microsoft%20Fabric/#que-es-una-unidad-de-capacidad-cu","title":"\u00bfQu\u00e9 es una Unidad de Capacidad (CU)?","text":"<p>En Microsoft Fabric, una Unidad de Capacidad (CU) es la medida que define la cantidad de recursos de c\u00f3mputo y almacenamiento asignados a una capacidad espec\u00edfica. Estas unidades determinan el rendimiento y la eficiencia de las cargas de trabajo que se ejecutan en la plataforma. Las capacidades se ofrecen en diferentes tama\u00f1os o SKU, que van desde F2 (2 CUs) hasta F2048 (2048 CUs), permitiendo a las organizaciones seleccionar la capacidad que mejor se adapte a sus necesidades operativas.</p>"},{"location":"01%20Introduccion%20a%20Microsoft%20Fabric/01-1%20Que%20es%20Microsoft%20Fabric/#gestion-dinamica-de-recursos-bursting-y-smoothing","title":"Gesti\u00f3n Din\u00e1mica de Recursos: Bursting y Smoothing","text":"<p>Para optimizar el uso de los recursos y manejar eficientemente las fluctuaciones en la demanda, Microsoft Fabric implementa dos mecanismos clave: Bursting y Smoothing.</p> <p>El Bursting permite que una capacidad consuma m\u00e1s CUs de las asignadas temporalmente para completar tareas que requieren un alto rendimiento en menos tiempo. Por ejemplo, si una capacidad F2 tiene 2 CUs asignadas, pero una carga de trabajo intensa necesita m\u00e1s recursos, el sistema puede proporcionar capacidad adicional moment\u00e1neamente para acelerar la ejecuci\u00f3n de la tarea. Este enfoque es especialmente \u00fatil para manejar picos de demanda sin necesidad de escalar permanentemente la capacidad contratada.</p> <p>El Smoothing se encarga de equilibrar el consumo de CUs a lo largo del tiempo, distribuyendo el uso de los recursos para evitar picos y optimizar la eficiencia. Despu\u00e9s de un per\u00edodo de bursting, el sistema \"suaviza\" el consumo adicional distribuy\u00e9ndolo en intervalos de tiempo espec\u00edficos: para operaciones interactivas, el consumo se promedia en un m\u00ednimo de 5 minutos, mientras que para trabajos en segundo plano o programados, se extiende hasta 24 horas. Este mecanismo permite que las organizaciones planifiquen su capacidad en funci\u00f3n del uso promedio en lugar del m\u00e1ximo, optimizando as\u00ed los costos y recursos.</p> <p>Es importante tener en cuenta que, aunque el bursting y smoothing ofrecen flexibilidad, existen l\u00edmites definidos para evitar el uso excesivo de recursos. Si una capacidad excede continuamente su rendimiento asignado, se aplicar\u00e1n pol\u00edticas de regulaci\u00f3n o \"throttling\", donde las nuevas tareas no se ejecutar\u00e1n hasta que haya suficientes recursos disponibles. Adem\u00e1s, las capacidades pueden pausarse y reanudarse seg\u00fan sea necesario, y se facturan por segundo, lo que permite un control m\u00e1s preciso de los costos operativos.</p> <p>Fabric est\u00e1 dise\u00f1ado para ofrecer un rendimiento \u00f3ptimo, permitiendo que las operaciones utilicen m\u00e1s CU de las asignadas temporalmente. Para gestionar este comportamiento, se implementa un proceso de suavizado que promedia el uso de CU en intervalos de tiempo espec\u00edficos:  - Operaciones Interactivas: El consumo de CU se suaviza durante un m\u00ednimo de 5 minutos, permitiendo manejar picos temporales sin aplicar limitaciones inmediatas.  - Operaciones en Segundo Plano: Para tareas de larga duraci\u00f3n, el suavizado se extiende hasta 24 horas, facilitando la ejecuci\u00f3n de trabajos programados sin preocuparse por picos de carga.</p> <p>La limitaci\u00f3n se aplica en fases, dependiendo del nivel de consumo de CU:  1. Alerta de Consumo Completo: Cuando una capacidad consume el 100% de sus CU aprovisionadas, se env\u00eda una alerta al administrador, indicando que podr\u00eda ser necesario ajustar la capacidad.  2. Primera Fase de Limitaci\u00f3n: Si el consumo excede las CU disponibles para los pr\u00f3ximos 10 minutos, se introduce un retraso de 20 segundos en las nuevas operaciones interactivas.  3. Segunda Fase de Limitaci\u00f3n: Si la deuda de CU alcanza una hora, las solicitudes interactivas adicionales son rechazadas, pero las operaciones en segundo plano contin\u00faan ejecut\u00e1ndose.  4. Fase Cr\u00edtica de Limitaci\u00f3n: Si la deuda de CU llega a 24 horas, todas las operaciones, incluidas las de segundo plano, son rechazadas hasta que la deuda se reduzca.</p> <p>Para recuperar una capacidad que ha entrado en un estado de limitaci\u00f3n severa, podemos tomar las siguientes acciones:  - Esperar: Permitir que la capacidad se recupere naturalmente a medida que la deuda de CU se reduce con el tiempo.  - Ajustar la SKU: Incrementar temporalmente el tama\u00f1o de la SKU de la capacidad para proporcionar m\u00e1s recursos y acelerar la recuperaci\u00f3n.  - Reasignar \u00c1reas de Trabajo: Mover \u00e1reas de trabajo de menor prioridad o que consumen muchos recursos a otras capacidades para equilibrar la carga.</p> <p>Para monitorizar los consumos y ver el detalle, disponemos de una aplicaci\u00f3n, llamada Microsoft Fabric Capacity Metrics https://learn.microsoft.com/es-es/fabric/enterprise/metrics-app </p> <p></p>"},{"location":"01%20Introduccion%20a%20Microsoft%20Fabric/01-1%20Que%20es%20Microsoft%20Fabric/#5-resumen-de-capacidades","title":"5. Resumen de Capacidades","text":"<p>A continuaci\u00f3n, ver\u00e1s una tabla resumen de lo que proporciona cada una de las capacidades de Fabric en cuanto a recursos</p> Capacidad CU Memoria M\u00e1xima Power BI v-cores Spark vCores Max Spark vCores con Burst Factor Limite de Cola Tareas Dataflow Gen2 en paralelo Refresco del modelo en paralelo F2 2 3 0.25 4 20 4 96 1 F4 4 3 0.5 8 24 4 96 2 F8 8 3 1 16 48 8 96 5 F16 16 5 2 32 96 16 96 10 F32 32 10 4 64 192 32 96 20 F64 64 25 8 128 384 64 384 40 F128 128 50 16 256 768 128 384 80 F256 256 100 32 512 1536 256 384 160 F512 512 200 64 1024 3072 512 1536 320 F1024 1024 400 128 2048 6144 1024 1536 640 F2048 2048 400 4096 12288 2048 1536 1280 <p>El detalle completo puedes revisarlo en este enlace https://learn.microsoft.com/en-us/power-bi/enterprise/service-premium-what-is#capacities-and-skus</p>"},{"location":"01%20Introduccion%20a%20Microsoft%20Fabric/01-1%20Que%20es%20Microsoft%20Fabric/#6-creacion-del-recurso-fabric","title":"6. Creaci\u00f3n del recurso Fabric","text":"<p>Tal y como hemos comentado, para crear una capacidad de Fabric, es necesario crear un recurso de Fabric en una suscripci\u00f3n de Azure. A la hora de crear el recurso, debemos de especificar las propiedades que se muestran en la siguiente imagen:</p> <p></p> <p>En la actualidad, Fabric soporte el concepto de multiregi\u00f3n, por lo que no tenemos por qu\u00e9 crear nuestra capacidad en la misma regi\u00f3n en la que est\u00e9 nuestro tenant (que ser\u00e1 donde est\u00e9n los recursos de Power BI y nuestro Onelake) , pero eso puede tener implicaciones de rendimiento, por lo que no es aconsejable. Las capacidades pueden escalar de tama\u00f1o en cualquier momento, y podemos agregar m\u00e1s administradores en caso de que lo necesitemos. </p>"},{"location":"01%20Introduccion%20a%20Microsoft%20Fabric/01-1%20Que%20es%20Microsoft%20Fabric/#asignar-la-capacidad-al-espacio-de-trabajo","title":"\ud83d\udd39 Asignar la capacidad al espacio de trabajo","text":"<p>Una vez que tenemos la capacidad creada, e iniciada, podemos asignar esa capacidad a nuestros espacios de trabajo, tanto en el momento de su creaci\u00f3n, como una vez est\u00e9n creados desde la configuraci\u00f3n del espacio de trabajo. Para ello es necesario que el usuario que est\u00e1 configurando el espacio de trabajo, tenga permisos sobre esa capacidad para poder asignarla.</p> <p></p> <p>Del mismo modo, si tenemos permisos de administraci\u00f3n, desde la administraci\u00f3n del portal, podemos ver las capacidades que tenemos desplegadas en nuestro tenant, y los permisos que tenemos sobre ellas, as\u00ed como los espacios de trabajo que est\u00e1n utilizando esas capacidades. </p>"},{"location":"01%20Introduccion%20a%20Microsoft%20Fabric/01-1%20Que%20es%20Microsoft%20Fabric/#pausar-e-iniciar-la-capacidad-de-fabric","title":"\ud83d\udd39 Pausar e iniciar la capacidad de Fabric","text":"<p>Tal y como hemos comentado, sino hemos adquirido la capacidad de Fabric con un compromiso de utilizaci\u00f3n, solo pagaremos por ella cuando la instancia est\u00e9 iniciada. Obviamente, cuando una capacidad de Fabric est\u00e1 pausada, no es posible acceder a los objetos de Fabric que est\u00e9 sirviendo esa capacidad en los espacios de trabajo en los que est\u00e9 asignada, ni siquiera a lo almacenado en Onelake. </p> <p>Pausar e iniciar la capacidad es una tarea que debemos de realizar en el portal de Azure, como se muestra en la imagen</p> <p></p> <p>No tenemos una forma \"directa\" de automatizar el pausado e inicio de la capacidad, pero es posible realizarlo de m\u00faltiples maneras, con un script a trav\u00e9s del API, utilizando Power Automate, o directamente con recursos de Automatizaci\u00f3n de Azure, dependiendo de con que soluci\u00f3n nos encontremos m\u00e1s c\u00f3modos.  </p>"},{"location":"01%20Introduccion%20a%20Microsoft%20Fabric/01-1%20Que%20es%20Microsoft%20Fabric/#7-conclusion-y-preguntas-clave","title":"7. Conclusi\u00f3n y Preguntas Clave","text":""},{"location":"01%20Introduccion%20a%20Microsoft%20Fabric/01-1%20Que%20es%20Microsoft%20Fabric/#resumen-de-conceptos-clave","title":"\ud83d\udd39 Resumen de conceptos clave","text":"<p>\u2705 Microsoft Fabric es una plataforma unificada para gesti\u00f3n de datos y anal\u00edtica avanzada. \u2705 Ofrece capacidades de almacenamiento, procesamiento y visualizaci\u00f3n en un solo entorno. \u2705 Power BI y Microsoft Fabric est\u00e1n integrados, pero Fabric es una soluci\u00f3n m\u00e1s completa. \u2705 El modelo de licenciamiento se basa en unidades de capacidad (CU) con diferentes SKUs disponibles.</p>"},{"location":"01%20Introduccion%20a%20Microsoft%20Fabric/01-1%20Que%20es%20Microsoft%20Fabric/#preguntas-para-reflexionar-y-discusion","title":"\ud83d\udd39 Preguntas para reflexionar y discusi\u00f3n","text":"<ol> <li>\u00bfC\u00f3mo Fabric puede ayudar a mi organizaci\u00f3n a reducir los silos de datos?</li> <li>\u00bfQu\u00e9 diferencias clave encuentro entre Fabric y Power BI en mi caso de uso?</li> <li>\u00bfCu\u00e1l es la capacidad ideal para mi empresa en funci\u00f3n del volumen de datos?</li> </ol>"},{"location":"01%20Introduccion%20a%20Microsoft%20Fabric/01-2%20El%20Concepto%20de%20Onelake/","title":"01 2 El Concepto de Onelake","text":""},{"location":"01%20Introduccion%20a%20Microsoft%20Fabric/01-2%20El%20Concepto%20de%20Onelake/#1-introduccion-a-onelake-el-data-lake-unificado-de-microsoft-fabric","title":"1. Introducci\u00f3n a OneLake: El Data Lake Unificado de Microsoft Fabric","text":""},{"location":"01%20Introduccion%20a%20Microsoft%20Fabric/01-2%20El%20Concepto%20de%20Onelake/#que-es-onelake","title":"\ud83d\udd39 \u00bfQu\u00e9 es OneLake?","text":"<p>OneLake es el lago de datos unificado de Microsoft Fabric, dise\u00f1ado para centralizar la gesti\u00f3n de datos en toda la organizaci\u00f3n. Funciona como un repositorio \u00fanico, eliminando los silos de datos y simplificando la administraci\u00f3n y el acceso a los datos.</p> <p>![[Onelake.png]]</p> <p>Caracter\u00edsticas clave de OneLake: \u2705 Almacenamiento centralizado: Todos los datos en un solo lugar, eliminando la duplicaci\u00f3n de informaci\u00f3n. \u2705 Compatibilidad con m\u00faltiples motores: Integraci\u00f3n con Spark, SQL, Power BI y Data Factory. \u2705 Formato abierto Delta Lake: Almacenamiento basado en Delta Lake, lo que permite transacciones ACID y optimizaci\u00f3n del rendimiento. \u2705 Capacidad de uso compartido: Permite que m\u00faltiples equipos accedan a los mismos datos sin necesidad de copiarlos. \u2705 Gesti\u00f3n sencilla con Shortcuts: Los datos pueden estar distribuidos en m\u00faltiples ubicaciones, pero OneLake los expone como si fueran locales.</p>"},{"location":"01%20Introduccion%20a%20Microsoft%20Fabric/01-2%20El%20Concepto%20de%20Onelake/#2-arquitectura-de-onelake-diferencias-con-los-data-lakes-tradicionales","title":"2. Arquitectura de OneLake: Diferencias con los Data Lakes Tradicionales","text":"<p>OneLake introduce una arquitectura innovadora que mejora las limitaciones de los Data Lakes tradicionales.</p>"},{"location":"01%20Introduccion%20a%20Microsoft%20Fabric/01-2%20El%20Concepto%20de%20Onelake/#comparacion-entre-onelake-y-los-data-lakes-tradicionales","title":"\ud83d\udd39 Comparaci\u00f3n entre OneLake y los Data Lakes Tradicionales","text":"Caracter\u00edstica OneLake (Microsoft Fabric) Data Lakes Tradicionales Unificaci\u00f3n de datos Almacena todos los datos en un solo repositorio con acceso global. Cada equipo o departamento suele tener su propio almacenamiento. Formato de almacenamiento Basado en Delta Lake para transacciones ACID y optimizaci\u00f3n. Generalmente almacena datos en formatos como Parquet, ORC o Avro, sin soporte nativo para transacciones. Gesti\u00f3n y Gobernanza Integrado con Microsoft Purview para gobierno y cumplimiento de datos. Se requiere configuraci\u00f3n manual de seguridad y acceso. Integraci\u00f3n con Microsoft Fabric Integrado de forma nativa con Data Factory, Data Science, Power BI y Real-Time Analytics. Normalmente requiere conectores o procesos ETL adicionales. Estrategia Multicloud Permite integrar datos desde otras nubes como AWS y Google Cloud usando Shortcuts. Generalmente aislado dentro de un solo proveedor de nube. Gesti\u00f3n de costos Optimizaci\u00f3n autom\u00e1tica de almacenamiento y procesamiento con fabric capacity units (CU). Costos variables seg\u00fan el uso de almacenamiento y c\u00f3mputo sin una estrategia unificada."},{"location":"01%20Introduccion%20a%20Microsoft%20Fabric/01-2%20El%20Concepto%20de%20Onelake/#arquitectura-de-onelake","title":"\ud83d\udd39 Arquitectura de OneLake","text":"<p>OneLake est\u00e1 basado en tres conceptos clave:</p> <p>1\ufe0f\u20e3 Tenant \u00fanico: OneLake se implementa a nivel de tenant en Microsoft Entra ID (Azure AD), proporcionando un \u00fanico punto de acceso a los datos. 2\ufe0f\u20e3 Workspaces: Los datos se organizan en Workspaces, que representan \u00e1reas de trabajo separadas para distintos equipos y proyectos. 3\ufe0f\u20e3 Shortcuts: Permiten acceder a datos en otras ubicaciones sin copiarlos, facilitando el an\u00e1lisis distribuido y reduciendo costos de almacenamiento.</p> <p>\ud83d\udccc Ventaja clave: OneLake act\u00faa como el OneDrive para datos, donde cada equipo puede crear su propio Lakehouse o Data Warehouse sin preocuparse por la infraestructura. </p>"},{"location":"01%20Introduccion%20a%20Microsoft%20Fabric/01-2%20El%20Concepto%20de%20Onelake/#3-seguridad-gobernanza-y-control-de-acceso-en-onelake","title":"3. Seguridad, Gobernanza y Control de Acceso en OneLake","text":""},{"location":"01%20Introduccion%20a%20Microsoft%20Fabric/01-2%20El%20Concepto%20de%20Onelake/#seguridad-y-proteccion-de-datos-en-onelake","title":"\ud83d\udd39 Seguridad y Protecci\u00f3n de Datos en OneLake","text":"<p>OneLake hereda los controles de seguridad de Microsoft Entra ID (Azure AD) y Microsoft Purview, asegurando que los datos est\u00e9n protegidos y cumplan con las regulaciones empresariales.</p> <p>\u26a1 Principales medidas de seguridad: \u2705 Autenticaci\u00f3n y control de acceso basado en roles (RBAC): Permite definir permisos a nivel de usuario, grupo o aplicaci\u00f3n. \u2705 Etiquetado de Sensibilidad: Integraci\u00f3n con Microsoft Purview para clasificar datos confidenciales y aplicar restricciones. \u2705 Cifrado en reposo y en tr\u00e1nsito: Uso de Azure Key Vault para gestionar claves de cifrado y proteger la informaci\u00f3n. \u2705 Data Masking y Row-Level Security (RLS): Controla qu\u00e9 usuarios pueden ver datos espec\u00edficos dentro de un mismo conjunto de datos.</p> <p>\ud83d\udccc Caso de uso real: Una empresa de retail puede almacenar datos de clientes en OneLake y usar Row-Level Security para que solo los empleados de cada pa\u00eds accedan a la informaci\u00f3n de sus respectivas regiones.</p> <p>Algunos de estos mecanismos de seguridad, todav\u00eda est\u00e1n en preview, puedes ver los detalles aqu\u00ed: https://learn.microsoft.com/en-us/fabric/onelake/security/fabric-onelake-security</p> <p>Si no vamos a proporcionar acceso externo a trav\u00e9s del API de Onelake a usuarios o aplicaciones, no tenemos necesidad de aplicar permisos directamente a los usuarios sobre los ficheros almacenados en Onelake. Podemos dar permisos a un usuario, por ejemplo, para que pueda acceder al SQL Endpoint que se genera para un lakehouse, sin necesidad de proporcionarle acceso a los ficheros subyacentes. </p>"},{"location":"01%20Introduccion%20a%20Microsoft%20Fabric/01-2%20El%20Concepto%20de%20Onelake/#gobernanza-y-cumplimiento-en-onelake","title":"\ud83d\udd39 Gobernanza y Cumplimiento en OneLake","text":"<p>OneLake ofrece capacidades avanzadas de gesti\u00f3n y cumplimiento de datos, ayudando a las organizaciones a cumplir con normativas como GDPR, HIPAA y SOC 2.</p> <p>Funciones clave de gobernanza en OneLake: \ud83d\udd39 Auditor\u00eda y Monitorizaci\u00f3n: Microsoft Fabric registra autom\u00e1ticamente el acceso y las modificaciones a los datos en Microsoft Purview. \ud83d\udd39 Data Lineage: Permite rastrear el origen y transformaci\u00f3n de los datos a lo largo del tiempo. \ud83d\udd39 Gesti\u00f3n de Retenci\u00f3n y Eliminaci\u00f3n: Se pueden definir pol\u00edticas para retenci\u00f3n y eliminaci\u00f3n segura de datos.</p> <p>\ud83d\udccc Ventaja clave: OneLake garantiza que todos los datos dentro de Microsoft Fabric se gestionen de manera centralizada, evitando inconsistencias y riesgos de cumplimiento.</p> <p>Todo esto podemos verlo desde el hub de Onlake que tenemos en el portal, donde no solo podemos ver los recursos disponibles, sino tambi\u00e9n monitorizar y ver las configuraciones de Gobierno y linaje. </p> <p>![[Onelake Catalogo.png]]</p>"},{"location":"01%20Introduccion%20a%20Microsoft%20Fabric/01-2%20El%20Concepto%20de%20Onelake/#4costes-en-onelake","title":"4.Costes en OneLake","text":"<p>Microsoft Fabric usa un modelo de licenciamiento basado en capacidades (Capacity-based licensing), lo que impacta el rendimiento, pero los costes de Onelake no est\u00e1n incluidos en esa capacidad.</p>"},{"location":"01%20Introduccion%20a%20Microsoft%20Fabric/01-2%20El%20Concepto%20de%20Onelake/#como-funcionan-las-unidades-de-capacidad-cu-en-onelake","title":"\ud83d\udd39 \u00bfC\u00f3mo funcionan las Unidades de Capacidad (CU) en OneLake?","text":"<p>OneLake consume Capacidad de Fabric (CU) seg\u00fan el nivel de procesamiento y almacenamiento requerido para las operaciones de procesamiento de datos.</p> Capacidad Unidades de Capacidad (CU) Casos de Uso en OneLake F2 2 CU Pruebas y peque\u00f1os vol\u00famenes de datos. F4 4 CU Ingesta de datos en ETL ligero. F8 8 CU Workloads intermedios con an\u00e1lisis SQL. F16 16 CU Procesamiento avanzado con Spark y ML. F32+ 32 CU o m\u00e1s An\u00e1lisis en tiempo real y grandes vol\u00famenes de datos. Como hemos comentado, el coste real del almacenamiento no est\u00e1 incluido en las CU y est\u00e1 reflejado en este imagen <p>![[Onelake Costs.png]] Debemos de tener en cuenta que: - Si eliminamos un espacio de trabajo, se nos sigue cobrando durante el periodo de retenci\u00f3n - La cach\u00e9 est\u00e1 relacionada con el an\u00e1lisis en tiempo real (KQL y Data activator)</p> <p>\ud83d\udccc Optimizaci\u00f3n de Costos en OneLake \u2705 Uso de Shortcuts: Accede a datos en AWS, Google Cloud o Azure Data Lake sin duplicarlos. Pagamos por la transferencia de datos  \u2705 Autoscaling de Capacidad: OneLake ajusta autom\u00e1ticamente el consumo de recursos seg\u00fan la carga de trabajo. \u2705 Optimizaci\u00f3n de Consultas: Uso de formatos optimizados como Delta Lake para mejorar el rendimiento y reducir costos de almacenamiento.</p> <p>El detalle de las operaciones y los costes pod\u00e9is consultarlo en este enlace https://learn.microsoft.com/es-es/fabric/onelake/onelake-consumption </p>"},{"location":"01%20Introduccion%20a%20Microsoft%20Fabric/01-2%20El%20Concepto%20de%20Onelake/#5-onelake-explorer","title":"5. Onelake Explorer","text":"<p>Disponemos de una aplicaci\u00f3n, denominada Onelake Explorer que nos permite acceder al contenido de nuestro Onelake, y navegar por \u00e9l, como si estuvi\u00e9semos utilizando Onedrive. </p> <p>![[Onelake Explorer.png]] Al crear, actualizar o eliminar un archivo a trav\u00e9s del Explorador de archivos de Windows, se sincronizan autom\u00e1ticamente los cambios en el servicio OneLake, y podemos tener replicados en local. Adem\u00e1s los shortcuts que tengamos creados, aparecen como si estuviesen los datos realmente almacenados en nuestro OneLake, proporcionando una capa de abstracci\u00f3n completa a como y donde los datos est\u00e1n realmente almacenados. </p>"},{"location":"01%20Introduccion%20a%20Microsoft%20Fabric/01-2%20El%20Concepto%20de%20Onelake/#6-conectando-a-onelake","title":"6. Conectando a Onelake","text":"<p>Podemos acceder de forma abierta a los contenidos de Onelake a trav\u00e9s de las API y los SDK de Azure Data Lake Storage (ADLS) gen2. Tan solo debemos de conocer la URI del recurso al que queremos acceder, que tendr\u00e1 un formato de este estilo:</p> <p><code>https://onelake.dfs.fabric.microsoft.com/&lt;workspace&gt;/&lt;item&gt;.&lt;itemtype&gt;/&lt;path&gt;/&lt;fileName&gt;</code></p> <p>Podemos utilizar tambi\u00e9n el GUID de los elementos de esta forma:</p> <p><code>https://onelake.dfs.fabric.microsoft.com/&lt;workspaceGUID&gt;/&lt;itemGUID&gt;/&lt;path&gt;/&lt;fileName&gt;</code></p> <p>Para m\u00e1s informaci\u00f3n sobre estos accesos puedes revisar la documentaci\u00f3n en este punto: https://learn.microsoft.com/es-es/fabric/onelake/onelake-access-api </p>"},{"location":"01%20Introduccion%20a%20Microsoft%20Fabric/01-2%20El%20Concepto%20de%20Onelake/#7-conclusion-y-preguntas-clave","title":"7. Conclusi\u00f3n y Preguntas Clave","text":""},{"location":"01%20Introduccion%20a%20Microsoft%20Fabric/01-2%20El%20Concepto%20de%20Onelake/#resumen-de-conceptos-clave","title":"\ud83d\udd39 Resumen de conceptos clave","text":"<p>\u2705 OneLake es el Data Lake unificado de Microsoft Fabric, dise\u00f1ado para eliminar los silos de datos. \u2705 Su arquitectura permite integraci\u00f3n con m\u00faltiples motores y uso compartido de datos sin duplicaciones. \u2705 Ofrece medidas avanzadas de seguridad y cumplimiento con Microsoft Purview y Azure AD. \u2705 El consumo de recursos en OneLake se basa en Unidades de Capacidad (CU), permitiendo escalar seg\u00fan las necesidades del negocio.</p>"},{"location":"01%20Introduccion%20a%20Microsoft%20Fabric/01-2%20El%20Concepto%20de%20Onelake/#preguntas-para-reflexion-y-discusion","title":"\ud83d\udd39 Preguntas para reflexi\u00f3n y discusi\u00f3n","text":"<ol> <li>\u00bfC\u00f3mo impactar\u00eda OneLake en la estrategia de almacenamiento de mi empresa?</li> <li>\u00bfCu\u00e1les son las ventajas de OneLake frente a los Data Lakes tradicionales en mi caso de uso?</li> <li>\u00bfC\u00f3mo puedo optimizar costes en OneLake utilizando Shortcuts y estrategias de gobernanza?</li> </ol>"},{"location":"01%20Introduccion%20a%20Microsoft%20Fabric/01-3%20Escenarios%20End-to-End%20en%20Microsoft%20Fabric/","title":"01 3 Escenarios End to End en Microsoft Fabric","text":""},{"location":"01%20Introduccion%20a%20Microsoft%20Fabric/01-3%20Escenarios%20End-to-End%20en%20Microsoft%20Fabric/#1-etl-con-microsoft-fabric-uso-de-dataflows-gen2-y-pipelines","title":"1. ETL con Microsoft Fabric: Uso de Dataflows Gen2 y Pipelines","text":""},{"location":"01%20Introduccion%20a%20Microsoft%20Fabric/01-3%20Escenarios%20End-to-End%20en%20Microsoft%20Fabric/#introduccion-a-los-procesos-etl-en-fabric","title":"\ud83d\udd39 Introducci\u00f3n a los procesos ETL en Fabric","text":"<p>En Microsoft Fabric, los procesos ETL (Extract, Transform, Load) pueden realizarse a trav\u00e9s de tres  herramientas principales:</p> <ol> <li>Dataflows Gen2 (para transformaciones sin c\u00f3digo o de bajo c\u00f3digo).</li> <li>Pipelines en Data Factory (para cargas y orquestaci\u00f3n m\u00e1s avanzadas).</li> <li>Notebooks de PySpark, con c\u00f3digo completamente personalizable</li> </ol> <p>Ambas herramientas est\u00e1n dise\u00f1adas para simplificar la ingesta y transformaci\u00f3n de datos, integr\u00e1ndose con OneLake, Lakehouses y Data Warehouses.</p>"},{"location":"01%20Introduccion%20a%20Microsoft%20Fabric/01-3%20Escenarios%20End-to-End%20en%20Microsoft%20Fabric/#dataflows-gen2-transformaciones-sin-codigo","title":"\ud83d\udd39 Dataflows Gen2: Transformaciones sin c\u00f3digo","text":"<p>Los Dataflows Gen2 permiten la creaci\u00f3n de procesos ETL en un entorno visual, facilitando: \u2705 Conexi\u00f3n con m\u00faltiples fuentes de datos, incluyendo OneLake, SQL, SaaS y APIs. \u2705 Uso de Power Query para transformaciones sin necesidad de programaci\u00f3n. \u2705 Automatizaci\u00f3n de cargas peri\u00f3dicas con programaci\u00f3n de actualizaciones.</p> <p>\ud83d\udccc Ventajas de Dataflows Gen2</p> <ul> <li>Ideal para usuarios de negocio y analistas de datos que necesitan transformar datos sin escribir c\u00f3digo.</li> <li>Se ejecuta directamente en Microsoft Fabric, evitando la necesidad de exportaciones a otros servicios.</li> <li>Puede alimentar modelos sem\u00e1nticos de Power BI sin pasos adicionales.</li> </ul> <p>Una de las grandes diferencias entre los Dataflows tradicionales y los Dataflows de Generaci\u00f3n 2, es que estos \u00faltimos pueden almacenar el resultado de sus ejecuciones en diferentes destinos ![[Dataflow Gen2 Destinos.png]]</p>"},{"location":"01%20Introduccion%20a%20Microsoft%20Fabric/01-3%20Escenarios%20End-to-End%20en%20Microsoft%20Fabric/#pipelines-de-data-factory-orquestacion-avanzada","title":"\ud83d\udd39 Pipelines de Data Factory: Orquestaci\u00f3n avanzada","text":"<p>Los Pipelines en Data Factory dentro de Fabric proporcionan una soluci\u00f3n m\u00e1s avanzada y escalable para la integraci\u00f3n de datos: \u2705 Permite la creaci\u00f3n de flujos de trabajo de datos complejos con m\u00faltiples pasos. \u2705 Integraci\u00f3n con Azure Data Factory para reutilizaci\u00f3n de flujos existentes. \u2705 Soporta activadores y ejecuci\u00f3n basada en eventos, lo que facilita la automatizaci\u00f3n de cargas.</p> <p>\ud83d\udccc Diferencias entre Dataflows Gen2 y Pipelines</p> Caracter\u00edstica Dataflows Gen2 Pipelines en Data Factory Orientaci\u00f3n Usuarios de negocio, analistas Ingenieros de datos, arquitectos C\u00f3digo necesario No-Code/Low-Code (Power Query) Requiere configuraci\u00f3n avanzada Escalabilidad Adecuado para vol\u00famenes moderados de datos Ideal para cargas de datos masivas Automatizaci\u00f3n Programaciones b\u00e1sicas Orquestaci\u00f3n avanzada con dependencias <p>\ud83d\udd39 Ejemplo pr\u00e1ctico: Un Dataflow Gen2 puede usarse para transformar datos de ventas en bruto en OneLake, mientras que un Pipeline se encargar\u00e1 de la carga incremental en un Data Warehouse\u200bfabric-fundamentals.</p>"},{"location":"01%20Introduccion%20a%20Microsoft%20Fabric/01-3%20Escenarios%20End-to-End%20en%20Microsoft%20Fabric/#notebooks-de-pyspark","title":"\ud83d\udd39 Notebooks de PySpark","text":"<p>Para cargas de trabajo que requieran de la flexibilidad que proporciona un lenguaje de programaci\u00f3n como Python, escalando a trav\u00e9s de Apache Spark, tenemos la posibilidad de escribir todos nuestros procesos utilizando Notebooks de PySpark. Son una opci\u00f3n flexible, porque adem\u00e1s de tener a nuestra disposici\u00f3n una gran cantidad de librer\u00edas para procesado, y toda la potencia de los cl\u00fasteres de Apache Spark, estos Notebooks son la opci\u00f3n que m\u00e1s nos a\u00edsla de Microsoft Fabric. Si por ejemplo, el d\u00eda de ma\u00f1ana queremos migrar alguna carga de trabajo a Databricks, podr\u00edamos reutilizar una gran cantidad de ese c\u00f3digo, algo que no es posible si utilizamos Dataflows y muy complicado con los Pipelines de Data Factory. </p>"},{"location":"01%20Introduccion%20a%20Microsoft%20Fabric/01-3%20Escenarios%20End-to-End%20en%20Microsoft%20Fabric/#2-almacenamiento-y-analisis-integracion-con-lakehouses-y-data-warehouses","title":"2. Almacenamiento y An\u00e1lisis: Integraci\u00f3n con Lakehouses y Data Warehouses","text":""},{"location":"01%20Introduccion%20a%20Microsoft%20Fabric/01-3%20Escenarios%20End-to-End%20en%20Microsoft%20Fabric/#lakehouses-en-microsoft-fabric","title":"\ud83d\udd39 Lakehouses en Microsoft Fabric","text":"<p>El Lakehouse es una combinaci\u00f3n de Data Lake y Data Warehouse, permitiendo almacenar y analizar datos en un \u00fanico entorno.</p> <p>\u2705 Basado en el formato Delta Lake, lo que permite transacciones ACID y versionado de datos. \u2705 Integrado con OneLake, lo que elimina la duplicaci\u00f3n de datos. \u2705 Soporta acceso desde T-SQL y Spark, facilitando la colaboraci\u00f3n entre equipos de datos.</p> <p>\ud83d\udccc Cu\u00e1ndo usar un Lakehouse</p> <ul> <li>Si se necesitan tanto an\u00e1lisis estructurados como semiestructurados en un mismo entorno.</li> <li>Cuando los cient\u00edficos de datos y analistas deben trabajar sobre el mismo conjunto de datos.</li> <li>Para procesamiento en batch y en tiempo real dentro de Fabric\u200b.</li> </ul>"},{"location":"01%20Introduccion%20a%20Microsoft%20Fabric/01-3%20Escenarios%20End-to-End%20en%20Microsoft%20Fabric/#data-warehouses-en-microsoft-fabric","title":"\ud83d\udd39 Data Warehouses en Microsoft Fabric","text":"<p>Un Data Warehouse en Fabric es una soluci\u00f3n de almacenamiento estructurado, optimizada para consultas SQL y an\u00e1lisis de datos transaccionales.</p> <p>\u2705 Usa T-SQL nativo, facilitando la migraci\u00f3n desde bases de datos SQL tradicionales. \u2705 Soporta consultas de alto rendimiento y escalabilidad autom\u00e1tica. \u2705 Permite la integraci\u00f3n con Power BI y otros servicios de Fabric.</p> <p>\ud83d\udccc Cu\u00e1ndo usar un Data Warehouse</p> <ul> <li>Si los datos son estructurados y requieren consultas SQL optimizadas.</li> <li>Cuando se necesita compatibilidad con herramientas de an\u00e1lisis basadas en SQL.</li> <li>Si los datos deben mantenerse en un formato relacional con transacciones ACID.</li> </ul> <p>\ud83d\udd39 Ejemplo pr\u00e1ctico: Un Lakehouse puede almacenar grandes vol\u00famenes de datos en bruto de sensores IoT, mientras que un Data Warehouse almacena versiones agregadas para an\u00e1lisis financieros\u200b.</p> <p>Otro criterio de selecci\u00f3n, est\u00e1 relacionado con los conocimientos que tengan los equipos para el desarrollo y mantenimiento de la soluci\u00f3n. Teniendo en cuenta que el almacenamiento se realiza en Onelake y en formato Delta, es relevante el hecho de que los equipos deban de conocer o familiarizarse con Notebooks y PySpark, o por el contrario conocimientos de T-SQL. </p>"},{"location":"01%20Introduccion%20a%20Microsoft%20Fabric/01-3%20Escenarios%20End-to-End%20en%20Microsoft%20Fabric/#3-caso-practico-diseno-de-una-solucion-de-datos-escalable","title":"3. Caso pr\u00e1ctico: Dise\u00f1o de una soluci\u00f3n de datos escalable","text":""},{"location":"01%20Introduccion%20a%20Microsoft%20Fabric/01-3%20Escenarios%20End-to-End%20en%20Microsoft%20Fabric/#escenario-realista-plataforma-de-analisis-de-ventas","title":"\ud83d\udd39 Escenario realista: Plataforma de An\u00e1lisis de Ventas","text":"<p>Se dise\u00f1ar\u00e1 una soluci\u00f3n que permita a una empresa de retail analizar sus ventas y optimizar la distribuci\u00f3n de productos en sus tiendas.</p>"},{"location":"01%20Introduccion%20a%20Microsoft%20Fabric/01-3%20Escenarios%20End-to-End%20en%20Microsoft%20Fabric/#requerimientos","title":"Requerimientos","text":"<p>1\ufe0f\u20e3 Ingesta de datos de ventas en tiempo real desde m\u00faltiples sistemas POS. 2\ufe0f\u20e3 Procesamiento de datos con Dataflows Gen2 para limpieza y transformaci\u00f3n. 3\ufe0f\u20e3 Carga en OneLake utilizando Pipelines de Data Factory. 4\ufe0f\u20e3 Uso de un Lakehouse para almacenar datos hist\u00f3ricos en bruto. 5\ufe0f\u20e3 Creaci\u00f3n de un Data Warehouse para consultas SQL optimizadas. 6\ufe0f\u20e3 Modelado en Power BI para generaci\u00f3n de informes interactivos.</p>"},{"location":"01%20Introduccion%20a%20Microsoft%20Fabric/01-3%20Escenarios%20End-to-End%20en%20Microsoft%20Fabric/#arquitectura-de-la-solucion","title":"Arquitectura de la soluci\u00f3n","text":"<ol> <li>Extracci\u00f3n de datos desde bases de datos SQL y APIs de terceros.</li> <li>Transformaci\u00f3n en Dataflows Gen2 para limpieza y agregaciones.</li> <li>Carga en OneLake y procesamiento con Spark en un Lakehouse.</li> <li>Creaci\u00f3n de un modelo anal\u00edtico en un Data Warehouse para consultas SQL.</li> <li>Publicaci\u00f3n en Power BI con dashboards en tiempo real.</li> </ol> <p>\ud83d\udd39 Beneficios de esta arquitectura: \u2705 Eficiencia y escalabilidad: Fabric maneja grandes vol\u00famenes de datos sin sobrecargar sistemas transaccionales. \u2705 An\u00e1lisis en tiempo real: Las decisiones pueden tomarse con informaci\u00f3n actualizada al minuto. \u2705 Reducci\u00f3n de costos: No se requiere infraestructura dedicada, todo funciona en la nube con escalabilidad autom\u00e1tica.</p> <p>\ud83d\udccc Resultado esperado: Con esta soluci\u00f3n, los equipos de ventas y finanzas pueden acceder a datos en tiempo real sobre el desempe\u00f1o de productos en distintas ubicaciones, optimizando la cadena de suministro y aumentando la rentabilidad\u200b.</p>"},{"location":"01%20Introduccion%20a%20Microsoft%20Fabric/01-3%20Escenarios%20End-to-End%20en%20Microsoft%20Fabric/#4-escenarios-y-arquitecturas","title":"4. Escenarios y Arquitecturas","text":""},{"location":"01%20Introduccion%20a%20Microsoft%20Fabric/01-3%20Escenarios%20End-to-End%20en%20Microsoft%20Fabric/#el-mas-sencillo","title":"\ud83d\udd39 El m\u00e1s sencillo","text":"<p>Imaginemos que queremos actualizar nuestra arquitectura Power BI, porque tenemos m\u00e1s necesidades de carga de datos, o simplemente queremos comenzar a analizar datos no estructuras de un modo m\u00e1s \u00e1gil. En un escenario de este estilo, podr\u00edamos tener: - Un espacio de trabajo con una capacidad F2 o F4, iniciada \u00fanicamente en los momentos en los que necesitamos realizar la carga y procesamiento de los datos - Nuestros procesos de ingenier\u00eda de datos para limpieza y dem\u00e1s tareas que dejen los datos listos en nuestro warehouse - Otro espacio de trabajo en el que tengamos los modelos sem\u00e1nticos de Power BI, en modo import, y sus correspondientes informes De esta forma, podemos disponer de las capacidades de Fabric de un modo econ\u00f3mico, al estar pagando \u00fanicamente por el tiempo en el que realizamos las cargas de datos. Al tener los modelos sem\u00e1nticos en Modo Import, no necesitamos que la capacidad de Fabric est\u00e9 iniciada cuando los usuarios acceden a los informes. </p>"},{"location":"01%20Introduccion%20a%20Microsoft%20Fabric/01-3%20Escenarios%20End-to-End%20en%20Microsoft%20Fabric/#monolitico","title":"\ud83d\udd39 Monol\u00edtico","text":"<p>En un escenario de este tipo, disponemos \u00fanicamente de una capacidad que est\u00e1 asignada a un \u00fanico espacio de trabajo de Fabric, para dar soporte, por ejemplo, a lo comentado en la secci\u00f3n anterior.  ![[Despliegue Monolitico.png]]</p>"},{"location":"01%20Introduccion%20a%20Microsoft%20Fabric/01-3%20Escenarios%20End-to-End%20en%20Microsoft%20Fabric/#capacidad-compartida","title":"\ud83d\udd39 Capacidad Compartida","text":"<p>Disponemos de una \u00fanica capacidad de Fabric que es compartida por diferentes espacios de trabajo, para dar soporte a las cargas que se ejecuten en dichos espacios de trabajo</p> <p>![[Despliegue Compartido.png]]</p>"},{"location":"01%20Introduccion%20a%20Microsoft%20Fabric/01-3%20Escenarios%20End-to-End%20en%20Microsoft%20Fabric/#escalado","title":"\ud83d\udd39 Escalado","text":"<p>Para escenarios que requieren de recursos dedicados, o que sea necesario escalar los recursos, podemos disponer de diferentes capacidades asignadas a distintos espacios de trabajo de Fabric</p> <p>![[Despliegue Escalado.png]]</p>"},{"location":"01%20Introduccion%20a%20Microsoft%20Fabric/01-3%20Escenarios%20End-to-End%20en%20Microsoft%20Fabric/#5-conclusion-y-preguntas-clave","title":"5. Conclusi\u00f3n y Preguntas Clave","text":"<p>\u2705 Microsoft Fabric permite dise\u00f1ar soluciones end-to-end combinando ETL, almacenamiento y an\u00e1lisis en una \u00fanica plataforma. \u2705 Dataflows Gen2 y Pipelines facilitan la ingesta y transformaci\u00f3n de datos en Fabric. \u2705 El uso combinado de Lakehouses y Data Warehouses proporciona una arquitectura flexible y escalable. \u2705 Un dise\u00f1o bien estructurado en Fabric mejora la eficiencia, reduce costes y habilita an\u00e1lisis avanzados.</p>"},{"location":"01%20Introduccion%20a%20Microsoft%20Fabric/01-3%20Escenarios%20End-to-End%20en%20Microsoft%20Fabric/#preguntas-de-reflexion-para-la-sesion","title":"Preguntas de reflexi\u00f3n para la sesi\u00f3n","text":"<ol> <li>\u00bfCu\u00e1ndo deber\u00edas utilizar Dataflows Gen2 en lugar de Pipelines en Data Factory?</li> <li>\u00bfEn qu\u00e9 casos es preferible usar un Lakehouse en vez de un Data Warehouse?</li> <li>\u00bfC\u00f3mo asegurar\u00edas la escalabilidad de una soluci\u00f3n de datos en Fabric para grandes vol\u00famenes de datos?</li> </ol>"},{"location":"02%20Carga%20de%20Datos/02-0%20Ingesta%20de%20Datos%20en%20Microsoft%20Fabric/","title":"Ingesta de Datos en Microsoft Fabric","text":"<p>En este m\u00f3dulo veremos las opciones que tenemos para cargar datos en Microsoft Fabric, como primer paso en cualquier proyecto de Anal\u00edtica, puesto que necesitamos los datos disponibles para poder comenzar con los procesos de desarrollo. En este tema veremos:</p> <p>\u2705 Introducci\u00f3n a la carga de Datos en Fabric </p> <p>\u2705Dataflows de Generaci\u00f3n 2</p> <p>\u2705Orquestando cargas con Pipelines de Data Factory</p> <p>\u2705Desarrollo con Apache Spark</p>"},{"location":"02%20Carga%20de%20Datos/02-0%20Ingesta%20de%20Datos%20en%20Microsoft%20Fabric/#introduccion-a-la-carga-de-datos-en-microsoft-fabric","title":"Introducci\u00f3n a la Carga de Datos en Microsoft Fabric","text":"<p>Disponemos de diferentes mecanismos para disponibilizar los datos en Microsoft Fabric tales como: - Cargar los datos manualmente - Accesos directos en nuestros lakehouses - Mirrror de Bases de Datos en datawarehouse - Cargas de Datos con Dataflows de Generaci\u00f3n 2 - Actividad de Copia de Datos en Pipelines - Notebooks de Apache Spark</p> <p>Pero antes de evaluar estas opciones es necesario conectarnos a los or\u00edgenes, utilizando las conexiones y los gateways</p>"},{"location":"02%20Carga%20de%20Datos/02-0%20Ingesta%20de%20Datos%20en%20Microsoft%20Fabric/#1del-etl-al-elt","title":"1.Del ETL al ELT","text":"<p>Con el aumento paulatino en la cantidad de datos generados y que necesitamos utilizar para nuestros an\u00e1lisis, los patrones de carga de datos ha evolucionado del patr\u00f3n ETL (Extracci\u00f3n, Transformaci\u00f3n y Carga) al nuevo patr\u00f3n ELT (Extracci\u00f3n, Carga y Transformaci\u00f3n).  Ambos modelos tienen como objetivo procesar y estructurar la informaci\u00f3n, pero sus enfoques var\u00edan seg\u00fan la infraestructura y las necesidades de las empresas.</p> Caracter\u00edstica ETL ELT Orden de Procesamiento Extracci\u00f3n \u2192 Transformaci\u00f3n \u2192 Carga Extracci\u00f3n \u2192 Carga \u2192 Transformaci\u00f3n D\u00f3nde se realizan las transformaciones En un motor de procesamiento intermedio Directamente en el Data Lake o Data Warehouse Escalabilidad Limitada por la capacidad del servidor ETL Altamente escalable, aprovecha el poder del Data Lake y del Data Warehouse Tiempo de procesamiento M\u00e1s lento debido a la transformaci\u00f3n previa a la carga M\u00e1s r\u00e1pido, ya que los datos sin procesar se almacenan primero y luego se transforman en paralelo Casos de uso recomendados Integraci\u00f3n de m\u00faltiples fuentes con reglas de negocio complejas antes de la carga An\u00e1lisis masivo de datos, Big Data, Machine Learning, an\u00e1lisis en tiempo real <p>Tradicionalmente, las empresas utilizaban ETL debido a las limitaciones en almacenamiento y computaci\u00f3n. Antes de la llegada de los Data Lakes, los datos deb\u00edan ser transformados antes de cargarlos en un Data Warehouse, ya que este \u00faltimo requer\u00eda una estructura optimizada para consultas. Con los ELT, lo que hacemos es realizar la ingesta de los datos en bruto, y aplicar posteriormente las transformaciones necesarias para cada caso de uso. </p> <p>Sin embargo, la explosi\u00f3n del Big Data y el abaratamiento del almacenamiento en la nube han impulsado ELT, donde los datos se almacenan primero en bruto y se transforman solo cuando es necesario, utilizando el poder de c\u00f3mputo de soluciones modernas como Microsoft Fabric.</p>"},{"location":"02%20Carga%20de%20Datos/02-0%20Ingesta%20de%20Datos%20en%20Microsoft%20Fabric/#2conexiones-de-datos","title":"2.Conexiones de Datos","text":"<p>El primer paso para configurar una ingesta de datos, es obviamente conectarnos el origen donde se encuentran los datos y para ello disponemos de una opci\u00f3n de configuraci\u00f3n donde podremos ver las conexiones creadas, as\u00ed como crear nuevas conexiones. </p> <p></p> <p>Dentro de esta opci\u00f3n, podremos configurar tanto Conexiones a nuestros or\u00edgenes de datos, como las puertas de enlace que tengamos desplegadas para acceso a datos locales, o para acceso a datos de la nube que est\u00e1n en una red virtual, tal y como podemos apreciar en la imagen. </p> <p></p> <p>Lo recomendable es gestionar las conexiones y las puertas de enlace, desde esta opci\u00f3n centralizada de configuraci\u00f3n, para poder reutilizarlas, asignando permisos a aquellos usuarios que queramos que puedan utilizar dichas conexiones. Cuando seleccionamos la opci\u00f3n de crear una nueva conexi\u00f3n, veremos las tres opciones comentadas: local, nube o red virtual. Para las opciones de red virtual y local, es necesario referenciar a un gateway que ya tengamos instalado y configurado. Veremos como crearlos en el siguiente apartado.  Por lo que respecta a las conexiones de Nube dependiendo del tipo de conexi\u00f3n la parametrizaci\u00f3n ser\u00e1 diferente, pero b\u00e1sicamente necesitaremos los datos para crear una cadena de conexi\u00f3n. Tal y como se aprecia tambi\u00e9n en la imagen, ahora mismo los dataflows no soportan la utilizaci\u00f3n de estas conexiones de la nube, debiendo de crearlas desde el propio entorno de los dataflows. Se espera que en breve tambi\u00e9n soporte la reutilizaci\u00f3n de este tipo de conexiones. </p> <p></p> <p>Una vez tengamos esa conexi\u00f3n creada, podremos utilizarla en nuestros pipelines, y en los modemos sem\u00e1nticos de Power BI, para el acceso a esos datos. </p> <p>Adicionalmente si seleccionamos la conexi\u00f3n, tendremos la posibilidad de configurar permisos y compartir la conexi\u00f3n con otros usuarios de nuestro tenant, tal y como muestra la imagen</p> <p></p>"},{"location":"02%20Carga%20de%20Datos/02-0%20Ingesta%20de%20Datos%20en%20Microsoft%20Fabric/#3data-gateways-puertas-de-enlace","title":"3.Data Gateways (Puertas de Enlace)","text":"<p>Microsoft Fabric es una soluci\u00f3n en la nube, y es muy probable que dispongamos de datos que est\u00e9n en local en nuestra infraestructura, o incluso en m\u00e1quinas virtuales (IaaS) en una nube p\u00fablica o un hosting. En este caso, es necesario configurar un Data Gateway que act\u00fae de pasarela entre la nube de Microsoft y la ubicaci\u00f3n en la que se encuentran los datos.  Existen dos tipos diferentes de Data Gateways o puertas de enlace de datos locales, cada uno para un escenario diferente.</p> <ul> <li>Puerta de enlace de datos local: permite que varios usuarios se conecten a m\u00faltiples fuentes de datos locales. Con una instalaci\u00f3n de puerta de enlace \u00fanica, puede utilizar una puerta de enlace de datos local con todos los servicios compatibles. Esta puerta de enlace se adapta bien a escenarios complejos en los que varias personas acceden a m\u00faltiples fuentes de datos.</li> <li>Puerta de enlace de datos local (modo personal): permite que un usuario se conecte a fuentes de datos y no se puede compartir con otros. Una puerta de enlace de datos local (modo personal) solo se puede usar con Power BI. Esta puerta de enlace es ideal para escenarios en los que usted es el \u00fanico que crea informes y no necesita compartir ninguna fuente de datos con otros.</li> </ul> <p>Adem\u00e1s, hay una puerta de enlace de datos de red virtual (VNet) que permite que varios usuarios se conecten a m\u00faltiples fuentes de datos protegidas por redes virtuales. No se requiere instalaci\u00f3n porque es un servicio administrado por Microsoft. Esta puerta de enlace se adapta bien a escenarios complejos en los que varias personas acceden a m\u00faltiples fuentes de datos.</p> <p>Los usuarios de la organizaci\u00f3n pueden acceder a datos locales a los que ya tienen autorizaci\u00f3n de acceso desde servicios en la nube como Power BI, Power Platform y Microsoft Fabric. Pero antes de que esos usuarios puedan conectar el servicio en la nube a la fuente de datos local, es necesario instalar y configurar una puerta de enlace de datos local.</p>"},{"location":"02%20Carga%20de%20Datos/02-0%20Ingesta%20de%20Datos%20en%20Microsoft%20Fabric/#como-funciona","title":"C\u00f3mo funciona?","text":"<p>A continuaci\u00f3n se muestra el diagrama de funcionamiento de estas puertas de enlace:</p> <p></p> <ol> <li>El servicio en la nube crea una consulta y las credenciales cifradas para la fuente de datos local. La consulta y las credenciales se env\u00edan a la cola de la puerta de enlace para su procesamiento cuando la puerta de enlace sondea el servicio peri\u00f3dicamente. </li> <li>El servicio en la nube de puerta de enlace analiza la consulta y env\u00eda la solicitud a Azure Relay.</li> <li>Azure Relay env\u00eda las solicitudes pendientes a la puerta de enlace cuando sondea peri\u00f3dicamente. Tanto la puerta de enlace como el servicio Power BI est\u00e1n implementados para aceptar \u00fanicamente tr\u00e1fico TLS 1.2.</li> <li>La puerta de enlace recibe la consulta, descifra las credenciales y se conecta a una o m\u00e1s fuentes de datos con esas credenciales.</li> <li>La puerta de enlace env\u00eda la consulta a la fuente de datos para su ejecuci\u00f3n.</li> <li>Los resultados se env\u00edan desde la fuente de datos a la puerta de enlace y luego al servicio en la nube. Luego, el servicio utiliza los resultados.</li> </ol> <p>La instalaci\u00f3n de estas puertas de enlace, puede realizarse en cualquier equipo de la red local (o DMZ) que tenga acceso a los or\u00edgenes de datos a los que se quiere acceder. El acceso al instalador lo tenemos en el \u00e1rea de descargas del portal de Microsoft Fabric. Una vez finalizada la descarga, cuando lancemos la ejecuci\u00f3n y aceptemos la licencia , nos solicitar\u00e1 iniciar sesi\u00f3n, para poder crear la puerta de enlace en MIcrosoft Fabric, solicit\u00e1ndonos un nombre, y una clave de recuperaci\u00f3n:</p> <p></p> <p>Al finalizar la configuraci\u00f3n, ya nos aparecer\u00e1 la puerta de enlace en el portal de Fabric, y podremos utilizarla para nuevas conexiones!</p>"},{"location":"02%20Carga%20de%20Datos/02-1%20ELT%20con%20flujos%20de%20datos%20de%20Gen2/","title":"ELT con flujos de Generaci\u00f3n 2","text":""},{"location":"02%20Carga%20de%20Datos/02-1%20ELT%20con%20flujos%20de%20datos%20de%20Gen2/#1-creacion-y-configuracion-de-dataflows-gen2","title":"1. Creaci\u00f3n y configuraci\u00f3n de Dataflows Gen2","text":""},{"location":"02%20Carga%20de%20Datos/02-1%20ELT%20con%20flujos%20de%20datos%20de%20Gen2/#que-es-dataflow-gen2","title":"\ud83d\udd39 \u00bfQu\u00e9 es Dataflow Gen2?","text":"<p>Dataflows Gen2 en Microsoft Fabric es una soluci\u00f3n low-code para la ingesta y transformaci\u00f3n de datos, dise\u00f1ada para simplificar el proceso ETL. Est\u00e1 basado en Power Query, lo que permite a los usuarios conectar, transformar y cargar datos desde diversas fuentes sin necesidad de c\u00f3digo. Los Flujos de Datos de Gen2 son la evoluci\u00f3n de los Dataflows de Power BI, con mejoras significativas en rendimiento, compatibilidad con OneLake y escalabilidad a nivel empresarial.</p> <p>\ud83d\udccc Principales Beneficios de Dataflows Gen2:</p> <p>\u2705 Procesamiento en la nube: No requieren infraestructura local. \u2705 Integraci\u00f3n con Power Query: Se usa la misma interfaz de Power BI, pero m\u00e1s potente. \u2705 Escalabilidad: Permite procesar grandes vol\u00famenes de datos en Fabric. \u2705 Compatibilidad con m\u00faltiples fuentes de datos. \u2705 Automatizaci\u00f3n de cargas con Pipelines de Data Factory.</p>"},{"location":"02%20Carga%20de%20Datos/02-1%20ELT%20con%20flujos%20de%20datos%20de%20Gen2/#componentes-clave-de-dataflows-gen2","title":"\ud83d\udd39 Componentes clave de Dataflows Gen2","text":"<ul> <li>Power Query: El motor de transformaci\u00f3n de datos utilizado en Dataflows Gen2 es el mismo que en Power BI y Excel. Esto permite a los usuarios aprovechar su experiencia con Power Query para construir flujos de datos complejos sin necesidad de escribir c\u00f3digo. Power Query ofrece m\u00e1s de 300 transformaciones de datos basadas en inteligencia artificial</li> <li>Conectores: Dataflows Gen2 ofrece una amplia gama de conectores a diversas fuentes de datos, incluyendo bases de datos SQL, archivos CSV, Excel, servicios en la nube, APIs y m\u00e1s. Esto permite la extracci\u00f3n de datos de m\u00faltiples sistemas y la integraci\u00f3n en una \u00fanica plataforma.</li> <li>Destinos: Dataflows Gen2 puede cargar datos transformados en varios destinos, incluyendo Azure Data Lake Storage Gen2, Lakehouses en Microsoft Fabric, y otros destinos como tablas de bases de datos</li> <li>Orquestaci\u00f3n: Aunque Dataflows Gen2 se centra en la transformaci\u00f3n de datos, tambi\u00e9n puede integrarse con Data Pipelines de Microsoft Fabric para la orquestaci\u00f3n de flujos de datos. Esto permite construir procesos ETL complejos con gesti\u00f3n de dependencias y disparadores.</li> <li>Staging: Dataflows Gen2 utiliza un \u00e1rea de almacenamiento temporal o \"staging\" para guardar los resultados intermedios de las transformaciones. Esto mejora el rendimiento del proceso, ya que las transformaciones se aplican en la nube y el resultado se carga en el destino final. .</li> </ul>"},{"location":"02%20Carga%20de%20Datos/02-1%20ELT%20con%20flujos%20de%20datos%20de%20Gen2/#pasos-para-crear-un-dataflow-gen2","title":"\ud83d\udd39 Pasos para crear un Dataflow Gen2","text":"<p>A continuaci\u00f3n, explicamos c\u00f3mo crear y configurar un Flujo de Datos de Gen2 paso a paso.</p>"},{"location":"02%20Carga%20de%20Datos/02-1%20ELT%20con%20flujos%20de%20datos%20de%20Gen2/#acceder-a-dataflows-gen2","title":"Acceder a Dataflows Gen2","text":"<ol> <li>En Microsoft Fabric, ir al espacio de trabajo donde se desea crear el flujo.</li> <li>Hacer clic en Nuevo \u2192 Flujo de Datos Gen2.</li> <li>Se abrir\u00e1 la interfaz de Power Query, donde se configuran las fuentes y transformaciones.</li> </ol>"},{"location":"02%20Carga%20de%20Datos/02-1%20ELT%20con%20flujos%20de%20datos%20de%20Gen2/#conectar-una-fuente-de-datos","title":"Conectar una Fuente de Datos","text":"<p>Los Flujos de Datos de Gen2 pueden conectarse a m\u00e1s de 300 fuentes de datos, como: \u2714 Azure SQL Database \u2714 OneLake (Almacenamiento central en Fabric) \u2714 Archivos CSV en Azure Blob Storage \u2714 Google BigQuery, Amazon S3, SAP HANA, APIs REST</p> <p>\ud83d\udccc Ejemplo Pr\u00e1ctico: Si queremos extraer datos de ventas desde una base de datos SQL en Azure:</p> <p><code>SELECT id_transaccion, cliente_id, monto, fecha_venta  FROM ventas WHERE fecha_venta &gt;= '2024-01-01'</code></p> <p>\ud83d\udca1 Optimizaci\u00f3n: Si la base de datos soporta particionamiento, aplicar filtros en la consulta para reducir el volumen de datos extra\u00eddos.</p>"},{"location":"02%20Carga%20de%20Datos/02-1%20ELT%20con%20flujos%20de%20datos%20de%20Gen2/#aplicar-transformaciones-en-power-query","title":"Aplicar Transformaciones en Power Query","text":"<p>Una vez importados los datos, podemos realizar transformaciones como: \u2714 Eliminar duplicados \u2714 Filtrar datos innecesarios \u2714 Unir datos de m\u00faltiples fuentes \u2714 Convertir formatos de fecha, n\u00famero y texto \u2714 Crear columnas calculadas</p> <p>\ud83d\udccc Ejemplo en Power Query:</p> <p><code>= Table.AddColumn(#\"Datos Importados\", \"Margen\", each [Precio] - [Costo], type number)</code></p>"},{"location":"02%20Carga%20de%20Datos/02-1%20ELT%20con%20flujos%20de%20datos%20de%20Gen2/#definir-el-destino-de-datos","title":"Definir el Destino de Datos","text":"<p>El resultado del Dataflow Gen2 puede almacenarse en: \u2714 OneLake \u2714 Azure SQL Database \u2714 Un modelo sem\u00e1ntico en Power BI</p> <p>Para definir el destino:</p> <ol> <li>Seleccionar OneLake para almacenar los datos transformados.</li> <li>Configurar una actualizaci\u00f3n programada para la carga de datos.</li> </ol>"},{"location":"02%20Carga%20de%20Datos/02-1%20ELT%20con%20flujos%20de%20datos%20de%20Gen2/#2-conectores-disponibles-en-fabric-y-su-uso-en-escenarios-reales","title":"2. Conectores disponibles en Fabric y su uso en escenarios reales","text":""},{"location":"02%20Carga%20de%20Datos/02-1%20ELT%20con%20flujos%20de%20datos%20de%20Gen2/#tipos-de-conectores-en-dataflows-gen2","title":"\ud83d\udd39 Tipos de conectores en Dataflows Gen2","text":"<p>Los Dataflows Gen2 ofrecen una gran variedad de conectores para la ingesta de datos. Entre los m\u00e1s utilizados est\u00e1n\u200b:</p> Tipo de Conector Ejemplos de Fuentes Casos de Uso Bases de Datos SQL Server, PostgreSQL, SAP HANA Extracci\u00f3n de datos transaccionales para an\u00e1lisis. Servicios Cloud Azure Data Lake, AWS S3, Google BigQuery Integraci\u00f3n de datos multicloud. Archivos Excel, CSV, JSON, Parquet Transformaci\u00f3n y limpieza de datos no estructurados. SaaS y APIs Salesforce, Dynamics 365, SharePoint Conexi\u00f3n con sistemas empresariales para reporting."},{"location":"02%20Carga%20de%20Datos/02-1%20ELT%20con%20flujos%20de%20datos%20de%20Gen2/#ejemplo-de-escenario-real","title":"\ud83d\udd39 Ejemplo de escenario real","text":"<p>Una empresa de e-commerce necesita consolidar datos de ventas desde SQL Server, Google Analytics y un ERP en Dynamics 365.</p> <p>1\ufe0f\u20e3 Fuente: SQL Server, Google Analytics y Dynamics 365. 2\ufe0f\u20e3 Proceso: Se utiliza un Dataflow Gen2 para transformar y consolidar la informaci\u00f3n. 3\ufe0f\u20e3 Destino: Los datos limpios se almacenan en un Data Warehouse en Fabric para reportes en Power BI.</p> <p>\ud83d\udccc Beneficio: Automatizaci\u00f3n del proceso ETL y reducci\u00f3n del tiempo de consolidaci\u00f3n de datos en un 70% \u200b.</p>"},{"location":"02%20Carga%20de%20Datos/02-1%20ELT%20con%20flujos%20de%20datos%20de%20Gen2/#3-transformaciones-avanzadas-y-optimizacion-del-rendimiento","title":"3. Transformaciones avanzadas y optimizaci\u00f3n del rendimiento","text":"<p>Para mejorar la calidad y estructura de los datos, se pueden aplicar:</p> <p>\u2714 Agrupaciones y agregaciones \u2714 Expresiones condicionales (IF, CASE) \u2714 Tablas pivote y no pivote \u2714 Normalizaci\u00f3n y desnormalizaci\u00f3n de datos</p> <p>\ud83d\udccc Ejemplo: Si queremos calcular el total de ventas por cliente en Power Query:</p> <p><code>= Table.Group(#\"Datos Importados\", {\"cliente_id\"}, {{\"TotalVentas\", each List.Sum([monto]), type number}})</code></p>"},{"location":"02%20Carga%20de%20Datos/02-1%20ELT%20con%20flujos%20de%20datos%20de%20Gen2/#optimizacion-del-rendimiento","title":"\ud83d\udd39Optimizaci\u00f3n del Rendimiento","text":"<p>Para mejorar el rendimiento en Dataflows Gen2, se recomienda: \ud83d\udccc Reducir la cantidad de datos extra\u00eddos aplicando filtros desde la fuente. \ud83d\udccc Usar formatos como Parquet o Delta Lake en OneLake. \ud83d\udccc Particionar grandes vol\u00famenes de datos para mejorar tiempos de procesamiento.</p>"},{"location":"02%20Carga%20de%20Datos/02-1%20ELT%20con%20flujos%20de%20datos%20de%20Gen2/#transformaciones-avanzadas-en-power-query","title":"\ud83d\udd39 Transformaciones avanzadas en Power Query","text":"<p>Power Query en Dataflows Gen2 permite realizar transformaciones avanzadas sin necesidad de programaci\u00f3n:</p> <p>\u2705 Filtrado y limpieza de datos \u2192 Eliminaci\u00f3n de duplicados y correcci\u00f3n de valores nulos. \u2705 Creaci\u00f3n de columnas calculadas \u2192 Uso de expresiones en M Query para c\u00e1lculos personalizados. \u2705 Uniones y combinaciones de datos \u2192 Integraci\u00f3n de m\u00faltiples fuentes en un solo dataset. \u2705 Pivot y unpivot \u2192 Transformaci\u00f3n de datos en distintos formatos para an\u00e1lisis\u200b.</p>"},{"location":"02%20Carga%20de%20Datos/02-1%20ELT%20con%20flujos%20de%20datos%20de%20Gen2/#optimizacion-del-rendimiento-en-dataflows-gen2","title":"\ud83d\udd39 Optimizaci\u00f3n del rendimiento en Dataflows Gen2","text":"<p>\ud83d\udd39 Fast Copy \u2192 Reduce el tiempo de ingesta de datos en un 50% gracias a una optimizaci\u00f3n en la transferencia de datos\u200b. \ud83d\udd39 Incremental Refresh \u2192 Permite actualizar solo los datos nuevos en lugar de recargar todo el dataset\u200b. \ud83d\udd39 Staging optimizado \u2192 Los Dataflows Gen2 crean artefactos de staging en OneLake para acelerar el procesamiento\u200b. \ud83d\udd39 Column Binding en SAP HANA \u2192 Mejora la eficiencia en la consulta de datos en sistemas SAP\u200b.</p> <p>\ud83d\udccc Ejemplo pr\u00e1ctico: Un equipo de analistas financieros necesita actualizar diariamente datos de ventas sin recargar todo el dataset. Se configura Incremental Refresh, logrando una reducci\u00f3n del 80% en el tiempo de carga\u200b.</p> <p>Estas opciones que veremos ahora en detalle, se configuran consulta por consulta: ![[Dataflow config consulta.png]]</p>"},{"location":"02%20Carga%20de%20Datos/02-1%20ELT%20con%20flujos%20de%20datos%20de%20Gen2/#refrescos-incrementales","title":"Refrescos Incrementales","text":"<p>El Refresco Incremental permite que los Dataflows solo carguen datos nuevos o modificados, en lugar de recargar toda la fuente de datos.</p> <p>\ud83d\udccc Ejemplo Pr\u00e1ctico: Si tenemos una tabla de transacciones de ventas con m\u00e1s de 100 millones de registros, un Dataflow con Refresco Completo recargar\u00e1 toda la tabla en cada ejecuci\u00f3n, lo que es ineficiente. Con Refresco Incremental, solo se importar\u00e1n las ventas nuevas del \u00faltimo mes.</p> <p>Para activar el Refresco Incremental en Dataflows Gen2:</p> <ol> <li>Seleccionar la columna de fecha que servir\u00e1 de referencia para el incremental.</li> <li>Configurar el rango de retenci\u00f3n (ejemplo: solo los \u00faltimos 6 meses).</li> <li>Publicar el Dataflow y activar el modo incremental.</li> </ol> <p>Existe alguna limitaci\u00f3n, la m\u00e1s importante relacionada con los destinos como podemos apreciar en esta imagen, porque no todos los destinos soportan actualizaciones incrementales.  ![[Dataflow Gen2 Incremental.png]] Si queremos tener un destino lakehouse, y cargas incrementales, estas no podremos configurarlas en el Dataflow, sino que tendremos que cargar los datos en crudo, y posteriomente refinarlos en el lakehouse. Si el destino est\u00e1 soportado entonces tendremos que configurar la carga incremental, de este modo: ![[Dataflow Carga Incremental.png]]</p> <ul> <li>Primero configurados una columna por la que filtrar los datos. Esta configuraci\u00f3n es necesaria y especifica la columna que usan los flujos de datos para filtrar los datos. Esta columna debe ser una columna DateTime, Date o DateTimeZone. El flujo de datos usa esta columna para filtrar los datos y solo recupera los datos que han cambiado desde la \u00faltima actualizaci\u00f3n.</li> <li>Extraer datos del pasado. Esta configuraci\u00f3n es necesaria y especifica el retroceso en el tiempo que el flujo de datos debe extraer datos. Esta configuraci\u00f3n se usa para recuperar la carga de datos inicial. El flujo de datos recupera todos los datos del sistema de origen que se encuentran dentro del intervalo de tiempo especificado.</li> <li>Tama\u00f1o del cubo. Esta configuraci\u00f3n es necesaria y especifica el tama\u00f1o de los cubos que usa el flujo de datos para filtrar los datos. El flujo de datos divide los datos en cubos en funci\u00f3n de la columna DateTime. Cada cubo contiene los datos que cambiaron desde la \u00faltima actualizaci\u00f3n. El tama\u00f1o del cubo determina la cantidad de datos que se procesan en cada iteraci\u00f3n. Un tama\u00f1o de cubo m\u00e1s peque\u00f1o significa que el flujo de datos procesa menos datos en cada iteraci\u00f3n, pero tambi\u00e9n significa que se requieren m\u00e1s iteraciones para procesar todos los datos. Un tama\u00f1o de cubo mayor significa que el flujo de datos procesa m\u00e1s datos en cada iteraci\u00f3n, pero tambi\u00e9n significa que se requieren menos iteraciones para procesar todos los datos.</li> <li>Valor m\u00e1ximo de columna. Esta configuraci\u00f3n es necesaria y especifica la columna que usa el flujo de datos para determinar si los datos han cambiado. El flujo de datos compara el valor m\u00e1ximo de esta columna con el valor m\u00e1ximo de la actualizaci\u00f3n anterior. Si se cambia el valor m\u00e1ximo, el flujo de datos recupera los datos que han cambiado desde la \u00faltima actualizaci\u00f3n. Si no se cambia el valor m\u00e1ximo, el flujo de datos no recupera ning\u00fan dato.</li> <li>Periodos Finalizados. Si seleccionamos esta opci\u00f3n se leer\u00e1n \u00fanicamente datos de periodos finalizados. </li> </ul> <p>Limitaciones: - Solo se admiten destinos basados en SQL - El n\u00famero m\u00e1ximo de cubos es de 50 para una tabla y 150 para el dataflow completo - Solo se admite el m\u00e9todo replace en el destino</p>"},{"location":"02%20Carga%20de%20Datos/02-1%20ELT%20con%20flujos%20de%20datos%20de%20Gen2/#fast-copy","title":"Fast Copy","text":"<p>Fast Copy es una funcionalidad que acelera la transferencia de datos en Fabric al utilizar procesamiento en paralelo y compresi\u00f3n eficiente.</p> <p>\ud83d\udccc Beneficios de Fast Copy: \u2714 Hasta 10 veces m\u00e1s r\u00e1pido que una copia normal. \u2714 Ideal para migraciones y cargas masivas de datos. \u2714 Compatible con Dataflows Gen2 y Pipelines de Data Factory.</p> <p>Para habilitar Fast Copy en un Dataflow Gen2:</p> <ol> <li>Seleccionar la fuente y destino de datos.</li> <li>Activar la opci\u00f3n \"Usar Fast Copy\" en la configuraci\u00f3n del Dataflow.</li> <li>Guardar y ejecutar el Dataflow.</li> </ol> <p>Requisitos: - Conectores y Transformaciones soportados - Para ficheros solo est\u00e1n soportados CSV y parquet almacenados en Azure Blob o ADLS de m\u00e1s de 100Mb de tama\u00f1o - Para or\u00edgenes SQL tablas con m\u00e1s de 5 millones de filas</p>"},{"location":"02%20Carga%20de%20Datos/02-1%20ELT%20con%20flujos%20de%20datos%20de%20Gen2/#4-hands-on-construccion-de-un-dataflow-gen2-desde-cero","title":"4. Hands-On: Construcci\u00f3n de un Dataflow Gen2 desde cero","text":""},{"location":"02%20Carga%20de%20Datos/02-1%20ELT%20con%20flujos%20de%20datos%20de%20Gen2/#objetivo-del-ejercicio","title":"\ud83d\udd39 Objetivo del ejercicio","text":"<p>Construir un Dataflow Gen2 que:</p> <ul> <li>Ingesta datos de una base de datos SQL Server en Azure en  OneLake.</li> <li>Realiza transformaciones de limpieza y c\u00e1lculo.</li> <li>Almacena el resultado en un Lakehouse en Fabric para su uso en Power BI.</li> </ul> <p>Los datos de conexi\u00f3n a la base de datos son: - Servidor:  srvvernedev.database.windows.net  - BBDD: SalesLT - Usuario: pruebas - Contrase\u00f1a: @@Secret0</p>"},{"location":"02%20Carga%20de%20Datos/02-1%20ELT%20con%20flujos%20de%20datos%20de%20Gen2/#pasos-del-ejercicio","title":"\ud83d\udd39 Pasos del ejercicio","text":"<p>1\ufe0f\u20e3 Crear un Dataflow Gen2 en Microsoft Fabric. 2\ufe0f\u20e3 A\u00f1adir conectores para SQL Server con los datos proporcionados</p> <p>Agregar las tablas: - SalesLT.Product - SalesLT.ProductCategory - SalesLT.Customer - SalesLT.SalesOrderDEtail - SalesLT.SalesOrderHeader</p> <p>Aseg\u00farate de mostrar todas las opciones de perfilado desde las opciones:</p> <p>![[Dataflow Perfiles de Datos.png]] 3\ufe0f\u20e3 Aplicar transformaciones en Power Query:</p> <ul> <li>En la tabla SalesLT.Product<ul> <li>quitar las columnas \"ThumbnailPhotoFileName\", \"ThumbNailPhoto\"</li> <li>Reemplazar los nulos de la columna Size por \"NA\"</li> <li>Expande la tabla SalesLT.ProductCategoy para llevar el campo Name a la tabla de SalesLT.Product</li> </ul> </li> <li>Renombra la tabla SalesLT.Product como Products</li> <li>Desmarca la opci\u00f3n de habilitar el almacenamiento temporal en la tablas en SAlesLT.ProductCategory</li> <li>En la tabla SalesLT.Customer<ul> <li>Quita las columnas \"Suffix\", \"NameStyle\", \"MiddleName\", \"PasswordHash\", \"PasswordSalt\"</li> <li>Renombra la tabla como Customers</li> </ul> </li> <li>En la tabla SalesLT.OrderDetail <ul> <li>Expande la tablas SalesLT.SalesOrderHeader para traer los campos de detalle, trae todos los campos</li> <li>Renombre la tabla como Sales</li> </ul> </li> <li> <p>Desmarca la opci\u00f3n de habilitar el almacenamiento temporal en las tablas en SalesLTSalesOrderHeader</p> <p>4\ufe0f\u20e3 Definir el destino en OneLake o Lakehouse. 5\ufe0f\u20e3 Ejecutar el Dataflow y validar resultados.</p> </li> </ul> <p>Abre el Modelo Sem\u00e1ntico creado e intenta generar un informe sobre esos datos</p> <p>\ud83d\udccc Resultado esperado: Los datos procesados estar\u00e1n disponibles para an\u00e1lisis en Power BI y otras herramientas dentro de Fabric\u200b.</p>"},{"location":"02%20Carga%20de%20Datos/02-1%20ELT%20con%20flujos%20de%20datos%20de%20Gen2/#5-conclusion-y-preguntas-clave","title":"5. Conclusi\u00f3n y Preguntas Clave","text":"<p>\u2705 Los Dataflows Gen2 permiten ingestar, transformar y automatizar procesos ETL en Fabric. \u2705 El uso de conectores nativos facilita la integraci\u00f3n con diversas fuentes de datos. \u2705 Las transformaciones avanzadas en Power Query mejoran la calidad y preparaci\u00f3n de los datos. \u2705 Las optimizaciones como Fast Copy e Incremental Refresh mejoran el rendimiento de los flujos de datos.</p>"},{"location":"02%20Carga%20de%20Datos/02-1%20ELT%20con%20flujos%20de%20datos%20de%20Gen2/#preguntas-de-reflexion","title":"Preguntas de reflexi\u00f3n","text":"<p>1\ufe0f\u20e3 \u00bfCu\u00e1ndo es mejor usar Dataflows Gen2 en lugar de Pipelines en Data Factory? 2\ufe0f\u20e3 \u00bfC\u00f3mo aprovechar Incremental Refresh para mejorar tiempos de procesamiento? 3\ufe0f\u20e3 \u00bfC\u00f3mo elegir el mejor destino (OneLake, Lakehouse o Data Warehouse) seg\u00fan el caso de uso?</p>"},{"location":"02%20Carga%20de%20Datos/02-2%20Orquestando%20Cargas%20con%20Pipelines%20de%20Data%20Factory/","title":"**Orquestando Cargas con Pipelines de Data Factory","text":""},{"location":"02%20Carga%20de%20Datos/02-2%20Orquestando%20Cargas%20con%20Pipelines%20de%20Data%20Factory/#1-introduccion-a-los-pipelines-en-microsoft-fabric","title":"1. Introducci\u00f3n a los Pipelines en Microsoft Fabric","text":"<p>La orquestaci\u00f3n de datos es un proceso fundamental en cualquier ecosistema de an\u00e1lisis de datos, permitiendo automatizar, programar y gestionar flujos de trabajo ETL (Extract, Transform, Load). Microsoft Fabric, a trav\u00e9s de Data Factory y sus Pipelines, proporciona una soluci\u00f3n escalable y flexible para la carga, transformaci\u00f3n y movimiento de datos en la nube.</p>"},{"location":"02%20Carga%20de%20Datos/02-2%20Orquestando%20Cargas%20con%20Pipelines%20de%20Data%20Factory/#por-que-es-importante-la-orquestacion-en-fabric","title":"\ud83d\udccc \u00bfPor qu\u00e9 es importante la Orquestaci\u00f3n en Fabric?","text":"<p>\u2714 Automatizaci\u00f3n del movimiento de datos entre diversas fuentes y destinos. \u2714 Gesti\u00f3n de dependencias entre actividades de carga y transformaci\u00f3n. \u2714 Monitorizaci\u00f3n y control de ejecuci\u00f3n con opciones de reintento y alertas. \u2714 Integraci\u00f3n con otros servicios como Power BI, Dataflows Gen2 y OneLake.</p>"},{"location":"02%20Carga%20de%20Datos/02-2%20Orquestando%20Cargas%20con%20Pipelines%20de%20Data%20Factory/#que-es-un-pipeline-en-microsoft-fabric","title":"\ud83d\udd39 \u00bfQu\u00e9 es un Pipeline en Microsoft Fabric?","text":"<p>Un Pipeline en Data Factory dentro de Microsoft Fabric es un flujo de trabajo automatizado que permite la ingesta, transformaci\u00f3n y movimiento de datos a trav\u00e9s de m\u00faltiples fuentes y destinos.</p> <p>Un Espacio de Trabajo de Microsoft Fabric  puede tener una o m\u00e1s canalizaciones. Una canalizaci\u00f3n es una agrupaci\u00f3n l\u00f3gica de actividades que juntas realizan una tarea. Por ejemplo, una canalizaci\u00f3n podr\u00eda contener un conjunto de actividades que ingieren y limpian datos de registro y luego inician un flujo de datos para analizar los datos de registro. La canalizaci\u00f3n nos permite gestionar las actividades como un conjunto en lugar de cada una individualmente. </p> <p>Caracter\u00edsticas clave de los Pipelines en Fabric: \u2705 Orquestaci\u00f3n de procesos ETL/ELT con l\u00f3gica condicional y ejecuci\u00f3n programada. \u2705 Integraci\u00f3n con m\u00faltiples fuentes como Azure SQL, OneLake, AWS S3, Google Cloud y m\u00e1s. \u2705 Automatizaci\u00f3n de flujos de datos entre Dataflows Gen2, Lakehouses y Data Warehouses. \u2705 Escalabilidad y paralelizaci\u00f3n, permitiendo el procesamiento eficiente de grandes vol\u00famenes de datos\u200b.</p>"},{"location":"02%20Carga%20de%20Datos/02-2%20Orquestando%20Cargas%20con%20Pipelines%20de%20Data%20Factory/#principales-caracteristicas-de-los-pipelines-en-fabric","title":"\ud83d\udccc Principales caracter\u00edsticas de los Pipelines en Fabric","text":"<p>\u2714 Definici\u00f3n visual o basada en c\u00f3digo: Se pueden construir desde una interfaz gr\u00e1fica o con JSON. \u2714 Activaci\u00f3n basada en eventos: Los pipelines pueden ejecutarse seg\u00fan reglas de negocio o disparadores temporales. \u2714 Escalabilidad: Soporta cargas de trabajo que van desde megabytes hasta petabytes de datos. \u2714 Integraci\u00f3n con m\u00faltiples fuentes: SQL, OneLake, Azure Data Lake, APIs, entre otros.</p> <p>\ud83d\udccc Ejemplo de caso de uso: Imagina que una empresa necesita extraer datos de ventas desde un sistema SAP, transformarlos y almacenarlos en un Data Warehouse en Fabric. Un Pipeline de Data Factory puede:  </p> <p>1\ufe0f\u20e3 Ejecutar un proceso de extracci\u00f3n desde SAP. 2\ufe0f\u20e3 Transformar los datos con un Notebook en Spark. 3\ufe0f\u20e3 Cargar los datos en OneLake para su an\u00e1lisis en Power BI.</p>"},{"location":"02%20Carga%20de%20Datos/02-2%20Orquestando%20Cargas%20con%20Pipelines%20de%20Data%20Factory/#principales-componentes-de-un-pipeline","title":"\ud83d\udd39 Principales Componentes de un Pipeline","text":"<p>Un pipeline de Data Factory en Microsoft Fabric se compone de:</p> <p>1\ufe0f\u20e3 Actividades: Representan pasos dentro del pipeline, como copia de datos, transformaci\u00f3n o ejecuci\u00f3n de scripts. 2\ufe0f\u20e3 Conjuntos de Datos: Conexiones a fuentes y destinos de datos. 3\ufe0f\u20e3 Flujos de Control: Reglas de ejecuci\u00f3n como condicionales y bucles. 4\ufe0f\u20e3 Eventos y Programadores: Permiten ejecutar el pipeline en funci\u00f3n de triggers espec\u00edficos\u200b.</p>"},{"location":"02%20Carga%20de%20Datos/02-2%20Orquestando%20Cargas%20con%20Pipelines%20de%20Data%20Factory/#actividades","title":"Actividades","text":"<p>Las actividades en una canalizaci\u00f3n definen acciones a realizar con los datos. Por ejemplo, podemos usar una actividad de copia para copiar datos de SQL Server a Azure Blob Storage. Luego, usaremos una actividad de Dataflow o una actividad de Notebook para procesar y transformar datos del almacenamiento de blobs a un grupo de Azure Synapse Analytics sobre el cual se crean soluciones de informes de inteligencia empresarial.</p> <p>Microsoft Fabric tiene tres tipos de actividades: actividades de movimiento de datos, actividades de transformaci\u00f3n de datos y actividades de control.</p>"},{"location":"02%20Carga%20de%20Datos/02-2%20Orquestando%20Cargas%20con%20Pipelines%20de%20Data%20Factory/#actividades-de-movimiento-de-datos","title":"Actividades de movimiento de datos","text":"<p>Realmente solo tenemos una actividad que es la actividad de copia de datos. Para copiar datos de un origen a un destino, el servicio que ejecuta la actividad de copia realiza estos pasos:</p> <ol> <li>Lee datos desde un almac\u00e9n de datos de origen.</li> <li>Realiza procesos de serializaci\u00f3n y deserializaci\u00f3n, compresi\u00f3n y descompresi\u00f3n, asignaci\u00f3n de columnas, etc. Realiza estas operaciones en funci\u00f3n de la configuraci\u00f3n.</li> <li>Escribe datos en el almac\u00e9n de datos de destino.</li> </ol>"},{"location":"02%20Carga%20de%20Datos/02-2%20Orquestando%20Cargas%20con%20Pipelines%20de%20Data%20Factory/#actividades-de-transformacion","title":"Actividades de transformaci\u00f3n","text":"<p>Son las actividades que nos permiten realizar transformaciones sobre los datos. Son las siguientes:</p> Actividad de transformaci\u00f3n de datos Entorno de procesos Copia de datos Administrador de proceso de Microsoft Fabric Dataflow Gen2 Administrador de proceso de Microsoft Fabric Eliminaci\u00f3n de datos Administrador de proceso de Microsoft Fabric Fabric Notebook Cl\u00fasteres de Apache Spark administrados por Microsoft Fabric Actividad de HDInsight Cl\u00fasteres de Apache Spark administrados por Microsoft Fabric Definici\u00f3n de trabajos de Spark Cl\u00fasteres de Apache Spark administrados por Microsoft Fabric Procedimiento almacenado Azure SQL, Azure Synapse Analytics o SQL Server Script de SQL Azure SQL, Azure Synapse Analytics o SQL Server ##### Actividades de Flujo de Control <p>Son las actividades que nos permiten modificar el flujo de ejecuci\u00f3n de nuestro pipeline, tales como Bucles, actividades, condicionales, notificaciones, etc.. </p>"},{"location":"02%20Carga%20de%20Datos/02-2%20Orquestando%20Cargas%20con%20Pipelines%20de%20Data%20Factory/#ejemplo-de-un-pipeline-en-fabric","title":"\ud83d\udd39 Ejemplo de un pipeline en Fabric","text":"<p>Un pipeline t\u00edpico puede extraer datos desde un ERP en SQL Server, transformarlos con un Notebook en Spark, y cargarlos en un Data Warehouse en Fabric para su consumo en Power BI\u200b.</p>"},{"location":"02%20Carga%20de%20Datos/02-2%20Orquestando%20Cargas%20con%20Pipelines%20de%20Data%20Factory/#2-actividad-de-copia-en-detalle","title":"2. Actividad de Copia en Detalle","text":""},{"location":"02%20Carga%20de%20Datos/02-2%20Orquestando%20Cargas%20con%20Pipelines%20de%20Data%20Factory/#comportamiento-de-la-actividad-de-copia","title":"\ud83d\udd39Comportamiento de la actividad de copia","text":"<p>En la actividad de copia de datos, podemos elegir entre los siguientes modos de carga de datos. - Modo de copia completa: cada trabajo de copia que se ejecuta copia todos los datos del origen al destino a la vez. - Modo de copia incremental: la ejecuci\u00f3n del trabajo inicial copia todos los datos y el trabajo posterior solo copia los cambios desde la \u00faltima ejecuci\u00f3n. Los datos modificados se anexan al almac\u00e9n de destino. En el modo de copia incremental, deberemos de seleccionar una columna incremental para cada tabla para identificar los cambios. El trabajo de copia usa esta columna como marca de agua, comparando su valor con el mismo desde la \u00faltima ejecuci\u00f3n para copiar solo los datos nuevos o actualizados. La columna incremental debe ser una marca de tiempo o un INT incremental.</p> <p>Tambi\u00e9n podemos elegir c\u00f3mo se escriben los datos en el almac\u00e9n de destino.</p> <p>De manera predeterminada, el trabajo de Copia\u00a0anexa\u00a0datos al destino, de modo que no se pierda ning\u00fan historial de cambios. Pero tambi\u00e9n podemos ajustar el comportamiento de escritura a\u00a0upsert\u00a0o\u00a0sobrescribir.</p> <ul> <li>Cuando se copian datos en el almac\u00e9n de almacenamiento: las nuevas filas de las tablas o archivos se copian en archivos nuevos en el destino. Si ya existe un archivo con el mismo nombre en el almac\u00e9n de destino, se sobrescribir\u00e1.</li> <li>Al copiar datos en una base de datos: las nuevas filas de las tablas o archivos se anexan a las tablas de destino. Podemos cambiar el comportamiento de escritura a upsert (en SQL DB o SQL Server) o sobrescribir (en tablas de Fabric Lakehouse).</li> </ul>"},{"location":"02%20Carga%20de%20Datos/02-2%20Orquestando%20Cargas%20con%20Pipelines%20de%20Data%20Factory/#escalabilidad-y-rendimiento-de-la-actividad-de-copia","title":"\ud83d\udd39 Escalabilidad y Rendimiento de la Actividad de Copia","text":"<p>Las canalizaciones de Data Factory ofrecen una arquitectura sin servidor que permite el paralelismo en distintos niveles. Esta arquitectura hace posible el desarrollo de canalizaciones que maximizan el rendimiento del movimiento de datos para nuestro entorno. Estas canalizaciones hacen un uso completo de los siguientes recursos:</p> <ul> <li>Ancho de banda de red entre los almacenes de datos de origen y destino</li> <li>Operaciones de entrada/salida por segundo (IOPS) y ancho de banda del almac\u00e9n de datos de origen o destino</li> </ul> <p>La copia puede escalarse en diferentes niveles:</p> <ul> <li>El\u00a0flujo de control\u00a0puede iniciar varias actividades de copia en paralelo, por ejemplo, mediante un\u00a0bucle ForEach.</li> <li>Una sola actividad de copia puede aprovechar los\u00a0recursos de proceso escalables.<ul> <li>Puede especificar la optimizaci\u00f3n de rendimiento inteligente como m\u00e1ximo para cada actividad de copia y sin servidor.</li> </ul> </li> <li>Una \u00fanica actividad de copia lee y escribe en el almac\u00e9n de datos\u00a0mediante varios subprocesos\u00a0en paralelo.</li> </ul> <p></p>"},{"location":"02%20Carga%20de%20Datos/02-2%20Orquestando%20Cargas%20con%20Pipelines%20de%20Data%20Factory/#copia-paralela","title":"Copia Paralela","text":"<p>Podemos establecer el valor de la propiedad \"Grado de paralelismo de copia\" en la configuraci\u00f3n de la actividad.  Esta propiedad se define como el n\u00famero m\u00e1ximo de subprocesos dentro de la actividad de copia. Los subprocesos operan en paralelo. Estos subprocesos leen desde el origen o escriben en los almacenes de datos de destino.</p> <p>La copia paralela es independiente de la configuraci\u00f3n de optimizaci\u00f3n de rendimiento inteligente. Para cada ejecuci\u00f3n de la actividad de copia, el servicio aplica din\u00e1micamente la configuraci\u00f3n de copia paralela \u00f3ptima en funci\u00f3n del patr\u00f3n de datos y el par de destino de origen.</p> <p>Para controlar la carga en las m\u00e1quinas que hospedan los almacenes de datos o para optimizar el rendimiento de la copia, podemos modificar el valor predeterminado y especificar un valor para el grado de paralelismo. El valor debe ser un entero mayor o igual que 1. En tiempo de ejecuci\u00f3n, y para obtener el mejor rendimiento, la actividad de copia usa un valor inferior o igual al valor que ha establecido.</p>"},{"location":"02%20Carga%20de%20Datos/02-2%20Orquestando%20Cargas%20con%20Pipelines%20de%20Data%20Factory/#optimizacion-inteligente-del-rendimiento","title":"Optimizaci\u00f3n Inteligente del rendimiento","text":"<p>La optimizaci\u00f3n del rendimiento inteligente permite al servicio optimizar el rendimiento mediante la combinaci\u00f3n de los factores de asignaci\u00f3n de recursos de CPU, memoria y red y el coste esperado de ejecutar una \u00fanica actividad de copia. Las opciones permitidas para habilitar una ejecuci\u00f3n de actividad de copia de forma inteligente son\u00a0Autom\u00e1tico, Est\u00e1ndar, Equilibrado, M\u00e1ximo. Tambi\u00e9n podemos especificar el valor\u00a0entre 4 y 256.</p> <p>En la tabla siguiente se muestra el valor recomendado en diferentes escenarios de copia:</p> <p>Expandir tabla</p> Valor Descripci\u00f3n Autom\u00e1tico Permitir que el servicio aplique din\u00e1micamente la optimizaci\u00f3n \u00f3ptima del rendimiento en funci\u00f3n del patr\u00f3n de datos y el par de destino de origen. Est\u00e1ndar Permitir que el servicio aplique din\u00e1micamente la optimizaci\u00f3n del rendimiento en los recursos de proceso est\u00e1ndar en funci\u00f3n del patr\u00f3n de datos y el par de destino de origen. Equilibrada Permitir que el servicio aplique din\u00e1micamente la optimizaci\u00f3n del rendimiento que equilibra el rendimiento y los recursos de proceso disponibles en funci\u00f3n del patr\u00f3n de datos y el par de destino de origen. M\u00e1xima Permitir que el servicio aplique din\u00e1micamente la optimizaci\u00f3n del rendimiento mediante el uso de los recursos de proceso m\u00e1ximos disponibles en funci\u00f3n del patr\u00f3n de datos y el par de destino de origen."},{"location":"02%20Carga%20de%20Datos/02-2%20Orquestando%20Cargas%20con%20Pipelines%20de%20Data%20Factory/#optimizacion-al-leer-del-origen","title":"Optimizaci\u00f3n al leer del origen","text":"<p>La actividad de copia tambi\u00e9n tiene configuraciones de optimizaci\u00f3n en la lectura de datos, que dependen de los or\u00edgenes de datos que utilicemos. Si nos fijamos en el caso concreto, por ejemplo de un origen SQL Server, vemos las siguientes opciones:</p> <p></p> <p>Al habilitar la copia con particiones, la actividad de copia ejecuta consultas en paralelo en el origen de SQL\u00a0Server para cargar los datos por particiones. El grado en paralelo se controla mediante el valor\u00a0<code>parallelCopies</code>\u00a0de la actividad de copia. Por ejemplo, si establecemos\u00a0<code>parallelCopies</code>\u00a0en cuatro, el servicio genera y ejecuta al mismo tiempo cuatro consultas de acuerdo con la configuraci\u00f3n y la opci\u00f3n de partici\u00f3n que hemos especificado, y cada consulta recupera una porci\u00f3n de datos de SQL\u00a0Server.</p>"},{"location":"02%20Carga%20de%20Datos/02-2%20Orquestando%20Cargas%20con%20Pipelines%20de%20Data%20Factory/#carga-incremental","title":"\ud83d\udd39 Carga Incremental","text":"<p>La actividad de copia, a diferencia del dataflow, no est\u00e1 orientada a aplicar transformaciones, por lo que no tenemos una configuraci\u00f3n concreta para la carga incremental. Sin embargo, esto depende de los tipo de origen de datos que manejemos. Por ejemplo, en caso de los ficheros tenemos esta configuraci\u00f3n:</p> <p></p> <p>Esta opci\u00f3n comprueba la fecha de modificaci\u00f3n de los ficheros, y si se encuentra en el intervalo especificado, los lee, y sino, los descarta. </p>"},{"location":"02%20Carga%20de%20Datos/02-2%20Orquestando%20Cargas%20con%20Pipelines%20de%20Data%20Factory/#3-automatizacion-de-cargas-elt-y-administracion-de-dependencias","title":"3. Automatizaci\u00f3n de cargas ELT y administraci\u00f3n de dependencias","text":""},{"location":"02%20Carga%20de%20Datos/02-2%20Orquestando%20Cargas%20con%20Pipelines%20de%20Data%20Factory/#automatizacion-de-cargas-en-fabric-data-factory","title":"\ud83d\udd39 Automatizaci\u00f3n de cargas en Fabric Data Factory","text":"<p>Los pipelines permiten configurar flujos de trabajo completamente automatizados, asegurando que los datos sean procesados sin intervenci\u00f3n manual.</p> <p>\ud83d\udccc M\u00e9todos de automatizaci\u00f3n en Data Factory: \u2705 Programaci\u00f3n basada en tiempo: Ejecuci\u00f3n de pipelines a intervalos definidos. \u2705 Eventos y Triggers: Disparo autom\u00e1tico cuando nuevos datos est\u00e1n disponibles. \u2705 Condicionales y Flujos de decisi\u00f3n: Dependencias entre tareas dentro del pipeline.</p> <p>En el caso concreto de los triggers y evento, estos se disparan creando una alerta sobre un eventstream de un Azure Blob Storage. Esta es la \u00fanica configuraci\u00f3n soportada por el momento. </p> <p></p>"},{"location":"02%20Carga%20de%20Datos/02-2%20Orquestando%20Cargas%20con%20Pipelines%20de%20Data%20Factory/#administracion-de-dependencias-en-pipelines","title":"\ud83d\udd39 Administraci\u00f3n de dependencias en pipelines","text":"<p>En escenarios donde las tareas deben ejecutarse en orden, es necesario definir dependencias entre actividades.</p> <p>\ud83d\udccc Tipos de dependencias en Data Factory:</p> <ul> <li>Encadenamiento de actividades: Una actividad solo inicia si la anterior se completa con \u00e9xito.</li> <li>Ejecuci\u00f3n paralela: Varias actividades pueden ejecutarse al mismo tiempo para optimizar rendimiento.</li> <li>Reintentos autom\u00e1ticos: Si una tarea falla, se pueden configurar intentos adicionales\u200b</li> </ul> <p>Las dependencias entre las actividades del pipeline, se configuran a la hora de conectar las actividades.</p> <p></p> <ul> <li>Flecha gris, para decidir que actividad ejecutar cuando se omita la ejecuci\u00f3n de la actividad actual</li> <li>Verde, para cuando se ejecute correctamente</li> <li>Roja para cuando falle</li> <li>Azul, siempre que finalice</li> </ul> <p>\ud83d\udd39 Ejemplo de un flujo ELT automatizado </p> <p>1\ufe0f\u20e3 Un Pipeline en Data Factory extrae datos de una API y almacena en OneLake. 2\ufe0f\u20e3 Se activa un Notebook en Spark para transformar los datos. 3\ufe0f\u20e3 Se ejecuta una consulta SQL para cargar datos en un Data Warehouse en Fabric. 4\ufe0f\u20e3 Power BI actualiza autom\u00e1ticamente los dashboards cuando los datos est\u00e1n listos\u200b</p>"},{"location":"02%20Carga%20de%20Datos/02-2%20Orquestando%20Cargas%20con%20Pipelines%20de%20Data%20Factory/#4-monitorizacion-de-procesos-de-carga-y-troubleshooting","title":"4. Monitorizaci\u00f3n de procesos de carga y troubleshooting","text":""},{"location":"02%20Carga%20de%20Datos/02-2%20Orquestando%20Cargas%20con%20Pipelines%20de%20Data%20Factory/#herramientas-de-monitorizacion-en-fabric-data-factory","title":"\ud83d\udd39 Herramientas de monitorizaci\u00f3n en Fabric Data Factory","text":"<p>Microsoft Fabric incluye herramientas avanzadas para monitorear la ejecuci\u00f3n de los pipelines y resolver errores r\u00e1pidamente.</p> <p>\ud83d\udccc Opciones de monitorizaci\u00f3n disponibles: \u2705 Fabric Monitor: Proporciona logs en tiempo real y estad\u00edsticas de ejecuci\u00f3n. \u2705 Registro de errores en ejecuci\u00f3n: Muestra detalles de cada actividad y su estado. \u2705 Alertas y notificaciones: Configuraci\u00f3n de alertas en caso de fallos\u200bfabric-fundamentals.</p>"},{"location":"02%20Carga%20de%20Datos/02-2%20Orquestando%20Cargas%20con%20Pipelines%20de%20Data%20Factory/#uso-de-copilot-para-troubleshooting","title":"\ud83d\udd39 Uso de Copilot para Troubleshooting","text":"<p>Fabric Data Factory incluye Copilot (a partir de F64), una herramienta de IA que ayuda a identificar y solucionar problemas en pipelines\u200b.</p> <p>\ud83d\udccc Ejemplo de troubleshooting con Copilot: 1\ufe0f\u20e3 Se detecta una falla en la actividad de copia de datos. 2\ufe0f\u20e3 Copilot analiza los logs y proporciona una descripci\u00f3n del error. 3\ufe0f\u20e3 Se sugiere una soluci\u00f3n, como ajustar el formato del archivo fuente o modificar permisos en la base de datos\u200bfabric-fundamentals.</p> <p>\ud83d\udd39 Ejemplo real de monitorizaci\u00f3n Una empresa usa Fabric para integrar datos de distintos departamentos. Un pipeline programado diariamente falla intermitentemente debido a cambios en la estructura de los archivos de origen. Fabric Monitor y Copilot identifican el problema y sugieren una transformaci\u00f3n autom\u00e1tica para corregirlo\u200b.</p>"},{"location":"02%20Carga%20de%20Datos/02-2%20Orquestando%20Cargas%20con%20Pipelines%20de%20Data%20Factory/#5-hands-on-orquestando-procesos","title":"5. Hands-On: Orquestando procesos","text":""},{"location":"02%20Carga%20de%20Datos/02-2%20Orquestando%20Cargas%20con%20Pipelines%20de%20Data%20Factory/#objetivo-del-ejercicio","title":"\ud83d\udd39 Objetivo del ejercicio","text":"<p>En este laboratorio, el objetivo es entender como orquestar realmente las ingestas de datos desde \"el exterior\" a Microsoft Fabric, generando las transformacions con Dataflows de Generaci\u00f3n 2, y orquestando el proceso con la actividad de Dataflows de las Pipelines. Utilizaremos el Dataflow generado en el ejercicio anterior</p>"},{"location":"02%20Carga%20de%20Datos/02-2%20Orquestando%20Cargas%20con%20Pipelines%20de%20Data%20Factory/#pasos-del-ejercicio","title":"\ud83d\udd39 Pasos del ejercicio","text":"<ol> <li>Crea un elemento de tipo Data pipeline. </li> <li>En la opci\u00f3n de Comenzar con un lienzo en blanco, selecciona, Actividad de canalizaci\u00f3n y selecciona Flujo de Datos. </li> <li>En la pesta\u00f1a de configuraci\u00f3n, selecciona el Dataflow generado en el ejercicio anterior</li> <li>Agrega una actividad de Teams y con\u00e9ctala cuando finalice correctamente la actividad de Flujo de Datos. </li> <li>Desde la pesta\u00f1a de Configuraci\u00f3n inicia sesi\u00f3n en Teams</li> <li>En Post-In selecciona donde quieres publicar. Tenemos dos opciones: Canal o Chat de Grupo</li> <li>Comp\u00f3n un mensaje </li> <li>Ejecuta el pipeline. Deber\u00eda de ejecutarse correctamente y llegarte el mensaje a Teams</li> </ol>"},{"location":"02%20Carga%20de%20Datos/02-2%20Orquestando%20Cargas%20con%20Pipelines%20de%20Data%20Factory/#6-conclusion-y-preguntas-clave","title":"6. Conclusi\u00f3n y Preguntas Clave","text":"<p>\u2705 Los Pipelines en Data Factory permiten automatizar flujos ETL y orquestar cargas de datos. \u2705 Las dependencias entre actividades aseguran que las cargas de datos se ejecuten de forma ordenada. \u2705 Las herramientas de monitoreo y troubleshooting facilitan la detecci\u00f3n y correcci\u00f3n de errores. \u2705 El uso de Copilot permite optimizar el mantenimiento y resoluci\u00f3n de problemas en pipelines.</p>"},{"location":"02%20Carga%20de%20Datos/02-2%20Orquestando%20Cargas%20con%20Pipelines%20de%20Data%20Factory/#preguntas-de-reflexion","title":"Preguntas de reflexi\u00f3n","text":"<ol> <li>\u00bfCu\u00e1les son las ventajas de usar triggers en vez de programaciones manuales en un pipeline?</li> <li>\u00bfC\u00f3mo se pueden optimizar los pipelines para manejar grandes vol\u00famenes de datos?</li> <li>\u00bfC\u00f3mo se puede aprovechar Copilot para mejorar la depuraci\u00f3n de errores en pipelines?</li> </ol>"},{"location":"02%20Carga%20de%20Datos/02-3%20Modelado%20Dimensional/","title":"Modelado Dimensional","text":"<p>Cuando se est\u00e1 trabajando en proyectos de anal\u00edtica de datos al uso, lo m\u00e1s habitual es que nuestra capa de servicio o capa gold, aquella destina a servir ya finalmente a los usuarios que necesiten conectarse directamente a nuestros datos para sus an\u00e1lisis, o los modelos sem\u00e1nticos de Power BI, es muy habitual que se sigue un modelo en estrella para </p>"},{"location":"02%20Carga%20de%20Datos/02-3%20Modelado%20Dimensional/#1-por-que-es-importante-el-modelado-dimensional","title":"1. \u00bfPor qu\u00e9 es importante el Modelado Dimensional?","text":"<p>Imagina un supermercado donde m\u00faltiples procesos ocurren simult\u00e1neamente:</p> <ul> <li>Cajeros registrando compras.</li> <li>Administradores ingresando facturas.</li> <li>Gerentes consultando ventas en tiempo real.</li> </ul> <p>Dado que muchas personas leen y escriben informaci\u00f3n al mismo tiempo, la base de datos debe estar dise\u00f1ada para soportar este acceso concurrente de manera eficiente.</p>"},{"location":"02%20Carga%20de%20Datos/02-3%20Modelado%20Dimensional/#diferencias-entre-oltp-y-olap","title":"\ud83d\udd39Diferencias entre OLTP y OLAP","text":""},{"location":"02%20Carga%20de%20Datos/02-3%20Modelado%20Dimensional/#bases-de-datos-transaccionales-oltp","title":"Bases de datos transaccionales (OLTP)","text":"<p>Las bases de datos operacionales (OLTP) est\u00e1n dise\u00f1adas para manejar transacciones r\u00e1pidas y frecuentes, como ventas, registros de clientes y actualizaciones de inventario. Se caracterizan por:  \u2705 Alta concurrencia (muchos usuarios escribiendo datos).  \u2705 Estructura altamente normalizada para evitar redundancia.  \u2705 Optimizaci\u00f3n para inserciones y actualizaciones r\u00e1pidas.</p> <p></p>"},{"location":"02%20Carga%20de%20Datos/02-3%20Modelado%20Dimensional/#modelado-dimensional-para-analisis-olap","title":"Modelado Dimensional para an\u00e1lisis (OLAP)","text":"<p>El Modelado Dimensional se centra en la optimizaci\u00f3n para el an\u00e1lisis de datos, consolidando informaci\u00f3n en estructuras de f\u00e1cil acceso. Sus caracter\u00edsticas son:  \u2705 Desnormalizaci\u00f3n para mejorar la velocidad de consulta.  \u2705 Optimizaci\u00f3n para reportes y an\u00e1lisis hist\u00f3ricos.  \u2705 Uso de tablas de hechos y dimensiones.</p>"},{"location":"02%20Carga%20de%20Datos/02-3%20Modelado%20Dimensional/#2-pasos-para-construir-un-modelo-dimensional","title":"2. Pasos para construir un modelo dimensional","text":"<p>1\ufe0f\u20e3 Identificar el proceso de negocio a analizar \ud83d\udccc Ejemplo: An\u00e1lisis de ventas en un supermercado.</p> <p>2\ufe0f\u20e3 Determinar la granularidad de los datos \ud83d\udccc Nivel de detalle con el que se almacenan los datos (ejemplo: ventas por d\u00eda, por tienda, por producto).</p> <p>3\ufe0f\u20e3 Definir dimensiones y atributos \ud83d\udccc Crear tablas de dimensiones como Tiempo, Producto, Cliente, Ubicaci\u00f3n.</p> <p>4\ufe0f\u20e3 Construir la tabla de hechos \ud83d\udccc Contiene las m\u00e9tricas clave (Ej: cantidad de ventas, ingresos totales).</p>"},{"location":"02%20Carga%20de%20Datos/02-3%20Modelado%20Dimensional/#3principales-enfoques-de-modelado-dimensional","title":"3.Principales enfoques de Modelado Dimensional","text":""},{"location":"02%20Carga%20de%20Datos/02-3%20Modelado%20Dimensional/#enfoque-de-inmon-top-down","title":"\ud83d\udd39 Enfoque de Inmon (Top-Down)","text":"<ul> <li>Se construye un Data Warehouse corporativo antes de crear Data Marts.</li> <li>Priorizado en integridad y calidad de datos.</li> <li>Adecuado para grandes empresas con m\u00faltiples \u00e1reas de negocio.</li> <li>Arquitectura centralizada que luego se desglosa en Data Marts.</li> </ul>"},{"location":"02%20Carga%20de%20Datos/02-3%20Modelado%20Dimensional/#enfoque-de-kimball-bottom-up","title":"\ud83d\udd39 Enfoque de Kimball (Bottom-Up)","text":"<ul> <li>Se crean Data Marts espec\u00edficos para procesos clave del negocio.</li> <li>Cada Data Mart est\u00e1 optimizado para reportes y an\u00e1lisis r\u00e1pidos.</li> <li>Se integran gradualmente en una estructura de Data Warehouse.</li> </ul>"},{"location":"02%20Carga%20de%20Datos/02-3%20Modelado%20Dimensional/#modelo-hibrido","title":"\ud83d\udd39 Modelo H\u00edbrido","text":"<ul> <li>Combina la velocidad del enfoque Kimball con la integraci\u00f3n del Inmon.</li> <li>Permite una implementaci\u00f3n \u00e1gil con una estrategia de consolidaci\u00f3n a largo plazo.</li> </ul>"},{"location":"02%20Carga%20de%20Datos/02-3%20Modelado%20Dimensional/#4elementos-clave-del-modelado-dimensional","title":"4.Elementos clave del Modelado Dimensional","text":""},{"location":"02%20Carga%20de%20Datos/02-3%20Modelado%20Dimensional/#tablas-de-hechos","title":"\ud83d\udd39 Tablas de Hechos","text":"<p>Contienen m\u00e9tricas o hechos que representan eventos o transacciones. Caracter\u00edsticas:</p> <ul> <li>Altas y estrechas (muchos registros, pocas columnas).</li> <li>Clave primaria compuesta por claves for\u00e1neas de las dimensiones.</li> <li>Ejemplo: Tabla de Ventas con columnas de cantidad, precio total, ID del producto, fecha.</li> </ul>"},{"location":"02%20Carga%20de%20Datos/02-3%20Modelado%20Dimensional/#tablas-de-dimension","title":"\ud83d\udd39 Tablas de Dimensi\u00f3n","text":"<p>Representan entidades descriptivas como clientes, productos o fechas. Caracter\u00edsticas:</p> <ul> <li>Anchas y cortas (menos registros, muchas columnas descriptivas).</li> <li>Desnormalizadas para mejorar el rendimiento.</li> <li>Contienen jerarqu\u00edas (Ej: A\u00f1o \u2192 Mes \u2192 D\u00eda).</li> </ul> <p>Ejemplo: Dimensi\u00f3n Producto</p> ID_Producto Nombre Categor\u00eda Marca 101 Laptop Electr\u00f3nica Dell 102 Tel\u00e9fono M\u00f3viles Samsung"},{"location":"02%20Carga%20de%20Datos/02-3%20Modelado%20Dimensional/#5modelos-de-esquemas-dimensionales","title":"5.Modelos de esquemas dimensionales","text":""},{"location":"02%20Carga%20de%20Datos/02-3%20Modelado%20Dimensional/#esquema-en-estrella","title":"\ud83d\udd39 Esquema en Estrella","text":"<ul> <li>Cada dimensi\u00f3n se almacena en una \u00fanica tabla.</li> <li>La tabla de hechos se relaciona directamente con cada dimensi\u00f3n.</li> <li>Ventajas: R\u00e1pido para consultas, f\u00e1cil de entender y usar.</li> </ul>"},{"location":"02%20Carga%20de%20Datos/02-3%20Modelado%20Dimensional/#esquema-en-copo-de-nieve","title":"\ud83d\udd39 Esquema en Copo de Nieve","text":"<ul> <li>Las dimensiones est\u00e1n normalizadas en m\u00faltiples tablas.</li> <li>Ventajas: Reducci\u00f3n de redundancia, pero con mayor complejidad de consultas.</li> </ul>"},{"location":"02%20Carga%20de%20Datos/02-3%20Modelado%20Dimensional/#6patron-scd-slowly-changing-dimension","title":"6.Patr\u00f3n SCD (Slowly Changing Dimension)","text":"<p>Como hemos comentado anteriormente, la informaci\u00f3n de los sistemas transaccionales puede ser modificada, aunque \u00e9stos s\u00f3lo guardan la \u00faltima versi\u00f3n. Por el contrario en un Data Warehouse, debemos reflejar ese historial de cambios para mostrar la verdad que hab\u00eda en el momento en que se produjeron los hechos.</p>"},{"location":"02%20Carga%20de%20Datos/02-3%20Modelado%20Dimensional/#seguimientos-de-cambios-en-las-dimensiones","title":"\ud83d\udd39 Seguimientos de Cambios en las dimensiones","text":"<p>Veamos un ejemplo. Si en nuestro sistema transaccional asociamos cada venta al comercial que la realiza, y \u00e9ste a su vez depende de un director de zona. En la tabla de ventas queda reflejado el comercial que realiza la venta, y en la tabla del empleado se almacena el director de zona del que depende, ya que tenemos los datos normalizados. \u00bfQu\u00e9 ocurre si un comercial, por cualquier motivo, bien personal o bien laboral, le cambian la zona asignada?, \u00bfY si adem\u00e1s la nueva zona depende de otro director de zona?, \u00bfY qu\u00e9 ocurre si sacamos un informe de ventas de ese nuevo director de zona? Pues que se le han trasladado a \u00e9l todas las ventas que ha hecho este comercial durante toda su vida laboral en la empresa. Esto no es real, e imaginamos que su antiguo jefe de zona no estar\u00e1 en absoluto de acuerdo con estos informes de ventas, adem\u00e1s de que no son ciertos. Cuando dise\u00f1amos un Data Warehouse debemos evitar esta problem\u00e1tica que tenemos en muchos sistemas transaccionales, donde s\u00f3lo tenemos la versi\u00f3n actual de los datos. Para ello hay una serie de t\u00e9cnicas que nos permiten ir detectando los cambios que ocurren en el transaccional y dej\u00e1ndolos reflejados. Volviendo con el ejemplo anterior, en la tabla de dimensiones se deber\u00edan tener dos filas (o versiones) del empleado, una en la que se indica cu\u00e1l es su jefe de zona antiguo, y durante qu\u00e9 periodo ha sido su jefe de zona, y otra que indica cu\u00e1l es su jefe actual y desde cu\u00e1ndo. Adicionalmente, cada una de las ventas debe estar apuntando a la versi\u00f3n correcta del comercial, es decir, las ventas deben apuntar a la versi\u00f3n del comercial correspondiente al momento en que se produjeron, quedando as\u00ed reflejado el jefe de zona y la zona que realmente ten\u00eda asignados en el momento de cada venta.</p> <p>Por el contrario, hay otros casos en los que no necesito reflejar el historial, por ejemplo, si corrijo el nombre de dicho comercial porque lo ten\u00eda mal escrito, no quiero tener dos versiones de \u00e9l, una con el nombre mal escrito y otra con \u00e9l bien escrito, sino que quiero que se sobrescriba y siempre aparezca la versi\u00f3n actual que es donde est\u00e1 escrito correctamente.</p>"},{"location":"02%20Carga%20de%20Datos/02-3%20Modelado%20Dimensional/#tipos-de-scd","title":"\ud83d\udd39 Tipos de SCD","text":"<p>Vistos estos ejemplos, pasemos a describir los diferentes tipos de SCD m\u00e1s habituales. Existe otros muchos, porque la combinatoria de lo que necesitemos hacer puede ser variada, pero para simplificar, los dos m\u00e1s habituales y b\u00e1sicos son :</p> <ul> <li> <p>SCD tipo 1, Sobrescritura: la nueva informaci\u00f3n sobrescribe a la antigua, no se guardan hist\u00f3ricos y s\u00f3lo se tiene la versi\u00f3n actual. Dicha sobre escritura se produce cuando se detecta alg\u00fan error en los valores para corregirlo y mejorar la calidad del dato. Desde el punto de vista anal\u00edtico s\u00f3lo interesa la versi\u00f3n actual.</p> </li> <li> <p>SCD tipo 2, historial de cambios: refleja toda la informaci\u00f3n hist\u00f3rica. Por cada cambio que se produzca, se crea una nueva fila en la tabla de dimensiones con la fecha de inicio y una nueva clave subrogada, y se marca la fecha de fin de la versi\u00f3n anterior. Cada hecho que entra, debe comprobar a qu\u00e9 versi\u00f3n de la fila en la tabla de dimensiones se debe asociar (qu\u00e9 clave subrogada debe almacenar) en funci\u00f3n de la fecha en la que se produzca.</p> </li> </ul> <p>Para dar soporte a este tipo de seguimiento de cambios para las SCD2 , existen varias maneras, pero la m\u00e1s habitual, utiliza fechas de inicio y fin que indican la validez, por lo que la estructura de nuestras dimensiones quedar\u00eda compuesta por :</p> <ul> <li>Clave subrogada: Es la clave principal de la tabla de dimensiones. Nos permite identificar de forma \u00fanica cada fila, suele ser un entero auto- incremental. Es totalmente transparente al usuario de negocio, no la usar\u00e1 en ning\u00fan momento, ni tan siquiera tendr\u00e1 conocimiento de su existencia.</li> <li>Clave de negocio: Es la clave con la que trabaja habitualmente el usuario, pero no puede ser la clave principal porque se pueden producir duplicidades.</li> <li>Atributos de la dimensi\u00f3n: ser\u00e1n cada una de las caracter\u00edsticas que necesitemos almacenar. Lo habitual es que haya varias decenas de ellos, incluso que en algunos casos superen el centenar. -</li> <li>Fecha de Inicio y Fecha de Fin: Servir\u00e1n para conocer el periodo de vigencia de cada una de las versiones de los atributos.</li> </ul>"},{"location":"02%20Carga%20de%20Datos/02-3%20Modelado%20Dimensional/#hands-on-implementando-scd2","title":"\ud83d\udd39 Hands On: Implementando SCD2","text":"<p>Para este ejemplo utilizaremos dos ficheros .csv muy simples, que puedes crearte directamente, y que por agilidad, y para ver alguna opci\u00f3n adicional, alojaremos en SharePoint o Onedrive para as\u00ed poder modificarlos f\u00e1cilmente y ver el funcionamiento de este seguimiento de cambios. Adicionalmente a eso, vamos a implementar una arquitectura m\u00e1s compleja, donde crearemos un lakehouse con esos cargados, para posteriormente simular un esquema en estrella en un datawarehouse.  Los fichero ser\u00edan:</p> <p></p> <p></p> <p>Puedes acceder a los archivos desde esta ruta si no quieres crearlos CursoFabric  y as\u00ed puedes copiarlos directamente en tu Onedrive. </p> <p>Una vez tenemos los ficheros creamos: - Un lakehouse que llamaremos lakescd2 y un datawarehouse que llamaremos dwscd2 - Crearemos un Dataflow Gen2, con el que leeremos ambos ficheros y simplemente los copiaremos (convirtiendo los tipos de datos) al lakehouse, como tablas.  - En el Dataflow, utilizaremos un origen de Texto/CSV y navegaremos a la carpeta de Onedrive donde los hayamos generado, y creamos las dos consultas, para clientes y para ventas.  - Seleccionamos como destino de ambos, el lakehouse lakescd2  - Como siguiente paso, vamos a nuestro dwscd2 y creamos el esquema necesario: <pre><code>CREATE SCHEMA [dim]\nGO\nCREATE SCHEMA [fact]\nGO\nCREATE TABLE [dim].[clientes]\n\u00a0 \u00a0 (clientesk int,\n\u00a0 \u00a0 idcliente int,\n\u00a0 \u00a0 nombre varchar(500),\n\u00a0 \u00a0 zona varchar(100),\n\u00a0 \u00a0 fechainicio DATE,\n\u00a0 \u00a0 fechafin DATE)\nGO\nCREATE TABLE [fact].[ventas]\n\u00a0 \u00a0 (idventa int,\n\u00a0 \u00a0 icliente int,\n\u00a0 \u00a0 catagoria varchar(500),\n\u00a0 \u00a0 importe FLOAT)\n</code></pre></p> <ul> <li>Una vez tenemos el esquema generado, ahora podemos cargar los datos desde el lakehouse, al datawarehouse</li> </ul> <p></p> <p>Para el resto del laboratorio, seguiremos el patr\u00f3n que est\u00e1 documentado en este ejemplo: https://learn.microsoft.com/en-us/fabric/data-factory/slowly-changing-dimension-type-two </p>"},{"location":"02%20Carga%20de%20Datos/02-3%20Modelado%20Dimensional/#7importancia-del-modelado-dimensional","title":"7.Importancia del Modelado Dimensional","text":"<p>\u2705 Simplifica el acceso y an\u00e1lisis de datos. \u2705 Optimiza el rendimiento de consultas y reportes. \u2705 Facilita la integraci\u00f3n con herramientas de BI. \u2705 Permite la reutilizaci\u00f3n de estructuras de datos. \u2705 Asegura que los datos soporten las necesidades del negocio.</p>"},{"location":"02%20Carga%20de%20Datos/02-4%20Desarrollo%20con%20Apache%20Spark/","title":"Desarrollo con Apache Spark","text":"<p>Apache Spark es un motor de procesamiento distribuido de c\u00f3digo abierto dise\u00f1ado para procesamiento de datos a gran escala. Se ejecuta en clusters y permite el an\u00e1lisis de datos en tiempo real y batch, proporcionando APIs en Python (PySpark), Scala, Java, R y SQL.</p> <p>\u2714 Alta velocidad gracias a su ejecuci\u00f3n en memoria. \u2714 Escalabilidad para procesar desde megabytes hasta petabytes de datos. \u2714 Compatibilidad con m\u00faltiples fuentes de datos, como OneLake, Data Warehouses, Data Lakes y bases de datos SQL.</p> <p>En Microsoft Fabric, Spark es una pieza clave del Data Engineering y se integra con otros servicios como Data Factory, OneLake, Power BI y Data Science.</p>"},{"location":"02%20Carga%20de%20Datos/02-4%20Desarrollo%20con%20Apache%20Spark/#1-como-se-integra-apache-spark-en-microsoft-fabric","title":"**1. \u00bfC\u00f3mo se integra Apache Spark en Microsoft Fabric?","text":"<p>En Microsoft Fabric, Spark se ejecuta en un entorno completamente administrado, eliminando la necesidad de configurar clusters manualmente.</p>"},{"location":"02%20Carga%20de%20Datos/02-4%20Desarrollo%20con%20Apache%20Spark/#componentes-clave","title":"\ud83d\udd39 Componentes Clave","text":"<p>\u2714 Clusters de Spark administrados: Se asignan din\u00e1micamente recursos seg\u00fan la carga de trabajo. \u2714 Pools de Spark: Grupos de recursos donde se ejecutan notebooks y trabajos de Spark.  \u2714 Notebooks de Fabric: Espacios interactivos para escribir c\u00f3digo en PySpark, Scala y SQL. \u2714 OneLake: Almacenamiento unificado para datos procesados por Spark. \u2714 Integraci\u00f3n con Data Factory: Automatizaci\u00f3n de cargas y ejecuci\u00f3n de pipelines.</p> <p>\ud83d\udccc Ejemplo: Si una empresa quiere procesar grandes vol\u00famenes de datos de ventas, Spark puede leer los datos desde OneLake, aplicar transformaciones con PySpark y guardar el resultado en un Data Warehouse para su an\u00e1lisis en Power BI.</p>"},{"location":"02%20Carga%20de%20Datos/02-4%20Desarrollo%20con%20Apache%20Spark/#2-conceptos-clave-de-spark-compute","title":"2. Conceptos Clave de Spark Compute","text":""},{"location":"02%20Carga%20de%20Datos/02-4%20Desarrollo%20con%20Apache%20Spark/#compute-engines","title":"\ud83d\udd39Compute Engines","text":"<p>Microsoft Fabric permite ejecutar Spark en un entorno administrado donde los clusters se crean y gestionan autom\u00e1ticamente. Esto permite a los usuarios centrarse en el desarrollo sin preocuparse por la infraestructura subyacente.</p>"},{"location":"02%20Carga%20de%20Datos/02-4%20Desarrollo%20con%20Apache%20Spark/#tipos-de-jobs-en-fabric-spark","title":"\ud83d\udd39Tipos de Jobs en Fabric Spark","text":"<ul> <li>Batch Jobs: Se utilizan para procesamiento de datos en grandes vol\u00famenes.    </li> <li>Streaming Jobs: Permiten el procesamiento en tiempo real de datos en movimiento.    </li> <li>Notebooks y Pipelines: Usados para desarrollo interactivo y modelado de datos.    </li> </ul>"},{"location":"02%20Carga%20de%20Datos/02-4%20Desarrollo%20con%20Apache%20Spark/#3-configuracion-y-gestion-de-pools-de-spark","title":"3. Configuraci\u00f3n y Gesti\u00f3n de Pools de Spark","text":""},{"location":"02%20Carga%20de%20Datos/02-4%20Desarrollo%20con%20Apache%20Spark/#pools-predeterminados-starter-pools","title":"\ud83d\udd39Pools Predeterminados (Starter Pools)","text":"<p>Los Starter Pools en Microsoft Fabric son grupos preconfigurados de recursos de Spark que permiten ejecutar trabajos de manera r\u00e1pida sin necesidad de configuraci\u00f3n avanzada.</p> <p>Ventajas:</p> <ul> <li>No requieren configuraci\u00f3n manual.    </li> <li>Son ideales para entornos de desarrollo y pruebas.    </li> <li>Ofrecen un rendimiento \u00f3ptimo para cargas de trabajo moderadas.    </li> </ul>"},{"location":"02%20Carga%20de%20Datos/02-4%20Desarrollo%20con%20Apache%20Spark/#creacion-de-pools-personalizados","title":"\ud83d\udd39Creaci\u00f3n de Pools Personalizados","text":"<p>Para cargas de trabajo m\u00e1s complejas, es posible crear pools de Spark personalizados con configuraciones espec\u00edficas.</p> <p>Pasos para la configuraci\u00f3n:</p> <ol> <li>Definir el tama\u00f1o del pool de nodos.    </li> <li>Seleccionar el tipo de instancias de Spark (RAM/CPU).    </li> <li>Configurar l\u00edmites de concurrencia y asignaci\u00f3n de memoria.    </li> <li>Asociar el pool a una capacidad de Fabric.    </li> </ol> <p></p>"},{"location":"02%20Carga%20de%20Datos/02-4%20Desarrollo%20con%20Apache%20Spark/#facturacion-y-administracion-de-capacidad","title":"\ud83d\udd39Facturaci\u00f3n y Administraci\u00f3n de Capacidad","text":"<p>Microsoft Fabric usa un modelo de cobro por uso, donde los recursos de Spark consumen unidades de capacidad (CU). Esta es la tabla con los valores predeterminados que tienen los pools de Spark dependiendo de la SKU. </p> <p></p> <ul> <li>Estrategias de optimizaci\u00f3n de costes: <ul> <li>Utilizar pools compartidos para trabajos recurrentes.        </li> <li>Configurar l\u00edmites de uso en trabajos Spark.        </li> <li>Monitorear el consumo con herramientas de Fabric.        </li> </ul> </li> </ul> <p>Para un pool starter, estas son las tareas que se facturan y las que no. En el caso de un pool personalizado, simplemente desaparece el estado \"Pool Idle\". </p> <p></p>"},{"location":"02%20Carga%20de%20Datos/02-4%20Desarrollo%20con%20Apache%20Spark/#4-gestion-de-concurrencia-y-encolado-de-trabajos","title":"4. Gesti\u00f3n de Concurrencia y Encolado de Trabajos","text":""},{"location":"02%20Carga%20de%20Datos/02-4%20Desarrollo%20con%20Apache%20Spark/#modo-de-alta-concurrencia","title":"\ud83d\udd39Modo de Alta Concurrencia","text":"<p>El modo de alta concurrencia en Fabric permite ejecutar m\u00faltiples trabajos Spark simult\u00e1neamente dentro de un mismo pool de recursos.</p> <p>Beneficios:</p> <ul> <li>Mayor eficiencia en la ejecuci\u00f3n de tareas paralelas.    </li> <li>Menos sobrecarga en la creaci\u00f3n de clusters.    </li> <li>Uso m\u00e1s eficiente de los recursos disponibles.    </li> </ul>"},{"location":"02%20Carga%20de%20Datos/02-4%20Desarrollo%20con%20Apache%20Spark/#administracion-de-admision-de-trabajos","title":"\ud83d\udd39Administraci\u00f3n de Admisi\u00f3n de Trabajos","text":"<p>Fabric permite configurar pol\u00edticas de admisi\u00f3n de trabajos para controlar la cantidad de tareas en ejecuci\u00f3n simult\u00e1nea y evitar sobrecargas.</p> <p>Opciones disponibles:</p> <ul> <li>Asignar prioridades a diferentes tipos de trabajos.    </li> <li>Limitar el n\u00famero de trabajos simult\u00e1neos.    </li> <li>Definir reglas de asignaci\u00f3n de recursos.    </li> </ul>"},{"location":"02%20Carga%20de%20Datos/02-4%20Desarrollo%20con%20Apache%20Spark/#encolado-de-trabajos","title":"\ud83d\udd39Encolado de Trabajos","text":"<p>Cuando los recursos est\u00e1n ocupados, los nuevos trabajos ingresan a una cola de espera hasta que haya capacidad disponible.</p> <p>Buenas pr\u00e1cticas:</p> <ul> <li>Priorizar trabajos cr\u00edticos con etiquetas de urgencia.    </li> <li>Definir l\u00edmites de tiempo de espera para evitar bloqueos.    </li> <li>Monitorear el estado de la cola para ajustes din\u00e1micos.    </li> </ul>"},{"location":"02%20Carga%20de%20Datos/02-4%20Desarrollo%20con%20Apache%20Spark/#5-optimizacion-y-autoajuste-en-spark","title":"5. Optimizaci\u00f3n y Autoajuste en Spark","text":""},{"location":"02%20Carga%20de%20Datos/02-4%20Desarrollo%20con%20Apache%20Spark/#tecnicas-de-optimizacion","title":"\ud83d\udd39T\u00e9cnicas de Optimizaci\u00f3n","text":"<p>Para mejorar el rendimiento de trabajos Spark en Fabric, se pueden aplicar las siguientes estrategias:</p> <ul> <li>Particionamiento de Datos: Para paralelizar el procesamiento.    </li> <li>Broadcast Join: Para evitar movimientos de datos innecesarios.    </li> <li>Uso de Cache: Para almacenar datos frecuentemente utilizados.    </li> <li>Compresi\u00f3n de Datos: Para reducir el tiempo de procesamiento.    </li> </ul>"},{"location":"02%20Carga%20de%20Datos/02-4%20Desarrollo%20con%20Apache%20Spark/#uso-de-autotune","title":"\ud83d\udd39Uso de AutoTune","text":"<p>Fabric ofrece AutoTune, una funcionalidad que ajusta autom\u00e1ticamente los par\u00e1metros de ejecuci\u00f3n de Spark para maximizar la eficiencia.</p> <p>\u00bfC\u00f3mo funciona?</p> <ol> <li>Analiza patrones de ejecuci\u00f3n de trabajos anteriores.    </li> <li>Ajusta din\u00e1micamente par\u00e1metros como el n\u00famero de ejecutores y la asignaci\u00f3n de memoria.    </li> <li>Optimiza la distribuci\u00f3n de tareas dentro del cluster.    </li> </ol> <p>Ventajas:</p> <ul> <li>Reducci\u00f3n del tiempo de ejecuci\u00f3n.    </li> <li>Menos intervenci\u00f3n manual en la configuraci\u00f3n de Spark.    </li> <li>Uso m\u00e1s eficiente de los recursos disponibles.</li> </ul>"},{"location":"02%20Carga%20de%20Datos/02-4%20Desarrollo%20con%20Apache%20Spark/#6-uso-de-pyspark-para-la-ingenieria-de-datos","title":"6. Uso de PySpark para la ingenier\u00eda de datos","text":""},{"location":"02%20Carga%20de%20Datos/02-4%20Desarrollo%20con%20Apache%20Spark/#que-es-pyspark-y-por-que-usarlo-en-fabric","title":"\ud83d\udd39 \u00bfQu\u00e9 es PySpark y por qu\u00e9 usarlo en Fabric?","text":"<p>PySpark es la implementaci\u00f3n de Apache Spark en Python, y es una herramienta clave para procesar grandes vol\u00famenes de datos en paralelo dentro de Fabric.</p> <p>\ud83d\udccc Beneficios de PySpark en Fabric: \u2705 Escalabilidad: Ejecuta procesos de datos en cl\u00fasteres distribuidos. \u2705 Compatibilidad con Delta Lake: Soporta almacenamiento en OneLake con formato optimizado. \u2705 Interoperabilidad con SQL y Power BI: Permite consultas h\u00edbridas con T-SQL\u200b.</p>"},{"location":"02%20Carga%20de%20Datos/02-4%20Desarrollo%20con%20Apache%20Spark/#ejemplo-de-codigo-pyspark-en-un-notebook-de-fabric","title":"\ud83d\udd39 Ejemplo de c\u00f3digo PySpark en un Notebook de Fabric","text":"<p>`from pyspark.sql import SparkSession  </p>"},{"location":"02%20Carga%20de%20Datos/02-4%20Desarrollo%20con%20Apache%20Spark/#crear-sesion-de-spark","title":"Crear sesi\u00f3n de Spark","text":"<p>spark = SparkSession.builder.appName(\"FabricPySparkDemo\").getOrCreate()  </p>"},{"location":"02%20Carga%20de%20Datos/02-4%20Desarrollo%20con%20Apache%20Spark/#cargar-datos-desde-onelake","title":"Cargar datos desde OneLake","text":"<p>df = spark.read.format(\"delta\").load(\"onelake://mi_empresa/datasets/ventas\")  </p>"},{"location":"02%20Carga%20de%20Datos/02-4%20Desarrollo%20con%20Apache%20Spark/#transformaciones-en-pyspark","title":"Transformaciones en PySpark","text":"<p>df_filtrado = df.filter(df[\"ingresos\"] &gt; 10000)  # Mostrar resultados df_filtrado.show()`</p> <p>\ud83d\udccc Explicaci\u00f3n del c\u00f3digo: 1\ufe0f\u20e3 Se inicia una sesi\u00f3n de Spark en Fabric. 2\ufe0f\u20e3 Se cargan datos en formato Delta Lake desde OneLake. 3\ufe0f\u20e3 Se aplican transformaciones con PySpark (filtros y manipulaci\u00f3n de datos).</p>"},{"location":"02%20Carga%20de%20Datos/02-4%20Desarrollo%20con%20Apache%20Spark/#7-introduccion-a-los-notebooks-en-microsoft-fabric","title":"7. Introducci\u00f3n a los Notebooks en Microsoft Fabric","text":""},{"location":"02%20Carga%20de%20Datos/02-4%20Desarrollo%20con%20Apache%20Spark/#que-es-un-notebook-en-microsoft-fabric","title":"\ud83d\udd39 \u00bfQu\u00e9 es un Notebook en Microsoft Fabric?","text":"<p>Un Notebook en Microsoft Fabric es un entorno interactivo que permite escribir y ejecutar c\u00f3digo en m\u00faltiples lenguajes (Python, Scala, SQL y .NET). Est\u00e1 dise\u00f1ado para ingenieros de datos, cient\u00edficos de datos y analistas, facilitando la exploraci\u00f3n y transformaci\u00f3n de datos\u200b.</p>"},{"location":"02%20Carga%20de%20Datos/02-4%20Desarrollo%20con%20Apache%20Spark/#caracteristicas-clave-de-los-notebooks-en-fabric","title":"\ud83d\udd39 Caracter\u00edsticas clave de los Notebooks en Fabric","text":"<p>\u2705 Soporte multi-lenguaje: Python (PySpark), Scala, SQL y .NET. \u2705 Integraci\u00f3n con OneLake: Accede y manipula datos almacenados en Fabric. \u2705 Ejecuta Spark Jobs en paralelo: Alta capacidad de c\u00f3mputo distribuido. \u2705 Versionado con Git: Soporte para control de versiones e integraci\u00f3n con CI/CD. \u2705 Orquestaci\u00f3n con Pipelines: Los notebooks pueden ejecutarse como parte de un flujo de datos\u200b.</p> <p>\ud83d\udccc Ejemplo de uso: Un equipo de an\u00e1lisis de datos puede usar un Notebook en Fabric para limpiar y transformar datos financieros antes de almacenarlos en un Data Warehouse.</p>"},{"location":"02%20Carga%20de%20Datos/02-4%20Desarrollo%20con%20Apache%20Spark/#8-optimizacion-del-rendimiento-en-spark-y-mejores-practicas","title":"8. Optimizaci\u00f3n del rendimiento en Spark y mejores pr\u00e1cticas","text":""},{"location":"02%20Carga%20de%20Datos/02-4%20Desarrollo%20con%20Apache%20Spark/#estrategias-para-mejorar-el-rendimiento-de-apache-spark-en-fabric","title":"\ud83d\udd39 Estrategias para mejorar el rendimiento de Apache Spark en Fabric","text":"<p>\ud83d\udccc Optimizaci\u00f3n del rendimiento con Spark en Fabric:</p> <p>\u2705 Uso de formato Delta Lake \u2192 Acelera la lectura/escritura y permite transacciones ACID\u200b. \u2705 Paralelismo en tareas Spark \u2192 Ajustar el n\u00famero de particiones para evitar cuellos de botella. \u2705 Uso de cach\u00e9 en Spark \u2192 Permite mejorar tiempos de respuesta en datasets reutilizados. \u2705 Aplicaci\u00f3n de V-Order en Parquet \u2192 Optimiza el rendimiento de consultas SQL sobre datos en Fabric.</p> <p>\ud83d\udccc Ejemplo de optimizaci\u00f3n con cach\u00e9 en PySpark</p> <p><code>df.cache()  # Mantiene el DataFrame en memoria para acelerar consultas df.count()  # Ejecuta una acci\u00f3n para materializar el cache</code></p> <p>\ud83d\udd39 Estrategia de particionamiento recomendada:</p> <p><code>df.repartition(10)  # Distribuir datos en 10 particiones para balanceo de carga</code></p> <p>\u2705 Monitorizaci\u00f3n de Spark Jobs en Fabric: Fabric proporciona herramientas de mointorizaci\u00f3n para visualizar el rendimiento de cada tarea de Spark, permitiendo identificar cuellos de botella\u200b.</p>"},{"location":"02%20Carga%20de%20Datos/02-4%20Desarrollo%20con%20Apache%20Spark/#9carga-incremental-con-scd2-con-pyspark","title":"9.Carga Incremental con SCD2 con PySpark","text":"<p>En los escenarios de BI comunes donde disponemos de un Data Warehouses en SQL Server On-Premises o en la nube, nos encontr\u00e1bamos con la caracter\u00edstica IDENTITY que se asignaba a las claves subrogadas de las dimensiones. Ahora, con las tablas Delta nos encontramos con la problem\u00e1tica de que las IDENTITY desaparecen, por lo que el versionado de las SCD2 tiene que ser redise\u00f1ado.</p> <p>Para ello, proponemos una soluci\u00f3n para emular el atributo IDENTITY almacenando en una variable el \u00faltimo valor de la clave subrogada cargada, para despu\u00e9s con c\u00f3digo escrito en Spark SQL y la funci\u00f3n de ventana ROW_NUMBER(), gestionar y mantener los \u00edndices autoincrem\u00e9ntales en los elementos que entren nuevos o hayan cambiado de versi\u00f3n.</p> <p>1) Partiendo de la siguiente dimensi\u00f3n que presenta el nombre \u201ctabla_delta\u201d:</p> <p></p> <p>Con estas primeras l\u00edneas de c\u00f3digo importamos las funciones del m\u00f3dulo pyspark.sql.functions, almacenamos la pk en la variable key_column, guardamos los campos en una lista, la reorganizamos para dejar fromDate al final y convertimos esa lista en un string separado por comas.</p> <p></p> <p>2) Suponiendo que en la variable sdf tenemos almacenado el DataFrame con los datos que han entrado en la \u00faltima ejecuci\u00f3n:</p> <p></p> <p>Con el siguiente c\u00f3digo le a\u00f1adimos el campo toDate y isCurrent, adem\u00e1s de a\u00f1adir un hash con la funci\u00f3n xxhash64. Este hash puede estar compuesto por todos los elementos de la dimensi\u00f3n o \u00fanicamente por los que nos interesen para versionar.</p> <p></p> <p>1) Despu\u00e9s, almacenamos en la variable next_surrogate_key el valor de la \u00faltima sk cargada en la tabla, y guardamos en una vista temporal el DataFrame sdf.</p> <p></p> <p>2) Por \u00faltimo, a\u00f1adimos las l\u00edneas de Spark SQL encargadas de finalizar las versiones actuales de la dimensi\u00f3n y de insertar las nuevas.</p> <p></p> <p>Tras ejecutar todo el c\u00f3digo el resultado de la dimensi\u00f3n ser\u00eda el siguiente:</p> <p></p> <p>En el cual disponemos de dos versiones para los id 2 y 6 y la nueva inserci\u00f3n para el id 7, cada una de ellas con su propia clave subrogada (sk).</p>"},{"location":"02%20Carga%20de%20Datos/02-4%20Desarrollo%20con%20Apache%20Spark/#10-conclusion-y-preguntas-clave","title":"10. Conclusi\u00f3n y Preguntas Clave","text":"<p>\u2705 Los Notebooks en Microsoft Fabric son herramientas clave para la ingenier\u00eda de datos con Apache Spark. \u2705 PySpark permite la manipulaci\u00f3n y transformaci\u00f3n de datos a gran escala de manera eficiente. \u2705 Optimizar el rendimiento con Delta Lake y t\u00e9cnicas de caching mejora la eficiencia del procesamiento. \u2705 Fabric facilita la integraci\u00f3n de Spark con almacenamiento en OneLake y an\u00e1lisis en Power BI.</p>"},{"location":"02%20Carga%20de%20Datos/02-4%20Desarrollo%20con%20Apache%20Spark/#preguntas-para-reflexion-y-discusion","title":"Preguntas para reflexi\u00f3n y discusi\u00f3n","text":"<p>1\ufe0f\u20e3 \u00bfC\u00f3mo elegir entre SQL, PySpark y .NET dentro de un Notebook en Fabric? 2\ufe0f\u20e3 \u00bfCu\u00e1ndo se debe usar cache() y repartition() en Spark para optimizar rendimiento? 3\ufe0f\u20e3 \u00bfC\u00f3mo se pueden monitorear y depurar errores en Spark dentro de Microsoft Fabric?</p>"},{"location":"02%20Carga%20de%20Datos/02-5%20Guia%20de%20Decisi%C3%B3n/","title":"**Gu\u00eda de decisi\u00f3n entre Actividad de Copia de un Pipeline, Dataflow Gen2 y Apache Spark","text":"**  Actividad de copia de canalizaci\u00f3n** Flujo de datos Gen 2 Spark Caso de uso Migraci\u00f3n de Data Lake y almacenamiento de datos,  ingesta de datos,  transformaci\u00f3n ligera Ingesta de datos,  transformaci\u00f3n de datos,  limpieza y transformaci\u00f3n de datos,  generaci\u00f3n de perfiles de datos Ingesta de datos,  transformaci\u00f3n de datos,  procesamiento de datos  generaci\u00f3n de perfiles de datos rol de desarrollador principal Ingeniero de datos,  integrador de datos Ingeniero de datos,  integrador de datos,  analista de negocios Ingeniero de datos,  cient\u00edfico de datos,  desarrollador de datos Conjunto de aptitudes de desarrollador principal ETL,  SQL,  JSON ETL,  M,  SQL Spark (Scala, Python, Spark SQL, R) C\u00f3digo escrito Sin c\u00f3digo,  c\u00f3digo bajo Sin c\u00f3digo,  c\u00f3digo bajo C\u00f3digo Volumen de datos Baja a alta Baja a alta Baja a alta Interfaz de desarrollo Asistente,  lienzo Power Query Cuaderno,  Definici\u00f3n de trabajo de Spark Sources M\u00e1s de 30 conectores M\u00e1s de 150 conectores Cientos de bibliotecas de Spark Destinos +18 conectores Lakehouse,  -Azure SQL Database,  Explorador de datos de Azure,  An\u00e1lisis de Azure Synapse Cientos de bibliotecas de Spark Complejidad de la transformaci\u00f3n Baja:  ligero: conversi\u00f3n de tipos, mapeo de columnas, combinar/dividir archivos, aplanar jerarqu\u00eda Baja a alta:  M\u00e1s de 300 funciones de transformaci\u00f3n Baja a alta:  compatibilidad con Spark nativo y bibliotecas de c\u00f3digo abierto <p>Los siguiente escenarios est\u00e1n extraidos de la documentaci\u00f3n oficial para entender en que caso es mejor utilizar cada una de las opciones:</p>"},{"location":"02%20Carga%20de%20Datos/02-5%20Guia%20de%20Decisi%C3%B3n/#escenario1","title":"Escenario1","text":"<p>Leo, ingeniero de datos, debe ingerir un gran volumen de datos de sistemas externos, tanto locales como en la nube. Estos sistemas externos incluyen bases de datos, sistemas de archivos y API. Leo no quiere escribir y mantener c\u00f3digo para cada conector o operaci\u00f3n de movimiento de datos. Quiere seguir las pr\u00e1cticas recomendadas para los niveles de medall\u00f3n (bronce, plata y oro). Leo no tiene ninguna experiencia con Spark, por lo que prefiere la interfaz de usuario de arrastrar y colocar tanto como sea posible, con una codificaci\u00f3n m\u00ednima. Y tambi\u00e9n quiere procesar los datos seg\u00fan una programaci\u00f3n.</p> <p>El primer paso es obtener los datos sin procesar en el lago de datos del nivel bronce de los recursos de datos de Azure y varios or\u00edgenes de terceros (como Snowflake Web, REST, AWS S3, GCS, etc.). Quiere un lago de datos consolidado, de modo que todos los datos procedentes de distintos LOB, locales y or\u00edgenes en la nube residan en un \u00fanico lugar. Leo revisa las opciones y selecciona la\u00a0actividad de copia de canalizaci\u00f3n\u00a0como opci\u00f3n adecuada para su copia binaria sin procesar. Este patr\u00f3n se aplica tanto a la actualizaci\u00f3n de datos hist\u00f3ricos como incrementales. Con la actividad de copia, Leo puede cargar datos Oro en un almacenamiento de datos sin c\u00f3digo si surge la necesidad y las canalizaciones proporcionan una ingesta de datos a gran escala que puede mover datos a escala de petabyte. La actividad de copia es la mejor opci\u00f3n de c\u00f3digo bajo y sin c\u00f3digo para mover petabytes de datos a lago de datos y almacenamientos de variedades de or\u00edgenes, ya sea ad hoc o a trav\u00e9s de una programaci\u00f3n.</p>"},{"location":"02%20Carga%20de%20Datos/02-5%20Guia%20de%20Decisi%C3%B3n/#escenario2","title":"Escenario2","text":"<p>Mary es ingeniero de datos con un profundo conocimiento de los m\u00faltiples requisitos de informes anal\u00edticos de LOB. Un equipo ascendente ha implementado correctamente una soluci\u00f3n para migrar varios datos hist\u00f3ricos e incrementales de LOB a un lago de datos com\u00fan. Mary se ha encargado de limpiar los datos, aplicar l\u00f3gicas de negocios y cargarlos en varios destinos (como Azure SQL DB, ADX y lakehouse) como preparaci\u00f3n para sus respectivos equipos de informes.</p> <p>Mary es un usuario experimentado de Power Query y el volumen de datos est\u00e1 en el rango bajo a medio para lograr el rendimiento deseado. Los flujos de datos proporcionan interfaces sin c\u00f3digo o poco c\u00f3digo para ingerir datos de cientos de or\u00edgenes de datos. Con los flujos de datos, puede transformar datos mediante m\u00e1s de 300 opciones de transformaci\u00f3n de datos y escribir los resultados en varios destinos con una interfaz de usuario muy visual f\u00e1cil de usar. Mary revisa las opciones y decide que tiene sentido usar\u00a0Dataflow Gen 2\u00a0como opci\u00f3n de transformaci\u00f3n preferida.</p>"},{"location":"02%20Carga%20de%20Datos/02-5%20Guia%20de%20Decisi%C3%B3n/#escenario3","title":"Escenario3","text":"<p>Adam es un ingeniero de datos que trabaja para una gran empresa minorista que usa una casa de lago para almacenar y analizar sus datos de clientes. Como parte de su trabajo, Adam es responsable de crear y mantener las canalizaciones de datos que extraen, transforman y cargan datos en el lago. Uno de los requisitos empresariales de la empresa es realizar an\u00e1lisis de revisi\u00f3n de clientes para obtener informaci\u00f3n sobre las experiencias de sus clientes y mejorar sus servicios.</p> <p>Adam decide la mejor opci\u00f3n es usar\u00a0spark\u00a0para crear la l\u00f3gica de extracci\u00f3n y transformaci\u00f3n. Spark proporciona una plataforma inform\u00e1tica distribuida que puede procesar grandes cantidades de datos en paralelo. Escribe una aplicaci\u00f3n spark con Python o Scala, que lee datos estructurados, semiestructurados y no estructurados de OneLake para opiniones y comentarios de los clientes. La aplicaci\u00f3n limpia, transforma y escribe datos en las tablas Delta del lago de datos. Los datos est\u00e1n listos para ser utilizados en an\u00e1lisis posteriores.</p>"},{"location":"03%20Ingenieria%20de%20Datos/03-1%20Introducci%C3%B3n%20a%20la%20Arquitectura%20Medall%C3%B3n/","title":"Introducci\u00f3n a la Arquitectura Medall\u00f3n","text":"<p>La arquitectura Medall\u00f3n es un enfoque de dise\u00f1o de datos en lagos de datos (Data Lakes) que organiza la informaci\u00f3n en tres capas o zonas para garantizar calidad, gobernanza y eficiencia en el an\u00e1lisis. Su objetivo es estructurar los datos desde su ingesti\u00f3n hasta su consumo, asegurando integridad y optimizaci\u00f3n en la consulta y transformaci\u00f3n.</p> <p>Microsoft Fabric, con OneLake como n\u00facleo central, permite la implementaci\u00f3n de esta arquitectura de manera nativa, proporcionando una soluci\u00f3n unificada para la gesti\u00f3n de datos empresariales.</p>"},{"location":"03%20Ingenieria%20de%20Datos/03-1%20Introducci%C3%B3n%20a%20la%20Arquitectura%20Medall%C3%B3n/#1-concepto-de-bronce-plata-y-oro-estructura-de-capas-en-la-gestion-de-datos","title":"1. Concepto de Bronce, Plata y Oro: Estructura de capas en la gesti\u00f3n de datos","text":""},{"location":"03%20Ingenieria%20de%20Datos/03-1%20Introducci%C3%B3n%20a%20la%20Arquitectura%20Medall%C3%B3n/#que-es-la-arquitectura-medallon","title":"\ud83d\udd39 \u00bfQu\u00e9 es la Arquitectura Medall\u00f3n?","text":"<p>La Arquitectura Medall\u00f3n es un enfoque de gesti\u00f3n de datos en capas que mejora la calidad y accesibilidad de los datos almacenados en un Data Lake o Lakehouse. En Microsoft Fabric, esta arquitectura se implementa en OneLake, permitiendo gestionar la evoluci\u00f3n de los datos desde su ingesta hasta su consumo\u200b. Existen diferentes nomenclaturas para denominar a las capas (bronze, silver, gold ; raw, validated, curated,...) en la documentaci\u00f3n oficial de Onelake, Microsoft opta por bronze, silver, gold, as\u00ed que esa utilizaremos durante este curso. </p> <p></p>"},{"location":"03%20Ingenieria%20de%20Datos/03-1%20Introducci%C3%B3n%20a%20la%20Arquitectura%20Medall%C3%B3n/#capas-de-la-arquitectura-medallon","title":"\ud83d\udd39 Capas de la Arquitectura Medall\u00f3n","text":""},{"location":"03%20Ingenieria%20de%20Datos/03-1%20Introducci%C3%B3n%20a%20la%20Arquitectura%20Medall%C3%B3n/#capa-bronze","title":"Capa Bronze","text":"<p>Prop\u00f3sito: Almacenar los datos en su formato original tal y como llegan desde las fuentes.</p> <p>\ud83d\udccc Caracter\u00edsticas:</p> <ul> <li>Datos sin procesar, extra\u00eddos directamente de fuentes transaccionales, archivos, APIs, sensores IoT, etc.</li> <li>Formato: JSON, CSV, Parquet, Avro, Delta Lake.</li> <li>Gobernanza m\u00ednima, solo controles de acceso.</li> <li>Ejemplo en Fabric: Ingesta con Data Factory y almacenamiento en OneLake.</li> </ul>"},{"location":"03%20Ingenieria%20de%20Datos/03-1%20Introducci%C3%B3n%20a%20la%20Arquitectura%20Medall%C3%B3n/#capa-platavalidada","title":"Capa Plata(Validada)","text":"<p>\ud83d\udccc Prop\u00f3sito: Refinar y limpiar los datos para hacerlos \u00fatiles en an\u00e1lisis.</p> <p>\ud83d\udccc Caracter\u00edsticas:</p> <ul> <li>Se eliminan valores nulos, se corrigen errores y se filtran registros.</li> <li>Se generan particiones y optimizaciones para mejorar rendimiento.</li> <li>Se aplican modelos de gobernanza y seguridad.</li> </ul> <p>\ud83d\udccc Ejemplo en Fabric: Aplicaci\u00f3n de transformaciones en Data Engineering y almacenamiento en Delta Lake sobre OneLake.</p>"},{"location":"03%20Ingenieria%20de%20Datos/03-1%20Introducci%C3%B3n%20a%20la%20Arquitectura%20Medall%C3%B3n/#capa-orocurada","title":"Capa Oro(Curada)","text":"<p>\ud83d\udccc Prop\u00f3sito: Servir datos listos para an\u00e1lisis y modelos de negocio.</p> <p>\ud83d\udccc Caracter\u00edsticas:</p> <ul> <li>Datos enriquecidos, agregados y optimizados.</li> <li>Indexaci\u00f3n para consultas eficientes.</li> <li>Modelado dimensional (star schema) para integraci\u00f3n con BI.</li> </ul> <p>\ud83d\udccc Ejemplo en Fabric:</p> <ul> <li>Uso de Data Warehouse en Fabric para modelado relacional.</li> <li>Creaci\u00f3n de vistas optimizadas para Power BI.</li> </ul> Capa Prop\u00f3sito Ejemplo de uso Bronce (Raw Layer) Almacena los datos sin procesar, tal como llegan desde las fuentes. Datos de sensores IoT o logs de transacciones sin transformaci\u00f3n. Plata (Clean Layer) Se aplican procesos de limpieza, transformaci\u00f3n y deduplicaci\u00f3n. Eliminaci\u00f3n de valores nulos y normalizaci\u00f3n de datos. Oro (Curated Layer) Datos listos para el consumo empresarial, optimizados para BI y anal\u00edtica. Tablas agregadas para informes financieros en Power BI. <p>\ud83d\udccc Beneficio clave: Permite manejar datos a gran escala, asegurando que las transformaciones se realicen de manera eficiente y escalable en Fabric\u200b.</p>"},{"location":"03%20Ingenieria%20de%20Datos/03-1%20Introducci%C3%B3n%20a%20la%20Arquitectura%20Medall%C3%B3n/#2-estrategias-para-mover-datos-entre-capas-y-asegurar-la-calidad","title":"2. Estrategias para mover datos entre capas y asegurar la calidad","text":""},{"location":"03%20Ingenieria%20de%20Datos/03-1%20Introducci%C3%B3n%20a%20la%20Arquitectura%20Medall%C3%B3n/#proceso-elt-en-la-arquitectura-medallon","title":"\ud83d\udd39 Proceso ELT en la Arquitectura Medall\u00f3n","text":"<p>Cada capa tiene un conjunto de transformaciones clave que garantizan la calidad y gobernanza de los datos en Fabric.</p> <p>\ud83d\udd39 Estrategias para mover datos entre capas:</p> <p>1\ufe0f\u20e3 De Bronce a Plata: \u2705 Filtrado de datos err\u00f3neos o duplicados.(de-duplicaci\u00f3n) \u2705 Conversi\u00f3n de formatos (JSON \u2192 Parquet, CSV \u2192 Delta Lake). \u2705 Enriquecimiento con datos de otras fuentes.</p> <p>2\ufe0f\u20e3 De Plata a Oro: \u2705 Agregaciones y c\u00e1lculos avanzados para optimizar consultas. \u2705 Creaci\u00f3n de vistas optimizadas en un Data Warehouse. \u2705 Indexaci\u00f3n y particionamiento para mejorar el rendimiento\u200b.</p> <p>\ud83d\udccc Ejemplo pr\u00e1ctico: Un pipeline de Data Factory puede extraer datos brutos de sensores (Bronce), limpiarlos y calcular promedios por hora (Plata), y finalmente generar reportes con tendencias de uso (Oro).</p>"},{"location":"03%20Ingenieria%20de%20Datos/03-1%20Introducci%C3%B3n%20a%20la%20Arquitectura%20Medall%C3%B3n/#patrones-de-despliegue","title":"\ud83d\udd39 Patrones de despliegue","text":"<p>Para implementar la arquitectura medallion en Fabric, podemos usar lakehouses (uno para cada zona), un datawarehouse o una combinaci\u00f3n de ambos. La decisi\u00f3n depender\u00e1 de nuestras preferencias y de la experiencia de su equipo. Fabric no ofrece flexibilidad: podemos usar diferentes motores anal\u00edticos que trabajan con una \u00fanica copia de los datos en OneLake.</p> <p>Aqu\u00ed hay dos patrones que podemos considerar:</p> <p>\ud83d\udd39 Patr\u00f3n 1: Crear cada zona como un lakehouse. En este caso, los usuarios empresariales acceden a los datos a trav\u00e9s del punto de conexi\u00f3n de an\u00e1lisis SQL.</p> <p>\ud83d\udd39 Patr\u00f3n 2: Configurar las zonas de bronce y plata como lakehouse y la zona oro como datawarehouse. Aqu\u00ed, los usuarios empresariales acceden a los datos mediante el punto de conexi\u00f3n de almacenamiento de datos.</p> <p>Aunque podemos crear todos los almacenes de lago dentro de una \u00fanica \u00e1rea de trabajo en Fabric, lo recomendable es que cada almac\u00e9n tenga su propia \u00e1rea de trabajo independiente. Este enfoque nos da m\u00e1s control y mejor gobernanza a nivel de zona.</p> <p>Para la zona bronce, lo ideal es almacenar los datos en su formato original o usar Parquet o Delta Lake. Siempre que sea posible, debemos de mantener los datos en su formato original. Si los datos de origen provienen de OneLake, Azure Data Lake Store Gen2 (ADLS Gen2), Amazon S3 o Google, es mejor crear un acceso directo en la zona bronce en lugar de copiarlos.</p> <p>Para las zonas plata y oro, se recomienda usar tablas Delta por las funcionalidades adicionales y mejoras de rendimiento que ofrecen. Fabric normaliza el formato Delta Lake y, por defecto, cada motor de Fabric escribe los datos en este formato. Adem\u00e1s, estos motores aplican la optimizaci\u00f3n en tiempo de escritura V-Order al formato de archivo Parquet, lo que permite lecturas extremadamente r\u00e1pidas para motores como Power BI, SQL, Apache Spark y otros. </p>"},{"location":"03%20Ingenieria%20de%20Datos/03-1%20Introducci%C3%B3n%20a%20la%20Arquitectura%20Medall%C3%B3n/#3-delta-lakehouse","title":"3. Delta lakehouse","text":""},{"location":"03%20Ingenieria%20de%20Datos/03-1%20Introducci%C3%B3n%20a%20la%20Arquitectura%20Medall%C3%B3n/#apache-parquet","title":"\ud83d\udd39 Apache Parquet","text":"<p>Parquet es un formato de\u00a0almacenamiento columnar\u00a0de c\u00f3digo abierto que se cre\u00f3 como parte de Apache Hadoop y que actualmente se utiliza en muchos otros sistemas. Su principal caracter\u00edstica es que organiza los datos en columnas, a diferencia de otros formatos que los organizan en filas, como CSV. Adem\u00e1s, a cada columna se le asigna un tipo de dato. Esto tiene varias ventajas, entre ellas quiero resaltar las dos que me parecen m\u00e1s importantes. La primera ventaja es la\u00a0reducci\u00f3n del espacio de almacenamiento, ya que los datos de cada columna se pueden codificar de manera independiente, con un algoritmo espec\u00edfico para el tipo de dato de la columna. Por ejemplo, para una columna con textos donde los valores se repitan mucho, en lugar de almacenar directamente el texto de cada fila, se puede crear primero un diccionario con los valores \u00fanicos y luego para cada fila almacenar un valor num\u00e9rico con el \u00edndice al diccionario. De esta forma se logra reducir el espacio requerido para almacenar la columna. Parquet soporta varios m\u00e9todos de codificaci\u00f3n. Adem\u00e1s de esta codificaci\u00f3n, Parquet tambi\u00e9n soporta varios tipos de compresi\u00f3n, como Snappy o GZIP, con lo cu\u00e1l se logra reducir a\u00fan m\u00e1s el espacio de almacenamiento. Para dar una idea de cuanto se comprimen los datos, he convertido un archivo CSV con 11 millones de filas a Parquet. El tama\u00f1o en disco del CSV es de unos 500 MB mientras que el tama\u00f1o del archivo Parquet es de unos 42 MB. Por lo tanto,\u00a0con los archivos Parquet podemos reducir los costes de almacenamiento, sobre todo si estamos usando almacenamiento en la nube como es el caso de Onelake. La segunda ventaja es que\u00a0las columnas se pueden leer de manera independiente, o sea, si s\u00f3lo se quieren obtener los datos de algunas columnas, estos se pueden ir a buscar directamente, sin necesidad de cargar los datos de las columnas restantes. </p> <p>Otras dos caracter\u00edsticas muy importantes de Parquet son:</p> <ul> <li>Metadatos: adem\u00e1s de los datos, guarda informaci\u00f3n sobre los nombres y los tipos de las columnas, estad\u00edsticas de los datos, entre otras cosas.</li> <li>Estructuras anidadas: es capaz de representar datos con una estructura compleja, por ejemplo, que una columna contenga a su vez otra tabla.</li> </ul> <p>Se convirti\u00f3 r\u00e1pidamente en el formato de referencia en el paradigma Big Data, por la compresi\u00f3n que ayudaba a reducir costes, por poder manejar gran cantidad de datos y estar orientado al an\u00e1lisis, y por facilitar los patrones de ELT, con una estructura de metadata integrada. </p>"},{"location":"03%20Ingenieria%20de%20Datos/03-1%20Introducci%C3%B3n%20a%20la%20Arquitectura%20Medall%C3%B3n/#que-es-un-delta-lake","title":"\ud83d\udd39 \u00bfQu\u00e9 es un Delta lake?","text":"<p>Delta Lake es una capa de almacenamiento open source que ofrece transacciones ACID (atomicidad, coherencia, aislamiento y durabilidad) para cargas de trabajo de macrodatos de Apache Spark. Est\u00e1 basado en una evoluci\u00f3n del formato Apache Parquet, denominada Delta, desarrollada por Databricks, que a d\u00eda de hoy se ha convertido en un est\u00e1ndar de facto en el mundo lakehouse, desde que Databricks decidi\u00f3 liberarlo como un proyecto Open Source tambi\u00e9n.  Por lo tanto el formato de fichero Delta es un est\u00e1ndar de almacenamiento optimizado basado en Parquet, dise\u00f1ado para mejorar la eficiencia en la gesti\u00f3n de datos en lagos de datos y almacenes anal\u00edticos. Microsoft Fabric adopta Delta Lake como formato principal para almacenamiento en OneLake, asegurando compatibilidad con m\u00faltiples motores anal\u00edticos y soporte para transacciones ACID. A continuaci\u00f3n se detallas las principales caracter\u00edsticas de este formato:</p> Caracter\u00edstica Descripci\u00f3n Transacciones ACID Los lakehouses suelen llenarse con m\u00faltiples procesos y canalizaciones, donde algunos escriben datos mientras otros los leen. Delta Lake  ofrece transacciones ACID para garantizar la integridad de los datos, evitando problemas en los registros. As\u00ed podemos asegurarnos de que los datos sean consistentes y fiables. Control escalable de metadatos Administrar grandes vol\u00famenes de datos puede ser complicado, pero Delta Lake maneja incluso los metadatos a gran escala. Aprovecha la potencia de Spark para procesarlos de manera distribuida, lo que permite gestionar datos en la escala de petabytes sin perder eficiencia. Viaje en el tiempo (control de versiones de datos) \u00bfNecesitas volver a una versi\u00f3n anterior de los datos? Con Delta Lake podemos \"deshacer\" cambios y recuperar versiones pasadas, lo que facilita auditor\u00edas, an\u00e1lisis de tendencias y recuperaci\u00f3n de informaci\u00f3n en caso de errores. Formato abierto Delta Lake utiliza Apache Parquet como formato de referencia, lo que nos permite beneficiarse de esquemas de compresi\u00f3n y codificaci\u00f3n eficientes sin depender de formatos propietarios. Lote unificado y origen/receptor de streaming Las tablas en Delta Lake pueden funcionar tanto para procesamiento por lotes como en streaming. Podemos ingerir datos en tiempo real, reponer hist\u00f3ricos de manera sencilla y ejecutar consultas interactivas sin problemas. Aplicaci\u00f3n de esquemas Se aseguran de que los tipos de datos sean correctos y de que todas las columnas necesarias est\u00e9n presentes. As\u00ed evitan inconsistencias y mantienen la coherencia de los datos. Evoluci\u00f3n del esquema Pueden realizar cambios en los esquemas de las tablas sin necesidad de escribir DDL de migraci\u00f3n manualmente. As\u00ed, la estructura de los datos se adapta autom\u00e1ticamente a nuevas necesidades. Historial de auditor\u00edas Delta Lake registra autom\u00e1ticamente cada cambio en los datos, permiti\u00e9ndoles realizar auditor\u00edas completas y rastrear modificaciones de manera sencilla. Actualizaciones y eliminaciones Pueden actualizar y eliminar datos f\u00e1cilmente con operaciones como MERGE, UPDATE y DELETE, sin complicaciones y garantizando el cumplimiento de normativas. 100% compatible con la API de Apache Spark No necesitan hacer cambios en sus canalizaciones de datos actuales para usar Delta Lake, ya que es totalmente compatible con Spark. <p>Uno de los mayores desaf\u00edos en el almacenamiento de datos es garantizar que las consultas sean r\u00e1pidas y eficientes. En el formato Delta Lake, existen dos estrategias clave para mejorar la ejecuci\u00f3n de consultas anal\u00edticas sobre grandes vol\u00famenes de datos: Z-Ordering y V-Ordering. Ambas t\u00e9cnicas optimizan la organizaci\u00f3n de los datos dentro de los archivos Parquet, reduciendo el tiempo de b\u00fasqueda y mejorando la latencia de las consultas.</p>"},{"location":"03%20Ingenieria%20de%20Datos/03-1%20Introducci%C3%B3n%20a%20la%20Arquitectura%20Medall%C3%B3n/#z-ordering","title":"Z-Ordering","text":"<p>Z-Ordering es una t\u00e9cnica de clustering que mejora la eficiencia de las consultas al reorganizar los datos dentro de los archivos Delta seg\u00fan una columna clave. Esta optimizaci\u00f3n es particularmente \u00fatil en escenarios donde las consultas filtran frecuentemente por una o varias columnas espec\u00edficas.</p> <p>\ud83d\udccc \u00bfC\u00f3mo funciona?</p> <ul> <li>Organiza los datos utilizando un algoritmo de Morton (Z-order curve), que agrupa valores similares dentro del mismo archivo.</li> <li>Reduce la cantidad de archivos que deben escanearse en consultas con filtros en columnas clave.</li> <li>Mejora la localidad de los datos, facilitando la lectura secuencial en almacenamiento distribuido.</li> </ul> <p>\ud83d\udc49 Casos de uso \u00f3ptimos:</p> <ul> <li>Tablas con grandes vol\u00famenes de datos y consultas que filtran por rangos en columnas espec\u00edficas.</li> <li>Datos con altos \u00edndices de cardinalidad, donde la dispersi\u00f3n de valores en los archivos podr\u00eda generar lecturas innecesarias.</li> </ul>"},{"location":"03%20Ingenieria%20de%20Datos/03-1%20Introducci%C3%B3n%20a%20la%20Arquitectura%20Medall%C3%B3n/#v-ordering","title":"V-Ordering","text":"<p>V-Ordering (Vertical Ordering) es un m\u00e9todo de ordenaci\u00f3n dentro de los archivos Parquet que mejora el rendimiento de las consultas al minimizar los costos de descompresi\u00f3n y escaneo. La aplicaci\u00f3n de V-Ordering es autom\u00e1tica cuando almacenamos datos en formato delta en Onelake. </p> <p>\ud83d\udccc \u00bfC\u00f3mo funciona?</p> <ul> <li>Reordena los datos dentro de cada archivo Parquet en un patr\u00f3n de valores agrupados y ordenados.</li> <li>Optimiza la lectura de columnas altamente repetitivas, como dimensiones con pocos valores \u00fanicos.</li> <li>Reduce el tama\u00f1o del archivo comprimido, ya que los valores similares se agrupan de forma m\u00e1s eficiente.</li> </ul> <p>\ud83d\udc49 Casos de uso \u00f3ptimos:</p> <ul> <li>Datos con alta repetici\u00f3n en columnas espec\u00edficas, donde la organizaci\u00f3n de los valores mejora la compresi\u00f3n.</li> <li>Consultas con agregaciones frecuentes, ya que permite lecturas m\u00e1s eficientes y menos I/O.</li> </ul>"},{"location":"03%20Ingenieria%20de%20Datos/03-1%20Introducci%C3%B3n%20a%20la%20Arquitectura%20Medall%C3%B3n/#4-conclusion-y-preguntas-clave","title":"4. Conclusi\u00f3n y Preguntas Clave","text":"<p>\u2705 La Arquitectura Medall\u00f3n mejora la calidad y estructuraci\u00f3n de los datos en Fabric. \u2705 Cada capa (Bronce, Plata y Oro) tiene un prop\u00f3sito espec\u00edfico para la gesti\u00f3n de datos. \u2705 Fabric proporciona herramientas para mover datos entre capas de manera eficiente con Spark y Data Factory.</p>"},{"location":"03%20Ingenieria%20de%20Datos/03-1%20Introducci%C3%B3n%20a%20la%20Arquitectura%20Medall%C3%B3n/#preguntas-para-reflexion-y-discusion","title":"Preguntas para reflexi\u00f3n y discusi\u00f3n","text":"<p>1\ufe0f\u20e3 \u00bfC\u00f3mo puedo asegurar la gobernanza y calidad de los datos en cada capa? 2\ufe0f\u20e3 \u00bfCu\u00e1ndo debo utilizar un Data Warehouse en vez de un Lakehouse para la capa Oro? 3\ufe0f\u20e3 \u00bfQu\u00e9 impacto tiene la arquitectura Medall\u00f3n en el rendimiento de las consultas en Fabric?</p>"},{"location":"03%20Ingenieria%20de%20Datos/03-2%20Creaci%C3%B3n%20y%20Gesti%C3%B3n%20de%20un%20lakehouse/","title":"03 2 Creaci\u00f3n y Gesti\u00f3n de un lakehouse","text":""},{"location":"03%20Ingenieria%20de%20Datos/03-2%20Creaci%C3%B3n%20y%20Gesti%C3%B3n%20de%20un%20lakehouse/#1-definicion-y-ventajas-de-un-lakehouse-frente-a-un-data-warehouse-tradicional","title":"1. Definici\u00f3n y ventajas de un Lakehouse frente a un Data Warehouse tradicional","text":""},{"location":"03%20Ingenieria%20de%20Datos/03-2%20Creaci%C3%B3n%20y%20Gesti%C3%B3n%20de%20un%20lakehouse/#que-es-un-lakehouse","title":"\ud83d\udd39 \u00bfQu\u00e9 es un Lakehouse?","text":"<p>Un Lakehouse en Microsoft Fabric es una arquitectura h\u00edbrida que combina lo mejor de un Data Lake y un Data Warehouse, permitiendo almacenar, procesar y analizar tanto datos estructurados como no estructurados en un solo entorno\u200b.</p> <p>\ud83d\udccc Diferencias clave entre un Lakehouse y un Data Warehouse</p> Caracter\u00edstica Lakehouse Data Warehouse Tipo de datos Estructurados, semiestructurados y no estructurados. Solo datos estructurados. Almacenamiento Basado en OneLake con formato Delta Lake. Bases de datos relacionales con ACID completo. Procesamiento Compatible con Spark, SQL y Pipelines de Data Factory. Optimizado para consultas SQL transaccionales. Flexibilidad Almacena archivos en diferentes formatos (Parquet, JSON, CSV). Depende de modelos tabulares predefinidos. Costos Escalable seg\u00fan el consumo, sin necesidad de hardware dedicado. Costoso debido a infraestructura optimizada para rendimiento SQL. <p>\ud83d\udccc \u00bfCu\u00e1ndo usar un Lakehouse? \u2705 Cuando se manejan datos diversos (logs, sensores IoT, im\u00e1genes, documentos y datos tabulares). \u2705 Si se requiere alta escalabilidad y flexibilidad sin necesidad de una estructura r\u00edgida. \u2705 Cuando se desea combinar Spark con SQL para an\u00e1lisis avanzados\u200b.</p>"},{"location":"03%20Ingenieria%20de%20Datos/03-2%20Creaci%C3%B3n%20y%20Gesti%C3%B3n%20de%20un%20lakehouse/#2-implementacion-practica-de-un-lakehouse-en-microsoft-fabric","title":"2. Implementaci\u00f3n pr\u00e1ctica de un Lakehouse en Microsoft Fabric","text":""},{"location":"03%20Ingenieria%20de%20Datos/03-2%20Creaci%C3%B3n%20y%20Gesti%C3%B3n%20de%20un%20lakehouse/#pasos-para-crear-un-lakehouse-en-fabric","title":"\ud83d\udd39 Pasos para crear un Lakehouse en Fabric","text":"<p>1\ufe0f\u20e3 Acceder al entorno de Fabric</p> <ul> <li>Ir a Microsoft Fabric \u2192 Data Engineering \u2192 Nuevo Lakehouse.</li> <li>Asignar un nombre y confirmar la creaci\u00f3n\u200b.</li> </ul> <p>2\ufe0f\u20e3 Cargar datos en OneLake</p> <p><code>df = spark.read.format(\"csv\").option(\"header\", \"true\").load(\"onelake://empresa/datos_ventas/\") df.write.format(\"delta\").save(\"onelake://empresa/lakehouse/ventas/\")</code></p> <p>\ud83d\udccc Explicaci\u00f3n:</p> <ul> <li>Se ingestan datos en formato CSV.</li> <li>Se convierten a Delta Lake para mayor eficiencia.</li> </ul> <p>3\ufe0f\u20e3 Consulta de datos en SQL Analytics Endpoint</p> <p><code>SELECT categoria, SUM(ventas) AS total_ventas FROM lakehouse.ventas GROUP BY categoria;</code></p> <p>\ud83d\udccc Permite a analistas y usuarios SQL acceder a los datos directamente\u200b</p> <p>4\ufe0f\u20e3 Visualizaci\u00f3n en Power BI con Direct Lake</p> <ul> <li>Conectar Power BI con el SQL Analytics Endpoint del Lakehouse.</li> <li>Crear dashboards interactivos en tiempo real\u200b.</li> </ul>"},{"location":"03%20Ingenieria%20de%20Datos/03-2%20Creaci%C3%B3n%20y%20Gesti%C3%B3n%20de%20un%20lakehouse/#ficheros-y-tablas","title":"\ud83d\udd39 Ficheros y Tablas","text":"<p>Dentro de un lakehouse, tenemos dos \u00e1reas diferenciadas de almacenamiento, la de Ficheros y la tablas. El \u00e1rea de ficheros, est\u00e1 destinada fundamentalmente a actuar como zona de aterrizaje (landing) de los datos, y para alojar datos no estructurados, como pueden ser im\u00e1genes, dentro de nuestro lakehouse. La funcionalidad de accesos directos, ya comentada, proporciona una capa de \"virtualizaci\u00f3n\" que intenta minimizar el movimeinto de datos, posibilitando el acceso entre diferentes or\u00edgenes, incluso entre diferentes proveedores de nube p\u00fablica. </p> <p>Por lo que respecta al \u00e1rea de tablas, su funci\u00f3n es la de proporcionarnos una experiencia warehouse, sobre los datos que tenemos almacenados en nuestro onelake, siempre en formato Delta. Podemos crear b\u00e1sicamente tres tipos diferenciados de tablas dentro de este \u00e1rea: tablas administradas, tablas externas, y accesos directos de tabla</p>"},{"location":"03%20Ingenieria%20de%20Datos/03-2%20Creaci%C3%B3n%20y%20Gesti%C3%B3n%20de%20un%20lakehouse/#tablas-administradas","title":"Tablas administradas","text":"<p>Son aquellas tablas de las que el motor Spark del lakehouse se encarga de gestionar y mantener, tanto en esquema como en datos. Son el tipo de tabla determinado, y el que proporciona toda la funcionalidad a nuestro lakehouse de control y mantenimiento de los datos. </p>"},{"location":"03%20Ingenieria%20de%20Datos/03-2%20Creaci%C3%B3n%20y%20Gesti%C3%B3n%20de%20un%20lakehouse/#tablas-externas","title":"Tablas externas","text":"<p>Se conocen tambi\u00e9n como tablas externas, y en este caso, la tabla no es m\u00e1s que una definici\u00f3n de un esquema, que apunta a una ubicaci\u00f3n externa. El esquema se fuerza \u00fanicamente en el momento de consulta de esa tabla.  Nos puede ser \u00fatil, fundamentalmente en la capa silver, en escenarios en los que el esquema de los datos de origen, es susceptible de cambiar con frecuencia.</p>"},{"location":"03%20Ingenieria%20de%20Datos/03-2%20Creaci%C3%B3n%20y%20Gesti%C3%B3n%20de%20un%20lakehouse/#accesos-directos-de-tabla","title":"Accesos directos de tabla","text":"<p>Adem\u00e1s de poder crear shortcuts a almacenamientos de ficheros en el \u00e1rea Files  de un lakehouse, tambi\u00e9n podemos crear estos accesos directos a tablas externas, lo que nos permitir\u00e1 poder acceder a esas tablas desde el SQL Endpoint, como si estuviesen realmente almacenadas en ese \u00edtem de lakehouse.  Si pensamos en nuestra arquitectura medall\u00f3n, y en el paso entre capas, los shortcuts nos pueden servir para, por ejemplo, referenciar tablas que no necesitamos modificar en el paso entre capas, evitando as\u00ed tener que copiar y duplicar esas tablas entre las diferentes capas.  </p>"},{"location":"03%20Ingenieria%20de%20Datos/03-2%20Creaci%C3%B3n%20y%20Gesti%C3%B3n%20de%20un%20lakehouse/#3-el-sql-analytics-endpoint","title":"3. El SQL Analytics Endpoint","text":""},{"location":"03%20Ingenieria%20de%20Datos/03-2%20Creaci%C3%B3n%20y%20Gesti%C3%B3n%20de%20un%20lakehouse/#consultando-con-sql-el-lakehouse","title":"\ud83d\udd39Consultando con SQL el lakehouse","text":"<p>El punto de conexi\u00f3n de an\u00e1lisis SQL en Microsoft Fabric ofrece una experiencia basada en SQL para interactuar con las tablas Delta dentro de un almac\u00e9n de lago (lakehouse). A continuaci\u00f3n, se detallan sus caracter\u00edsticas principales:</p> <ul> <li>Creaci\u00f3n autom\u00e1tica: Al generar un almac\u00e9n de lago, se crea autom\u00e1ticamente un punto de conexi\u00f3n de an\u00e1lisis SQL que apunta al almacenamiento de tablas Delta del mismo.    </li> <li>Modo de solo lectura: Este punto de conexi\u00f3n permite \u00fanicamente operaciones de lectura sobre las tablas Delta. Para modificar los datos, es necesario cambiar al modo de almac\u00e9n de lago y utilizar Apache Spark.</li> <li>Flexibilidad en la gesti\u00f3n de datos: Aunque las operaciones de escritura no est\u00e1n permitidas, se pueden crear funciones, definir vistas e implementar seguridad a nivel de objeto en SQL para estructurar y gestionar el acceso a los datos de manera efectiva.</li> <li>Control de acceso mediante seguridad SQL: Es posible establecer reglas de seguridad a nivel de objeto que se aplican al acceder a los datos a trav\u00e9s del punto de conexi\u00f3n de an\u00e1lisis SQL. Para garantizar que los datos no sean accesibles por otros medios, se deben configurar los roles y permisos adecuados en el \u00e1rea de trabajo.</li> <li>Reaprovisionamiento del punto de conexi\u00f3n: Si el aprovisionamiento inicial falla, se ofrece la opci\u00f3n de reintentar directamente desde el almac\u00e9n de lago, evitando la necesidad de crear uno nuevo.</li> <li>Limitaciones actuales: Las tablas Delta externas creadas con c\u00f3digo Spark no son visibles para el punto de conexi\u00f3n de an\u00e1lisis SQL. </li> </ul> <p>Resumiendo, el punto de conexi\u00f3n de an\u00e1lisis SQL en Microsoft Fabric facilita el an\u00e1lisis y la gesti\u00f3n de datos en lakehouses mediante T-SQL, proporcionando una integraci\u00f3n fluida y herramientas robustas para el control de acceso y la estructuraci\u00f3n de datos.</p>"},{"location":"03%20Ingenieria%20de%20Datos/03-2%20Creaci%C3%B3n%20y%20Gesti%C3%B3n%20de%20un%20lakehouse/#4-gobernanza-y-seguridad-configuracion-de-accesos-y-permisos","title":"4. Gobernanza y seguridad: Configuraci\u00f3n de accesos y permisos","text":""},{"location":"03%20Ingenieria%20de%20Datos/03-2%20Creaci%C3%B3n%20y%20Gesti%C3%B3n%20de%20un%20lakehouse/#esquemas-en-un-lakehouse","title":"\ud83d\udd39 Esquemas en un lakehouse","text":"<p>Microsoft Fabric Lakehouse admite la creaci\u00f3n de esquemas personalizados, lo que permite agrupar tablas para mejorar la detecci\u00f3n de datos, el control de acceso y la organizaci\u00f3n. Al crear un nuevo lakehouse, es posible habilitar esta funci\u00f3n seleccionando la opci\u00f3n \"Esquemas de lakehouse (versi\u00f3n preliminar p\u00fablica)\". Una vez creado, nos encontraremos un esquema predeterminado llamado \"dbo\" bajo la secci\u00f3n de Tablas, el cual es permanente y no puede ser modificado o eliminado. </p> <p>Para almacenar una tabla en un esquema espec\u00edfico, es necesario especificar el nombre del esquema al guardar la tabla. Si no se indica, la tabla se ubicar\u00e1 en el esquema predeterminado \"dbo\". Por ejemplo, utilizando PySpark, se podemos guardar una tabla en el esquema \"ventas\" con el siguiente comando:</p> <p><code>df.write.mode(\"Overwrite\").saveAsTable(\"ventas.nombre_tabla\")</code></p> <p>Adem\u00e1s, el Explorador de Lakehouse permite organizar las tablas mediante la funci\u00f3n de arrastrar y soltar, facilitando su reubicaci\u00f3n entre diferentes esquemas. Es importante tener en cuenta que, al modificar una tabla, tambi\u00e9n se deben actualizar los elementos relacionados, como el c\u00f3digo en cuadernos o los flujos de datos, para asegurar que est\u00e9n alineados con el esquema correcto. Obviamente estas tareas no se realizan autom\u00e1ticamente. </p> <ul> <li> <p>Para referenciar m\u00faltiples tablas Delta desde otro lakehouse de Fabric o desde un almacenamiento externo, se puede utilizar un acceso directo de esquema. Este acceso muestra todas las tablas bajo el esquema o carpeta seleccionados, reflejando cualquier cambio realizado en las tablas de la ubicaci\u00f3n de origen. </p> </li> <li> <p>En el contexto de la elaboraci\u00f3n de informes con Power BI, al crear un modelo sem\u00e1ntico, es posible seleccionar tablas de diferentes esquemas. Si existen tablas con el mismo nombre en distintos esquemas, se mostrar\u00e1n n\u00fameros junto a los nombres de las tablas en la vista de modelo para diferenciarlas.</p> </li> <li> <p>Al trabajar con cuadernos en un lakehouse que tiene habilitados los esquemas, las tablas se organizan seg\u00fan sus respectivos esquemas en el explorador de objetos del cuaderno. Es posible arrastrar y soltar una tabla en una celda de c\u00f3digo para obtener un fragmento de c\u00f3digo que hace referencia al esquema correspondiente. La nomenclatura para referenciar una tabla sigue el formato: \"\u00e1rea_de_trabajo.lakehouse.esquema.tabla\". Si se omite alg\u00fan elemento, el ejecutor utilizar\u00e1 la configuraci\u00f3n predeterminada; por ejemplo, si solo se proporciona el nombre de la tabla, se asumir\u00e1 el esquema \"dbo\" del lakehouse predeterminado para el cuaderno.</p> </li> <li>Para realizar consultas SQL de Spark que involucren m\u00faltiples \u00e1reas de trabajo, se utiliza la nomenclatura \"\u00e1rea_de_trabajo.lakehouse.esquema.tabla\" al referenciar las tablas en el c\u00f3digo. Esto permite combinar tablas de diferentes \u00e1reas de trabajo, siempre que el usuario que ejecuta el c\u00f3digo tenga los permisos necesarios para acceder a dichas tablas. Es fundamental asegurarse de que las tablas provengan de lakehouses con esquemas habilitados, ya que la combinaci\u00f3n de tablas de lakehouses sin esquemas habilitados no funcionar\u00e1.</li> </ul> <p>Actualmente (Febrero 2025), en la versi\u00f3n preliminar p\u00fablica, existen algunas limitaciones. Por ejemplo, el uso del \u00e1rea de trabajo en el espacio de nombres para lakehouses compartidos no funciona correctamente, y obtener el esquema para tablas administradas en formatos no Delta (como CSV) no est\u00e1 soportado. </p>"},{"location":"03%20Ingenieria%20de%20Datos/03-2%20Creaci%C3%B3n%20y%20Gesti%C3%B3n%20de%20un%20lakehouse/#modelo-de-seguridad-en-un-lakehouse","title":"\ud83d\udd39 Modelo de seguridad en un Lakehouse","text":"<p>\ud83d\udccc Microsoft Fabric permite la gesti\u00f3n de permisos en un Lakehouse mediante roles y controles de acceso\u200b.</p> Rol Permisos Admin Acceso total, configuraci\u00f3n y eliminaci\u00f3n de Lakehouse. Member Lectura y escritura de datos, ejecuci\u00f3n de scripts. Contributor Creaci\u00f3n y modificaci\u00f3n de objetos, sin configuraci\u00f3n de seguridad. Viewer Solo lectura de datos desde SQL Analytics Endpoint. Estos permisos se establecen a nivel de workspace y se heredan en todos los items, tambi\u00e9n los lakehouse. ### \ud83d\udd39 Implementaci\u00f3n de seguridad en Lakehouse <p>Desde el punto de vista de acceso al lakehouse, como cualquier otro item del espacio de trabajo, tendr\u00e1n acceso predeterminado los usuarios a los que hayamos dado acceso a trav\u00e9s de los roles del espacio de trabajo. En el caso de los usuarios con el rol de visor, tendr\u00e1n acceso al SQL EndPoint, pero no podr\u00e1n acceder al item de lakehouse. Sin embargo, podemos compartir con un usuario el acceso al endpoint SQL de nuestro lakehouse, sin necesidad de darle permisos sobre ning\u00fan otro item del espacio de trabajo. Esto lo realizamos desde la opci\u00f3n de compartir del lakehouse</p> <p>![[Compartir acceso lakehouse.png]] Una vez hemos configurado el acceso, podemos llegar a niveles m\u00e1s granulares, controlando el acceso a las tablas, a trav\u00e9s de los roles que veremos en el SQL Endpoint y la seguridad a nivel de columna y a nivel de fila, que configuramos a trav\u00e9s de las pol\u00edticas de seguridad. </p> <p>1\ufe0f\u20e3 Definir accesos mediante RBAC (Role-Based Access Control)</p> <p><code>GRANT SELECT ON lakehouse.ventas TO [grupo_finanzas];</code></p> <p>\ud83d\udccc Restringe acceso solo a usuarios autorizados\u200b.</p> <p>2\ufe0f\u20e3 Uso de Column-Level Security (CLS) y Row-Level Security (RLS)</p> <p><code>CREATE SECURITY POLICY Ventas_Politica ADD FILTER PREDICATE usuario_region = USER() ON lakehouse.ventas;</code></p> <p>\ud83d\udccc Permite restringir filas seg\u00fan el usuario autenticado\u200b.</p> <p>3\ufe0f\u20e3 Uso de Shortcuts en OneLake para compartir datos sin replicarlos</p> <p><code>CREATE SHORTCUT ventas_region TO \"onelake://empresa/lakehouse/ventas/\"</code></p> <p>\ud83d\udccc Facilita compartir datos con otros equipos sin duplicaciones\u200b.</p> <p>Tenemos que asegurarnos tambi\u00e9n, de que antes de todo esto, los usuarios tienen los permisos necesarios a nivel de almacenamiento de Onelake. Podemos ver la combinaci\u00f3n de opciones en este enlace de la documentaci\u00f3n Seguridad RBAC y Onelake</p>"},{"location":"03%20Ingenieria%20de%20Datos/03-2%20Creaci%C3%B3n%20y%20Gesti%C3%B3n%20de%20un%20lakehouse/#5-mantenimiento-de-tablas","title":"5. Mantenimiento de tablas","text":"<p>Como ocurre en cualquier motor de datos, debemos de realizar labores de mantenimiento sobre nuestros datos. Disponemos de una opci\u00f3n a trav\u00e9s del interfaz gr\u00e1fico de mantenimiento de tablas que nos proporciona las funcionalidades m\u00ednimas para realizar ese mantenimiento. </p> <p>![[Mantenimiento tablas lakehouse.png]]</p> <p>La caracter\u00edstica de mantenimiento de tablas nos ofrece tres operaciones.</p> <ul> <li>Optimizaci\u00f3n: consolida varios archivos Parquet peque\u00f1os en archivos grandes. Las arquitecturas de big data y lakehouse, est\u00e1n pensadas para trabajar con archivos de gran tama\u00f1o. Tener archivos de tama\u00f1o superior a 128 MB y, de forma \u00f3ptima, cerca de 1 GB, mejora la compresi\u00f3n y la distribuci\u00f3n de datos, en los nodos del cl\u00faster. Reduce la necesidad de examinar numerosos archivos peque\u00f1os para realizar operaciones de lectura eficaces. Por lo tanto, se recomienda ejectura estrategias de optimizaci\u00f3n orientadas a reducir el n\u00famero de ficheros. </li> <li>Orden en V (V-order): aplica la ordenaci\u00f3n, la codificaci\u00f3n y la compresi\u00f3n optimizadas a los archivos Parquet delta para permitir operaciones de lectura r\u00e1pidas en todos los motores de Fabric. Para m\u00e1s detalle sobre el funcionamiento de esta optimizaci\u00f3n, podemos consultar este enlace de la documentaci\u00f3n oficial  V-Order </li> <li>Vacuum: quita los archivos antiguos a los que ya no hace referencia un registro de tabla de Delta. Los archivos deben ser m\u00e1s antiguos que el umbral de retenci\u00f3n, y este es por defecto de siete d\u00edas. Todas las tablas delta de OneLake tienen el mismo per\u00edodo de retenci\u00f3n. El per\u00edodo de retenci\u00f3n de archivos es el mismo independientemente del motor de proceso de Fabric que estemos utilizando. Este mantenimiento es importante para optimizar el costo de almacenamiento. Establecer un per\u00edodo de retenci\u00f3n m\u00e1s corto afecta a las funcionalidades de viaje en el tiempo de Delta. </li> </ul>"},{"location":"03%20Ingenieria%20de%20Datos/03-2%20Creaci%C3%B3n%20y%20Gesti%C3%B3n%20de%20un%20lakehouse/#6-direct-lake","title":"6. Direct Lake","text":"<p>Direct Lake es una opci\u00f3n de modo de almacenamiento para las tablas de un modelo sem\u00e1ntico de Power BI almacenado en un \u00e1rea de trabajo de Microsoft Fabric. Est\u00e1 optimizado para grandes vol\u00famenes de datos que se pueden cargar r\u00e1pidamente en la memoria desde tablas Delta, las cuales almacenan sus datos en archivos Parquet en OneLake, el repositorio unificado para todos los datos anal\u00edticos. Una vez cargados en memoria, el modelo sem\u00e1ntico permite consultas de alto rendimiento, eliminando la necesidad de importar datos al modelo, lo que suele ser un proceso lento y costoso.</p> <p></p> <ul> <li>Carga R\u00e1pida de Datos: Permite la carga eficiente de grandes vol\u00famenes de datos en memoria, facilitando an\u00e1lisis m\u00e1s r\u00e1pidos.    </li> <li>Actualizaciones Eficientes: Las operaciones de actualizaci\u00f3n analizan los metadatos de las tablas Delta y actualizan el modelo para referenciar los archivos m\u00e1s recientes en OneLake, reduciendo el tiempo y los recursos necesarios en comparaci\u00f3n con las actualizaciones tradicionales.    </li> <li>Optimizaci\u00f3n de Recursos: Al eliminar la necesidad de importar datos, se reduce el consumo de recursos de capacidad, como memoria y CPU.</li> </ul> <p>Los modelos sem\u00e1nticos en modo Direct Lake se conectan a tablas o vistas de un \u00fanico lakehouse o warehouse en Fabric. Durante una operaci\u00f3n de actualizaci\u00f3n, el modelo analiza los metadatos de la versi\u00f3n m\u00e1s reciente de las tablas Delta y se actualiza para referenciar los archivos m\u00e1s recientes en OneLake. Este proceso, conocido como \"operaci\u00f3n de enmarcado\", es r\u00e1pido y de bajo costo en comparaci\u00f3n con las actualizaciones tradicionales que implican la copia completa de los datos</p>"},{"location":"03%20Ingenieria%20de%20Datos/03-2%20Creaci%C3%B3n%20y%20Gesti%C3%B3n%20de%20un%20lakehouse/#7-conclusion-y-preguntas-clave","title":"7. Conclusi\u00f3n y Preguntas Clave","text":"<p>\u2705 Un Lakehouse en Microsoft Fabric combina lo mejor de los Data Lakes y Data Warehouses. \u2705 Se puede implementar r\u00e1pidamente en OneLake y optimizar su rendimiento con Delta Lake. \u2705 Los controles de seguridad y accesos permiten restringir y gestionar usuarios de manera eficiente.</p>"},{"location":"03%20Ingenieria%20de%20Datos/03-2%20Creaci%C3%B3n%20y%20Gesti%C3%B3n%20de%20un%20lakehouse/#preguntas-para-reflexion-y-discusion","title":"Preguntas para reflexi\u00f3n y discusi\u00f3n","text":"<p>1\ufe0f\u20e3 \u00bfCu\u00e1les son las ventajas de un Lakehouse sobre un Data Warehouse en mi empresa? 2\ufe0f\u20e3 \u00bfC\u00f3mo se pueden aplicar Row-Level Security y Column-Level Security en Fabric? 3\ufe0f\u20e3 \u00bfCu\u00e1ndo usar Power BI con Direct Lake en vez de otros modelos de conexi\u00f3n?</p>"},{"location":"03%20Ingenieria%20de%20Datos/03-3%20Trabajo%20con%20Datawarehouse/","title":"Trabajando con Datawarehouse en Microsoft Fabric","text":""},{"location":"03%20Ingenieria%20de%20Datos/03-3%20Trabajo%20con%20Datawarehouse/#1-diferencias-clave-entre-un-data-warehouse-y-un-lakehouse","title":"1. Diferencias clave entre un Data Warehouse y un Lakehouse","text":""},{"location":"03%20Ingenieria%20de%20Datos/03-3%20Trabajo%20con%20Datawarehouse/#que-es-un-data-warehouse-en-microsoft-fabric","title":"\ud83d\udd39 \u00bfQu\u00e9 es un Data Warehouse en Microsoft Fabric?","text":"<p>Un Data Warehouse en Fabric es una soluci\u00f3n optimizada para almacenar, procesar y analizar datos estructurados mediante SQL. Se diferencia del Lakehouse, que permite gestionar datos estructurados y no estructurados en un solo entorno\u200b.</p> <p>\ud83d\udccc Comparaci\u00f3n entre Data Warehouse y Lakehouse en Fabric</p> Caracter\u00edstica Data Warehouse Lakehouse Tipo de datos Solo estructurados. Estructurados y no estructurados. Modelo de almacenamiento SQL tradicional con ACID y transacciones multi-tabla. Basado en Delta Lake con acceso desde Spark y SQL. Optimizaci\u00f3n Indexaci\u00f3n, vistas materializadas y cach\u00e9 de consultas. Optimizaci\u00f3n con formato Parquet y Delta Lake. Casos de uso Informes anal\u00edticos empresariales y BI. Procesamiento de Big Data y an\u00e1lisis avanzado. Interfaz principal T-SQL y Power BI DirectQuery. Spark, SQL y Data Science Notebooks. <p>\ud83d\udccc \u00bfCu\u00e1ndo usar un Data Warehouse en Fabric? \u2705 Cuando se requiere transacciones SQL multi-tabla con soporte ACID. \u2705 Para modelos de datos altamente estructurados optimizados para informes en Power BI. \u2705 Cuando se necesita una arquitectura tradicional de almacenamiento y an\u00e1lisis de datos\u200b.</p> <p>Para una gu\u00eda de decisi\u00f3n m\u00e1s detallada puedes consultar el siguiente enlace Warehouse vs lakehouse</p> <p>El Data Warehouse de Microsoft Fabric ofrece una arquitectura moderna que se caracteriza por la separaci\u00f3n entre el almacenamiento y el procesamiento. Esta separaci\u00f3n permite escalar cada componente de forma independiente, lo que resulta crucial para organizaciones que manejan grandes vol\u00famenes de datos. Gracias a esta arquitectura, el almacenamiento puede crecer de manera indefinida, mientras que los recursos de c\u00f3mputo se ajustan din\u00e1micamente a las necesidades espec\u00edficas de las cargas de trabajo.</p> <p>Otro aspecto fundamental es el uso de formatos abiertos para el almacenamiento de datos, siendo el Delta Parquet el est\u00e1ndar adoptado. Este formato no solo garantiza la interoperabilidad entre los diferentes motores anal\u00edticos, como T-SQL, Apache Spark y Analysis Services, sino que tambi\u00e9n facilita el acceso a los datos sin necesidad de procesos de transformaci\u00f3n o duplicaci\u00f3n. Esta caracter\u00edstica se traduce en una reducci\u00f3n significativa del tiempo y los recursos necesarios para preparar los datos para su an\u00e1lisis.</p>"},{"location":"03%20Ingenieria%20de%20Datos/03-3%20Trabajo%20con%20Datawarehouse/#2-creacion-de-un-data-warehouse-en-fabric","title":"2. Creaci\u00f3n de un Data Warehouse en Fabric","text":""},{"location":"03%20Ingenieria%20de%20Datos/03-3%20Trabajo%20con%20Datawarehouse/#pasos-para-crear-un-data-warehouse-en-microsoft-fabric","title":"\ud83d\udd39 Pasos para crear un Data Warehouse en Microsoft Fabric","text":"<p>1\ufe0f\u20e3 Acceder al entorno de Fabric</p> <ul> <li>Ir a Microsoft Fabric \u2192 Data Warehouse \u2192 Nuevo Data Warehouse.</li> <li>Asignar un nombre y confirmar la creaci\u00f3n\u200b.</li> </ul> <p>2\ufe0f\u20e3 Cargar datos en el Data Warehouse</p> <p><code>COPY INTO mi_warehouse.ventas FROM 'onelake://empresa/datos_ventas/' WITH (FILE_TYPE = 'PARQUET', FORMAT = 'DELTA')</code></p> <p>\ud83d\udccc Explicaci\u00f3n:</p> <ul> <li>Carga masiva de datos desde OneLake.</li> <li>Uso de formato Parquet y Delta para optimizar rendimiento.</li> </ul> <p>3\ufe0f\u20e3 Ejecuci\u00f3n de consultas SQL avanzadas</p> <p><code>SELECT categoria, SUM(ventas) AS total_ventas FROM mi_warehouse.ventas GROUP BY categoria;</code></p> <p>\ud83d\udccc Optimizaci\u00f3n para BI: Permite consultas agregadas de alto rendimiento en Power BI\u200b.</p> <p>4\ufe0f\u20e3 Integraci\u00f3n con Power BI mediante DirectQuery</p> <ul> <li>Conectar Power BI al SQL Analytics Endpoint del Data Warehouse.</li> <li>Configurar modelos sem\u00e1nticos para acceso en tiempo real\u200b.</li> </ul>"},{"location":"03%20Ingenieria%20de%20Datos/03-3%20Trabajo%20con%20Datawarehouse/#3-optimizacion-de-cargas-y-consultas-para-analisis-eficientes","title":"3. Optimizaci\u00f3n de cargas y consultas para an\u00e1lisis eficientes","text":""},{"location":"03%20Ingenieria%20de%20Datos/03-3%20Trabajo%20con%20Datawarehouse/#estrategias-para-mejorar-el-rendimiento-de-un-data-warehouse-en-fabric","title":"\ud83d\udd39 Estrategias para mejorar el rendimiento de un Data Warehouse en Fabric","text":"<p>\ud83d\udccc Optimizaci\u00f3n de cargas de datos: \u2705 Uso de COPY INTO \u2192 Permite cargas eficientes desde OneLake y otras fuentes. \u2705 Compresi\u00f3n y almacenamiento en Delta Lake \u2192 Reduce el espacio en disco y mejora tiempos de consulta. \u2705 Particionamiento de tablas \u2192 Acelera la ejecuci\u00f3n de consultas en grandes vol\u00famenes de datos\u200b.</p> <p>\ud83d\udccc Estrategias de optimizaci\u00f3n en consultas SQL: \u2705 Indexaci\u00f3n autom\u00e1tica \u2192 Fabric crea \u00edndices de manera din\u00e1mica seg\u00fan el uso de datos. \u2705 Cach\u00e9 de consultas \u2192 Mejora la velocidad en consultas recurrentes. \u2705 Uso de vistas materializadas \u2192 Reduce el tiempo de ejecuci\u00f3n en reportes anal\u00edticos\u200b.</p> <p>\ud83d\udd39 Ejemplo de optimizaci\u00f3n con vistas materializadas</p> <p><code>CREATE MATERIALIZED VIEW ventas_resumen AS SELECT categoria, SUM(ventas) AS total_ventas FROM mi_warehouse.ventas GROUP BY categoria;</code></p> <p>\ud83d\udccc Beneficio: Permite generar reportes r\u00e1pidamente sin recalcular datos en cada consulta\u200b.</p>"},{"location":"03%20Ingenieria%20de%20Datos/03-3%20Trabajo%20con%20Datawarehouse/#4-mirror-de-bases-de-datos","title":"4. Mirror de Bases de Datos","text":"<p>EL mirror en Microsoft Fabric es una soluci\u00f3n rentable y de baja latencia que permite unificar datos de diversos sistemas en una \u00fanica plataforma de an\u00e1lisis. Esta funcionalidad facilita la replicaci\u00f3n continua de tu infraestructura de datos existente directamente en OneLake de Fabric, abarcando una variedad de bases de datos de Azure y fuentes de datos externas.</p> <p>Con los datos m\u00e1s actualizados en un formato consultable en OneLake, puedes aprovechar todos los servicios que ofrece Fabric, como an\u00e1lisis con Spark, ejecuci\u00f3n de notebooks, ingenier\u00eda de datos y visualizaci\u00f3n mediante informes de Power BI, entre otros.</p> <p>La replicaci\u00f3n en Fabric ofrece una experiencia altamente integrada, de extremo a extremo y f\u00e1cil de usar, dise\u00f1ada para simplificar tus necesidades anal\u00edticas. Construida para promover la apertura y la colaboraci\u00f3n entre Microsoft y soluciones tecnol\u00f3gicas que pueden leer el formato de tablas de Delta Lake de c\u00f3digo abierto, la replicaci\u00f3n es una soluci\u00f3n llave en mano de bajo costo y baja latencia que te permite crear una r\u00e9plica de tus datos en OneLake para satisfacer todas tus necesidades anal\u00edticas.</p> <p>Las tablas Delta resultantes pueden utilizarse en todo Fabric, acelerando as\u00ed tu transici\u00f3n hacia esta plataforma.</p>"},{"location":"03%20Ingenieria%20de%20Datos/03-3%20Trabajo%20con%20Datawarehouse/#por-que-utilizarla","title":"\u00bfPor qu\u00e9 utilizarla?","text":"<p>Actualmente, muchas organizaciones poseen datos operativos o anal\u00edticos cr\u00edticos dispersos en silos. Acceder y trabajar con estos datos suele requerir complejas canalizaciones ETL (Extracci\u00f3n, Transformaci\u00f3n y Carga), procesos empresariales y decisiones aisladas, lo que genera:</p> <ul> <li>Acceso restringido y limitado a datos importantes y en constante cambio.</li> <li>Fricci\u00f3n entre personas, procesos y tecnolog\u00eda.</li> <li>Largos tiempos de espera para crear canalizaciones de datos y procesos hacia informaci\u00f3n crucial.</li> <li>Falta de libertad para utilizar las herramientas necesarias para analizar y compartir conocimientos de manera c\u00f3moda.</li> <li>Ausencia de una base adecuada para que las personas compartan y colaboren en torno a los datos.</li> <li>Falta de formatos de datos comunes y abiertos para todos los escenarios anal\u00edticos: BI, IA, integraci\u00f3n, ingenier\u00eda e incluso aplicaciones.</li> </ul> <p>La replicaci\u00f3n en Fabric proporciona una experiencia sencilla que acelera el tiempo de obtenci\u00f3n de valor para obtener conocimientos y tomar decisiones, eliminando los silos de datos entre soluciones tecnol\u00f3gicas mediante:</p> <ul> <li>Replicaci\u00f3n casi en tiempo real de datos y metadatos en un lago de datos SaaS, con an\u00e1lisis integrados para BI e IA.</li> </ul> <p>La plataforma Microsoft Fabric se basa en una arquitectura de Software como Servicio (SaaS), lo que lleva la simplicidad y la integraci\u00f3n a un nivel completamente nuevo. Para obtener m\u00e1s informaci\u00f3n sobre Microsoft Fabric, consulta \u00bfQu\u00e9 es Microsoft Fabric?.</p> <p>La replicaci\u00f3n crea tres elementos en tu espacio de trabajo de Fabric:</p> <ul> <li>OneLake: gestiona la replicaci\u00f3n de datos y metadatos en OneLake y su conversi\u00f3n a Parquet, en un formato listo para an\u00e1lisis. Esto habilita escenarios posteriores como ingenier\u00eda de datos, ciencia de datos y m\u00e1s.</li> <li>Punto de conexi\u00f3n de an\u00e1lisis SQL: proporciona una interfaz para consultas SQL.</li> <li>Modelo sem\u00e1ntico predeterminado: facilita la interpretaci\u00f3n y el an\u00e1lisis de los datos replicados.</li> </ul> <p>Adem\u00e1s del editor de consultas SQL, existe un amplio ecosistema de herramientas, incluyendo SQL Server Management Studio (SSMS), la extensi\u00f3n mssql para Visual Studio Code e incluso GitHub Copilot.</p> <p>La funcionalidad de compartir permite un control y gesti\u00f3n sencillos del acceso, asegurando que puedas controlar el acceso a informaci\u00f3n sensible. Esto tambi\u00e9n facilita una toma de decisiones segura y democratizada en toda tu organizaci\u00f3n.</p>"},{"location":"03%20Ingenieria%20de%20Datos/03-3%20Trabajo%20con%20Datawarehouse/#tipos-de-mirror","title":"Tipos de Mirror","text":"<p>Fabric ofrece tres enfoques diferentes para incorporar datos en OneLake a trav\u00e9s de la replicaci\u00f3n:</p> <ol> <li>Replicaci\u00f3n de bases de datos: permite la replicaci\u00f3n de bases de datos y tablas completas, facilitando la unificaci\u00f3n de datos de diversos sistemas en una \u00fanica plataforma anal\u00edtica.</li> <li>Replicaci\u00f3n de metadatos: sincroniza metadatos (como nombres de cat\u00e1logos, esquemas y tablas) en lugar de mover f\u00edsicamente los datos. Este enfoque utiliza accesos directos, asegurando que los datos permanezcan en su origen mientras siguen siendo f\u00e1cilmente accesibles dentro de Fabric.</li> <li>Replicaci\u00f3n abierta: dise\u00f1ada para extender la replicaci\u00f3n basada en el formato de tabla Delta Lake abierto. Esta capacidad permite a cualquier desarrollador escribir los cambios de datos de su aplicaci\u00f3n directamente en un elemento de base de datos replicada en Microsoft Fabric, basado en el enfoque de replicaci\u00f3n abierta y API p\u00fablicas.</li> </ol>"},{"location":"03%20Ingenieria%20de%20Datos/03-3%20Trabajo%20con%20Datawarehouse/#5-conclusion-y-preguntas-clave","title":"5. Conclusi\u00f3n y Preguntas Clave","text":"<p>\u2705 Un Data Warehouse en Microsoft Fabric est\u00e1 optimizado para an\u00e1lisis SQL estructurado a gran escala. \u2705 Ofrece integraci\u00f3n nativa con Power BI y herramientas de modelado SQL. \u2705 Las estrategias de optimizaci\u00f3n, como vistas materializadas e indexaci\u00f3n autom\u00e1tica, mejoran el rendimiento.</p>"},{"location":"03%20Ingenieria%20de%20Datos/03-3%20Trabajo%20con%20Datawarehouse/#preguntas-para-reflexion-y-discusion","title":"Preguntas para reflexi\u00f3n y discusi\u00f3n","text":"<p>1\ufe0f\u20e3 \u00bfCu\u00e1ndo es mejor usar un Data Warehouse en lugar de un Lakehouse en Fabric? 2\ufe0f\u20e3 \u00bfC\u00f3mo se pueden optimizar cargas de datos masivas usando COPY INTO? 3\ufe0f\u20e3 \u00bfCu\u00e1les son las ventajas de usar DirectQuery en Power BI sobre un Data Warehouse en Fabric?</p>"},{"location":"04%20Analisis%20Avanzado%20y%20Automatizacion/04-1%20Fabric%20como%20plataforma%20de%20Data%20Science/","title":"04-01 Microsoft Fabric como plataforma de Ciencia de Datos","text":"<p>Microsoft Fabric se posiciona como una plataforma unificada y escalable para la integraci\u00f3n, an\u00e1lisis y explotaci\u00f3n de datos. Su arquitectura SaaS (Software as a Service) permite a las organizaciones transformar datos en conocimiento y acciones de negocio sin necesidad de administrar infraestructura subyacente. En este cap\u00edtulo, abordaremos c\u00f3mo Fabric se adapta al desarrollo de proyectos de Ciencia de Datos, explorando sus capacidades en machine learning, inteligencia artificial y la integraci\u00f3n sem\u00e1ntica de datos. \ud83d\ude0a</p>"},{"location":"04%20Analisis%20Avanzado%20y%20Automatizacion/04-1%20Fabric%20como%20plataforma%20de%20Data%20Science/#1-introduccion-a-microsoft-fabric-para-ciencia-de-datos","title":"1. Introducci\u00f3n a Microsoft Fabric para Ciencia de Datos","text":"<p>Microsoft Fabric es una soluci\u00f3n integral que combina varias cargas de trabajo, como Data Engineering, Data Warehousing, Data Science, Real-Time Analytics y m\u00e1s. Gracias a su dise\u00f1o unificado, Fabric permite a los equipos de datos trabajar de manera colaborativa en un entorno gobernado y seguro, acelerando el proceso de transformaci\u00f3n de datos en insights.</p> <p>Entre los aspectos clave para la Ciencia de Datos se encuentran:</p> <ul> <li>Unificaci\u00f3n de datos en OneLake: Una \u00fanica ubicaci\u00f3n l\u00f3gica (OneLake) en la que se almacenan y gestionan todos los datos, eliminando silos y facilitando la reutilizaci\u00f3n.</li> <li>Interoperabilidad entre motores anal\u00edticos: Permite el acceso simult\u00e1neo mediante T\u2011SQL, Spark, y herramientas de an\u00e1lisis avanzadas como Power BI.</li> <li>Capacidades de Machine Learning y AI integradas: Fabric ofrece soporte tanto para experimentos de ML como para la integraci\u00f3n con servicios de AI, facilitando la creaci\u00f3n, entrenamiento, despliegue y monitoreo de modelos.</li> </ul> <p>Este curso se centrar\u00e1 en c\u00f3mo aprovechar estas capacidades para construir y desplegar proyectos de Ciencia de Datos en Fabric.</p>"},{"location":"04%20Analisis%20Avanzado%20y%20Automatizacion/04-1%20Fabric%20como%20plataforma%20de%20Data%20Science/#2-opciones-de-machine-learning-en-microsoft-fabric","title":"2. Opciones de Machine Learning en Microsoft Fabric","text":"<p>Una de las grandes ventajas de Microsoft Fabric es su capacidad para soportar el ciclo completo de un proyecto de Ciencia de Datos, abarcando desde la preparaci\u00f3n de datos hasta el despliegue de modelos. En esta secci\u00f3n, exploraremos las principales opciones de Machine Learning disponibles en Fabric.</p>"},{"location":"04%20Analisis%20Avanzado%20y%20Automatizacion/04-1%20Fabric%20como%20plataforma%20de%20Data%20Science/#automl-en-fabric","title":"\ud83d\udd39AutoML en Fabric","text":"<p>El AutoML (Automated Machine Learning) es una funcionalidad que automatiza muchas de las tareas complejas del desarrollo de modelos, tales como la selecci\u00f3n de algoritmos, la ingenier\u00eda de caracter\u00edsticas y la optimizaci\u00f3n de hiperpar\u00e1metros. Algunas de sus ventajas incluyen:</p> <ul> <li>Reducci\u00f3n de la barrera de entrada: Permite a usuarios con conocimientos limitados en ML generar modelos competitivos sin necesidad de codificaci\u00f3n extensiva.</li> <li>Ahorro de tiempo y recursos: Automatiza tareas repetitivas, lo que acelera el proceso de iteraci\u00f3n y experimentaci\u00f3n.</li> <li>Soporte para diversas tareas: AutoML en Fabric admite tareas de regresi\u00f3n, clasificaci\u00f3n, y forecasting, entre otros.</li> </ul> <p>Ejemplo pr\u00e1ctico: Imagina que deseas predecir el churn de clientes en una entidad financiera. Con AutoML, simplemente cargas el dataset, defines la variable objetivo y el sistema autom\u00e1ticamente probar\u00e1 distintos modelos, optimizando los par\u00e1metros y entreg\u00e1ndote el mejor resultado evaluado con m\u00e9tricas relevantes.</p>"},{"location":"04%20Analisis%20Avanzado%20y%20Automatizacion/04-1%20Fabric%20como%20plataforma%20de%20Data%20Science/#integracion-con-mlflow-para-experimentacion-y-modelos","title":"\ud83d\udd39Integraci\u00f3n con MLFlow para Experimentaci\u00f3n y Modelos","text":"<p>La integraci\u00f3n de MLFlow en Fabric permite llevar un seguimiento detallado de experimentos y gestionar el ciclo de vida de los modelos. Entre los beneficios de esta integraci\u00f3n se encuentran:</p> <ul> <li>Tracking de experimentos: Registra las configuraciones, m\u00e9tricas y par\u00e1metros de cada experimento, facilitando la comparaci\u00f3n de distintos modelos.</li> <li>Model Registry: Permite registrar y versionar modelos, de modo que se pueda controlar el despliegue y la actualizaci\u00f3n de cada versi\u00f3n.</li> <li>Interoperabilidad: Los modelos entrenados con MLFlow pueden ser utilizados tanto en notebooks como en pipelines de producci\u00f3n.</li> </ul> <p>Beneficios adicionales:</p> <ul> <li>Reproducibilidad: Al registrar cada experimento, se asegura que los resultados sean reproducibles y se puedan auditar f\u00e1cilmente.</li> <li>Colaboraci\u00f3n: Facilita la colaboraci\u00f3n entre data scientists, ya que pueden revisar, comentar y validar los experimentos de sus colegas.</li> </ul>"},{"location":"04%20Analisis%20Avanzado%20y%20Automatizacion/04-1%20Fabric%20como%20plataforma%20de%20Data%20Science/#sinergia-con-synapseml-y-otros-frameworks","title":"\ud83d\udd39Sinergia con SynapseML y Otros Frameworks","text":"<p>Adem\u00e1s de AutoML y MLFlow, Fabric integra SynapseML (anteriormente MMLSpark), una biblioteca de c\u00f3digo abierto que simplifica la construcci\u00f3n de pipelines de machine learning a gran escala. Entre sus caracter\u00edsticas destacan:</p> <ul> <li>Interfaz unificada: Permite combinar algoritmos de distintos ecosistemas (como Scikit-learn, PyTorch, ONNX) en una sola API.</li> <li>Ejecuci\u00f3n distribuida: Optimiza el entrenamiento de modelos en entornos distribuidos, aprovechando el poder de Apache Spark.</li> <li>Integraci\u00f3n con Azure AI: Facilita el uso de servicios preentrenados y recursos de Azure AI para mejorar los modelos.</li> </ul> <p>Ejemplo de uso: Un data scientist puede construir una pipeline que integre procesamiento de texto con an\u00e1lisis de sentimiento, utilizando SynapseML para el entrenamiento y luego desplegar el modelo mediante MLFlow, aprovechando todo el ecosistema de Fabric.</p>"},{"location":"04%20Analisis%20Avanzado%20y%20Automatizacion/04-1%20Fabric%20como%20plataforma%20de%20Data%20Science/#4-servicios-de-inteligencia-artificial-disponibles-en-fabric","title":"4. Servicios de Inteligencia Artificial Disponibles en Fabric","text":"<p>Adem\u00e1s de las capacidades de Machine Learning, Microsoft Fabric ofrece un conjunto robusto de servicios de inteligencia artificial que facilitan la incorporaci\u00f3n de funcionalidades avanzadas a las aplicaciones. Estas opciones se dividen en diferentes categor\u00edas.</p>"},{"location":"04%20Analisis%20Avanzado%20y%20Automatizacion/04-1%20Fabric%20como%20plataforma%20de%20Data%20Science/#ai-skills-y-ai-functions","title":"\ud83d\udd39AI Skills y AI Functions","text":"<p>Dentro de Fabric, se han integrado funcionalidades denominadas AI Skills y AI Functions. Estas ofrecen capacidades preconstruidas que permiten enriquecer los datos y extraer insights sin necesidad de desarrollar modelos desde cero.</p> <ul> <li>AI Skills:     Estas son funciones de AI preentrenadas que se pueden aplicar para tareas espec\u00edficas, como la detecci\u00f3n de sentimientos, la extracci\u00f3n de entidades o la clasificaci\u00f3n de texto.    <ul> <li>Ventajas:<ul> <li>F\u00e1cil integraci\u00f3n con pipelines de datos.</li> <li>Permite estandarizar procesos de transformaci\u00f3n y an\u00e1lisis de datos.</li> </ul> </li> <li>Ejemplo:<ul> <li>Aplicar un AI Skill para extraer entidades nombradas (NER) de comentarios de clientes y, de esta forma, categorizar y agrupar informaci\u00f3n relevante para an\u00e1lisis de satisfacci\u00f3n.</li> </ul> </li> </ul> </li> <li>AI Functions:     Estas funciones permiten ejecutar tareas de AI en tiempo real o en batch, integrando modelos de machine learning en el flujo de trabajo de datos.    <ul> <li>Ejemplo:<ul> <li>Una funci\u00f3n AI que, a partir de un modelo de predicci\u00f3n de churn, se ejecute en cada actualizaci\u00f3n del dataset para proporcionar una alerta temprana sobre clientes en riesgo.</li> </ul> </li> </ul> </li> </ul>"},{"location":"04%20Analisis%20Avanzado%20y%20Automatizacion/04-1%20Fabric%20como%20plataforma%20de%20Data%20Science/#servicios-de-azure-ai-cognitive-services-en-fabric","title":"\ud83d\udd39Servicios de Azure AI (Cognitive Services) en Fabric","text":"<p>Fabric se integra de manera nativa con los Azure Cognitive Services, ofreciendo un abanico de servicios de inteligencia artificial listos para consumir. Entre los servicios m\u00e1s destacados se encuentran:</p> <ul> <li>Azure AI Translate: <ul> <li>Permite traducir texto entre m\u00faltiples idiomas, realizar transliteraciones y detectar el idioma de origen.</li> <li>\u00datil para escenarios de an\u00e1lisis global de datos multiling\u00fces.</li> </ul> </li> <li>Azure AI Language: <ul> <li>Facilita el an\u00e1lisis de sentimientos, la extracci\u00f3n de frases clave, el reconocimiento de entidades (NER) y la detecci\u00f3n de informaci\u00f3n personal (PII).</li> <li>Ideal para la miner\u00eda de opiniones y el an\u00e1lisis de texto en redes sociales, encuestas o feedback de clientes.</li> </ul> </li> <li>Azure OpenAI: <ul> <li>Ofrece modelos de lenguaje avanzados (como GPT-35-turbo, text-davinci-003 y modelos de embeddings) que permiten generar texto, resumir informaci\u00f3n o incluso generar c\u00f3digo.</li> <li>Su integraci\u00f3n en Fabric abre la puerta a soluciones innovadoras en generaci\u00f3n de contenido, chatbots inteligentes y asistentes virtuales.</li> </ul> </li> </ul>"},{"location":"04%20Analisis%20Avanzado%20y%20Automatizacion/04-1%20Fabric%20como%20plataforma%20de%20Data%20Science/#implementacion-de-azure-ai-en-proyectos-de-ciencia-de-datos","title":"\ud83d\udd39Implementaci\u00f3n de Azure AI en Proyectos de Ciencia de Datos","text":"<p>La integraci\u00f3n de los servicios de Azure AI en Fabric se puede realizar a trav\u00e9s de dos m\u00e9todos principales:</p> <ol> <li> <p>Consumo de API REST:     Cada uno de estos servicios dispone de una API REST que permite enviar solicitudes desde cualquier lenguaje o herramienta compatible. Esta flexibilidad permite integrar f\u00e1cilmente los servicios de AI en flujos de datos y procesos de ETL.</p> </li> <li> <p>Utilizaci\u00f3n de SynapseML y SDKs:     Fabric facilita la integraci\u00f3n mediante bibliotecas como SynapseML, que ya incorporan conectores para estos servicios. Por ejemplo, se pueden utilizar funciones espec\u00edficas para llamar a Azure OpenAI o Azure Cognitive Services directamente desde un notebook, sin necesidad de escribir c\u00f3digo adicional para gestionar autenticaci\u00f3n y comunicaci\u00f3n.</p> </li> </ol> <p>Ejemplo pr\u00e1ctico: Un proyecto de an\u00e1lisis de opiniones de clientes puede utilizar Azure AI Language para extraer el sentimiento de los comentarios y, a su vez, utilizar Azure OpenAI para generar respuestas autom\u00e1ticas en un asistente virtual que interact\u00fae con los usuarios.</p>"},{"location":"04%20Analisis%20Avanzado%20y%20Automatizacion/04-1%20Fabric%20como%20plataforma%20de%20Data%20Science/#5-integracion-a-traves-de-semantic-link","title":"5. Integraci\u00f3n a trav\u00e9s de Semantic Link","text":"<p>Una caracter\u00edstica innovadora de Microsoft Fabric es la capacidad de Semantic Link, la cual facilita la integraci\u00f3n y el aprovechamiento de la sem\u00e1ntica de los datos en diferentes entornos y aplicaciones.</p>"},{"location":"04%20Analisis%20Avanzado%20y%20Automatizacion/04-1%20Fabric%20como%20plataforma%20de%20Data%20Science/#que-es-semantic-link","title":"\ud83d\udd39Qu\u00e9 es Semantic Link?","text":"<p>Semantic Link es una funcionalidad que permite:</p> <ul> <li>Reutilizaci\u00f3n de l\u00f3gica de negocio: Los modelos sem\u00e1nticos desarrollados en Power BI, por ejemplo, pueden ser accedidos y reutilizados en otros entornos sin tener que reimplementar reglas de negocio.</li> <li>Detecci\u00f3n y validaci\u00f3n de relaciones: Utilizando la librer\u00eda SemPy (disponible en Fabric), es posible detectar relaciones funcionales, dependencias y validar la consistencia de los datos.</li> <li>Integraci\u00f3n entre equipos: Facilita la colaboraci\u00f3n entre analistas y data scientists al compartir de manera transparente el conocimiento sem\u00e1ntico que reside en los modelos de datos.</li> </ul>"},{"location":"04%20Analisis%20Avanzado%20y%20Automatizacion/04-1%20Fabric%20como%20plataforma%20de%20Data%20Science/#ventajas-de-la-integracion-semantica","title":"\ud83d\udd39Ventajas de la Integraci\u00f3n Sem\u00e1ntica","text":"<p>Integrar la sem\u00e1ntica en el proceso de Ciencia de Datos aporta m\u00faltiples beneficios:</p> <ul> <li>Reducci\u00f3n de la duplicidad de esfuerzo: Una vez definido el modelo sem\u00e1ntico en una herramienta de BI, otros equipos pueden acceder a las mismas definiciones para realizar an\u00e1lisis o construir nuevos modelos.</li> <li>Mejora de la calidad de los insights: Al trabajar sobre una \u00fanica fuente de verdad sem\u00e1ntica, se reducen inconsistencias y se mejora la confiabilidad de los informes.</li> <li>Agilidad en el desarrollo: Los data scientists pueden concentrarse en el an\u00e1lisis y el desarrollo de modelos, confiando en que la capa sem\u00e1ntica ya integra el conocimiento y la l\u00f3gica del negocio.</li> </ul> <p>Ejemplo pr\u00e1ctico: Un equipo de an\u00e1lisis de ventas puede utilizar el modelo sem\u00e1ntico definido en Power BI para segmentar clientes por comportamiento, mientras que el equipo de data science utiliza Semantic Link para enriquecer el dataset con las mismas categor\u00edas y m\u00e9tricas, asegurando consistencia en ambos an\u00e1lisis.</p>"},{"location":"04%20Analisis%20Avanzado%20y%20Automatizacion/04-1%20Fabric%20como%20plataforma%20de%20Data%20Science/#6-ciclo-de-vida-de-un-proyecto-de-ciencia-de-datos-en-fabric","title":"6. Ciclo de Vida de un Proyecto de Ciencia de Datos en Fabric","text":"<p>A continuaci\u00f3n se describe el flujo t\u00edpico de un proyecto de Ciencia de Datos utilizando Microsoft Fabric, integrando las capacidades de ML, AI y Semantic Link.</p>"},{"location":"04%20Analisis%20Avanzado%20y%20Automatizacion/04-1%20Fabric%20como%20plataforma%20de%20Data%20Science/#descubrimiento-y-preparacion-de-datos","title":"\ud83d\udd39Descubrimiento y Preparaci\u00f3n de Datos","text":"<ul> <li>Ingesta de datos: Utilizando Data Factory y pipelines, se ingieren datos desde m\u00faltiples fuentes (on-premises, cloud, etc.) hacia OneLake.</li> <li>Transformaci\u00f3n y limpieza: Con herramientas como Data Wrangler y notebooks, se realiza la preparaci\u00f3n de datos, incluyendo la detecci\u00f3n de valores nulos, normalizaci\u00f3n y transformaci\u00f3n de formatos.</li> <li>Modelado sem\u00e1ntico: Se crean modelos sem\u00e1nticos en Power BI o mediante herramientas espec\u00edficas de Fabric para dar contexto a los datos.</li> </ul>"},{"location":"04%20Analisis%20Avanzado%20y%20Automatizacion/04-1%20Fabric%20como%20plataforma%20de%20Data%20Science/#desarrollo-y-experimentacion-de-modelos","title":"\ud83d\udd39Desarrollo y Experimentaci\u00f3n de Modelos","text":"<ul> <li>Selecci\u00f3n del enfoque de ML:<ul> <li>AutoML: Para escenarios r\u00e1pidos y sin necesidad de ajustar manualmente par\u00e1metros.</li> <li>Desarrollo customizado: Utilizando notebooks en Python o Scala, donde se implementan pipelines con SynapseML y se integran con MLFlow para el seguimiento de experimentos.</li> </ul> </li> <li>Entrenamiento y evaluaci\u00f3n:<ul> <li>Se configuran experimentos de entrenamiento, registrando cada ejecuci\u00f3n en MLFlow.</li> <li>Se comparan modelos en base a m\u00e9tricas clave como la precisi\u00f3n, F1 score, AUC, entre otras.</li> </ul> </li> <li>Optimizaci\u00f3n de hiperpar\u00e1metros: Con herramientas integradas (como flaml.tune en Fabric) se realiza el ajuste fino del modelo.</li> </ul>"},{"location":"04%20Analisis%20Avanzado%20y%20Automatizacion/04-1%20Fabric%20como%20plataforma%20de%20Data%20Science/#enriquecimiento-y-despliegue","title":"\ud83d\udd39Enriquecimiento y Despliegue","text":"<ul> <li>Integraci\u00f3n de AI Services:<ul> <li>Una vez entrenado el modelo, se puede enriquecer la soluci\u00f3n utilizando Azure Cognitive Services o Azure OpenAI para tareas complementarias (por ejemplo, an\u00e1lisis de sentimiento en comentarios o generaci\u00f3n de texto explicativo).</li> </ul> </li> <li>Publicaci\u00f3n y consumo:<ul> <li>Los modelos se publican y versionan mediante el Model Registry de MLFlow.</li> <li>Se integran en aplicaciones o dashboards en Power BI, utilizando el modo Direct Lake para obtener insights en tiempo real.</li> </ul> </li> <li>Monitoreo y mantenimiento:<ul> <li>Se configuran pipelines de reentrenamiento y actualizaciones autom\u00e1ticas basadas en la evoluci\u00f3n de los datos.</li> <li>El uso de Semantic Link garantiza que cualquier cambio en el modelo sem\u00e1ntico se refleje en todas las aplicaciones conectadas.</li> </ul> </li> </ul>"},{"location":"04%20Analisis%20Avanzado%20y%20Automatizacion/04-1%20Fabric%20como%20plataforma%20de%20Data%20Science/#7-caso-practico-implementacion-de-un-proyecto-de-prediccion-de-churn","title":"7. Caso Pr\u00e1ctico: Implementaci\u00f3n de un Proyecto de Predicci\u00f3n de Churn","text":"<p>Para ilustrar c\u00f3mo se pueden integrar todas estas capacidades, describiremos brevemente un caso pr\u00e1ctico.</p>"},{"location":"04%20Analisis%20Avanzado%20y%20Automatizacion/04-1%20Fabric%20como%20plataforma%20de%20Data%20Science/#objetivo-del-proyecto","title":"\ud83d\udd39Objetivo del Proyecto","text":"<p>El objetivo es predecir el churn (abandono) de clientes en una empresa del sector financiero. El proceso abarca desde la ingesti\u00f3n de datos hasta la visualizaci\u00f3n de insights.</p>"},{"location":"04%20Analisis%20Avanzado%20y%20Automatizacion/04-1%20Fabric%20como%20plataforma%20de%20Data%20Science/#flujo-del-proyecto","title":"\ud83d\udd39Flujo del Proyecto","text":"<ol> <li> <p>Ingesta y Preparaci\u00f3n de Datos:</p> <ul> <li>Origen: Datos transaccionales, encuestas y registros de interacci\u00f3n.</li> <li>Herramientas: Data Factory para la ingesta, OneLake como repositorio y Data Wrangler para la limpieza.</li> <li>Modelado Sem\u00e1ntico: Creaci\u00f3n de un modelo en Power BI que defina categor\u00edas clave (segmento de cliente, canales de contacto, etc.).</li> <li> <p>Desarrollo del Modelo Predictivo:</p> </li> <li> <p>AutoML: Se ejecuta un proceso AutoML para evaluar diferentes algoritmos de clasificaci\u00f3n y seleccionar el modelo m\u00e1s prometedor.</p> </li> <li>MLFlow: Cada experimento se registra en MLFlow, permitiendo comparar m\u00e9tricas y elegir la mejor versi\u00f3n del modelo.</li> <li>SynapseML: Se utiliza para crear un pipeline distribuido de entrenamiento que escale con el volumen de datos.</li> <li> <p>Integraci\u00f3n de Servicios AI:</p> </li> <li> <p>Azure AI Language: Se integra para analizar el sentimiento de los comentarios de clientes y enriquecer el dataset.</p> </li> <li>Azure OpenAI: Se puede utilizar para generar reportes autom\u00e1ticos que expliquen los motivos del churn en lenguaje natural.</li> <li> <p>Implementaci\u00f3n de Semantic Link:</p> </li> <li> <p>Se vincula el modelo sem\u00e1ntico desarrollado en Power BI con los notebooks de data science, asegurando que la segmentaci\u00f3n y las m\u00e9tricas definidas sean consistentes en todo el an\u00e1lisis.</p> </li> <li> <p>Despliegue y Visualizaci\u00f3n:</p> </li> <li> <p>El modelo se despliega a trav\u00e9s de un pipeline en Fabric, actualizando autom\u00e1ticamente los dashboards en Power BI utilizando Direct Lake.</p> </li> <li>Se configuran alertas para notificar a los equipos de marketing y atenci\u00f3n al cliente cuando se detecta un alto riesgo de churn.</li> </ul> </li> </ol>"},{"location":"04%20Analisis%20Avanzado%20y%20Automatizacion/04-1%20Fabric%20como%20plataforma%20de%20Data%20Science/#beneficios-obtenidos","title":"\ud83d\udd39Beneficios Obtenidos","text":"<ul> <li>Aceleraci\u00f3n del proceso de an\u00e1lisis: Gracias a AutoML y la integraci\u00f3n con MLFlow, el ciclo de experimentaci\u00f3n se redujo significativamente.</li> <li>Consistencia en la informaci\u00f3n: La utilizaci\u00f3n de Semantic Link garantiz\u00f3 que tanto los analistas como los data scientists trabajaran sobre una misma definici\u00f3n de m\u00e9tricas y segmentaciones.</li> <li>Respuesta en tiempo real: La integraci\u00f3n de Direct Lake en Power BI permiti\u00f3 que los insights estuvieran siempre actualizados, facilitando decisiones \u00e1giles.</li> </ul>"},{"location":"04%20Analisis%20Avanzado%20y%20Automatizacion/04-1%20Fabric%20como%20plataforma%20de%20Data%20Science/#8-buenas-practicas-y-consideraciones","title":"8. Buenas Pr\u00e1cticas y Consideraciones","text":"<p>Al trabajar con Microsoft Fabric para Ciencia de Datos es importante tener en cuenta algunas buenas pr\u00e1cticas:</p> <ul> <li>Definir una estrategia de gobernanza:     Aprovecha las capacidades de seguridad y gobernanza integradas en OneLake para asegurar que los datos se gestionen de forma centralizada y segura.    </li> <li>Documentar los experimentos:     Utiliza MLFlow para mantener un registro detallado de todos los experimentos, de modo que cada modelo y cada iteraci\u00f3n queden documentados para futuras auditor\u00edas y mejoras.    </li> <li>Fomentar la colaboraci\u00f3n entre equipos:     Aprovecha las integraciones sem\u00e1nticas y la capacidad de compartir modelos y pipelines para mejorar la comunicaci\u00f3n entre analistas, data engineers y data scientists.    </li> <li>Automatizar procesos recurrentes:     Configura pipelines de reentrenamiento y despliegue autom\u00e1tico para que el sistema se adapte a la evoluci\u00f3n de los datos sin intervenci\u00f3n manual constante.    </li> <li>Optimizaci\u00f3n de recursos:     Monitoriza el rendimiento de los modelos y los pipelines para ajustar la capacidad de c\u00f3mputo, aprovechando las funcionalidades de autoscaling y administraci\u00f3n de capacidad en Fabric.</li> </ul>"},{"location":"04%20Analisis%20Avanzado%20y%20Automatizacion/04-1%20Fabric%20como%20plataforma%20de%20Data%20Science/#9-conclusiones","title":"9. Conclusiones","text":"<p>Microsoft Fabric se presenta como una plataforma revolucionaria para el desarrollo de proyectos de Ciencia de Datos. Su arquitectura unificada, basada en OneLake, permite la integraci\u00f3n de m\u00faltiples motores de procesamiento y an\u00e1lisis, facilitando un flujo de trabajo continuo desde la ingesta de datos hasta el despliegue de modelos de machine learning.</p> <p>Las opciones de Machine Learning \u2013 desde el AutoML para usuarios sin experiencia  en ML, pasando por la integraci\u00f3n con MLFlow para la gesti\u00f3n de experimentos, hasta la utilizaci\u00f3n de SynapseML para pipelines distribuidos \u2013 ofrecen a los equipos la flexibilidad necesaria para afrontar proyectos de distintos tama\u00f1os y complejidades.</p> <p>Adem\u00e1s, la integraci\u00f3n de servicios de AI de Azure (como Azure AI Translate, Azure AI Language y Azure OpenAI) ampl\u00eda las capacidades de la plataforma, permitiendo enriquecer los datos y proporcionar soluciones de inteligencia artificial avanzadas sin necesidad de desarrollar modelos complejos desde cero.</p> <p>Por \u00faltimo, Semantic Link cierra el ciclo al permitir que la sem\u00e1ntica y la l\u00f3gica de negocio definidas en herramientas de BI se integren de forma transparente en los procesos de ciencia de datos, garantizando consistencia y eficiencia en la generaci\u00f3n de insights.</p> <p>En resumen, Microsoft Fabric no solo simplifica el proceso de desarrollo y despliegue de proyectos de Ciencia de Datos, sino que tambi\u00e9n fomenta la colaboraci\u00f3n y la reutilizaci\u00f3n del conocimiento, convirti\u00e9ndolo en una opci\u00f3n ideal para organizaciones que buscan transformar grandes vol\u00famenes de datos en decisiones de negocio informadas y precisas. \ud83d\ude80</p>"},{"location":"04%20Analisis%20Avanzado%20y%20Automatizacion/04-2%20An%C3%A1lisis%20de%20Datos%20en%20Tiempo%20Real%20con%20Microsoft%20Fabric/","title":"04-02Anal\u00edtica en Tiempo Real en Microsoft Fabric","text":""},{"location":"04%20Analisis%20Avanzado%20y%20Automatizacion/04-2%20An%C3%A1lisis%20de%20Datos%20en%20Tiempo%20Real%20con%20Microsoft%20Fabric/#1-introduccion-a-la-analitica-en-tiempo-real","title":"1. Introducci\u00f3n a la Anal\u00edtica en Tiempo Real","text":""},{"location":"04%20Analisis%20Avanzado%20y%20Automatizacion/04-2%20An%C3%A1lisis%20de%20Datos%20en%20Tiempo%20Real%20con%20Microsoft%20Fabric/#que-es-la-analitica-en-tiempo-real","title":"\ud83d\udd39Qu\u00e9 es la Anal\u00edtica en Tiempo Real?","text":"<p>La anal\u00edtica en tiempo real se refiere a la capacidad de procesar, analizar y visualizar datos a medida que se generan o se reciben, sin la necesidad de esperar a que se completen procesos batch o se acumulen grandes vol\u00famenes de datos. Esto permite a las organizaciones obtener insights instant\u00e1neos, reaccionar de inmediato ante eventos y tomar decisiones fundamentadas en informaci\u00f3n actualizada al instante.</p> <ul> <li>Definici\u00f3n:     Es el proceso de transformar datos en movimiento en informaci\u00f3n accionable en el mismo instante en que se producen los eventos.</li> <li>Importancia:     En un mundo cada vez m\u00e1s competitivo y din\u00e1mico, la velocidad en la toma de decisiones es clave. La anal\u00edtica en tiempo real ayuda a:<ul> <li>Detectar y responder r\u00e1pidamente a incidencias.</li> <li>Optimizar procesos operativos.</li> <li>Mejorar la experiencia del cliente mediante respuestas inmediatas.</li> </ul> </li> <li>Ejemplos comunes:<ul> <li>Monitorizaci\u00f3n de transacciones financieras para detectar fraudes.</li> <li>Seguimiento de flotas de veh\u00edculos en tiempo real en el sector log\u00edstico.</li> <li>Gesti\u00f3n de inventario en retail y supervisi\u00f3n de procesos en manufactura.</li> </ul> </li> </ul>"},{"location":"04%20Analisis%20Avanzado%20y%20Automatizacion/04-2%20An%C3%A1lisis%20de%20Datos%20en%20Tiempo%20Real%20con%20Microsoft%20Fabric/#contexto-y-necesidad-en-la-era-digital","title":"\ud83d\udd39Contexto y Necesidad en la Era Digital","text":"<p>El auge de tecnolog\u00edas como IoT, la creciente cantidad de dispositivos conectados y el aumento exponencial de datos generan la necesidad de poder analizar y actuar sobre estos datos en tiempo real.</p> <ul> <li>Big Data e IoT:     La proliferaci\u00f3n de dispositivos IoT y sensores en entornos industriales, urbanos y de consumo genera grandes vol\u00famenes de datos. Analizar estos datos en el instante en que se producen es esencial para obtener ventajas competitivas.</li> <li>Toma de Decisiones Inmediata:     En sectores como finanzas, salud, retail y manufactura, una decisi\u00f3n retrasada puede representar p\u00e9rdidas econ\u00f3micas o impactos negativos en la seguridad y satisfacci\u00f3n del cliente.</li> <li>Beneficios de la Inmediatez:<ul> <li>Mejora en la eficiencia operativa: Identificaci\u00f3n temprana de cuellos de botella y problemas en procesos cr\u00edticos.</li> <li>Optimizaci\u00f3n de recursos: Ajuste din\u00e1mico de operaciones y asignaci\u00f3n de recursos en funci\u00f3n de la demanda instant\u00e1nea.</li> <li>Experiencias personalizadas: Capacidad para interactuar con clientes en el momento preciso, brindando ofertas o soluciones personalizadas.</li> </ul> </li> </ul>"},{"location":"04%20Analisis%20Avanzado%20y%20Automatizacion/04-2%20An%C3%A1lisis%20de%20Datos%20en%20Tiempo%20Real%20con%20Microsoft%20Fabric/#2-arquitecturas-para-la-analitica-en-tiempo-real","title":"2. Arquitecturas para la Anal\u00edtica en Tiempo Real","text":""},{"location":"04%20Analisis%20Avanzado%20y%20Automatizacion/04-2%20An%C3%A1lisis%20de%20Datos%20en%20Tiempo%20Real%20con%20Microsoft%20Fabric/#diferencia-entre-arquitectura-tradicional-y-arquitectura-en-tiempo-real","title":"\ud83d\udd39Diferencia entre Arquitectura Tradicional y Arquitectura en Tiempo Real","text":"<p>Las arquitecturas tradicionales de an\u00e1lisis de datos suelen basarse en procesos batch (por lotes), donde los datos se recopilan, se almacenan y se procesan en per\u00edodos determinados (diarios, semanales, etc.). Este enfoque es adecuado para an\u00e1lisis hist\u00f3ricos y reportes peri\u00f3dicos, pero resulta insuficiente para escenarios donde la inmediatez es clave.</p> <p>En contraste, la arquitectura en tiempo real est\u00e1 dise\u00f1ada para manejar datos de forma continua y ofrecer respuestas inmediatas. Algunas diferencias principales son:</p> <ul> <li>Retraso en la actualizaci\u00f3n:<ul> <li>Batch: Puede haber horas o d\u00edas de retraso en la actualizaci\u00f3n de los datos.</li> <li>Tiempo Real: Los datos se actualizan en segundos o milisegundos.</li> </ul> </li> <li>Tipo de procesamiento:<ul> <li>Batch: Procesamiento por lotes, ideal para grandes vol\u00famenes de datos hist\u00f3ricos.</li> <li>Tiempo Real: Procesamiento de streams o flujos continuos de datos, permitiendo un an\u00e1lisis instant\u00e1neo.</li> </ul> </li> <li>Casos de uso:<ul> <li>Batch: Reportes peri\u00f3dicos, an\u00e1lisis hist\u00f3rico.</li> <li>Tiempo Real: Monitorizaci\u00f3n de sistemas, detecci\u00f3n de fraudes, an\u00e1lisis de tendencias inmediatas.</li> </ul> </li> </ul>"},{"location":"04%20Analisis%20Avanzado%20y%20Automatizacion/04-2%20An%C3%A1lisis%20de%20Datos%20en%20Tiempo%20Real%20con%20Microsoft%20Fabric/#modelos-arquitectonicos-para-procesamiento-en-tiempo-real","title":"\ud83d\udd39Modelos Arquitect\u00f3nicos para Procesamiento en Tiempo Real","text":"<p>Dentro de las arquitecturas de an\u00e1lisis en tiempo real, existen varios modelos que permiten organizar y estructurar el procesamiento de datos en movimiento. Dos de los enfoques m\u00e1s comunes son:</p>"},{"location":"04%20Analisis%20Avanzado%20y%20Automatizacion/04-2%20An%C3%A1lisis%20de%20Datos%20en%20Tiempo%20Real%20con%20Microsoft%20Fabric/#arquitectura-lambda","title":"Arquitectura Lambda","text":"<ul> <li>Concepto:     La arquitectura Lambda combina dos caminos de procesamiento:<ul> <li>Batch Layer: Procesa grandes vol\u00famenes de datos en intervalos regulares para generar vistas hist\u00f3ricas.</li> <li>Speed Layer (o Real-Time Layer): Procesa datos en tiempo real para ofrecer resultados inmediatos.</li> </ul> </li> <li>Ventajas:<ul> <li>Permite tener tanto an\u00e1lisis hist\u00f3ricos como en tiempo real.</li> <li>Asegura la exactitud al combinar resultados batch con resultados instant\u00e1neos.</li> </ul> </li> <li>Desaf\u00edos:<ul> <li>Mayor complejidad en la integraci\u00f3n de ambos caminos.</li> <li>Duplicaci\u00f3n de l\u00f3gica de procesamiento.</li> </ul> </li> </ul>"},{"location":"04%20Analisis%20Avanzado%20y%20Automatizacion/04-2%20An%C3%A1lisis%20de%20Datos%20en%20Tiempo%20Real%20con%20Microsoft%20Fabric/#arquitectura-kappa","title":"Arquitectura Kappa","text":"<ul> <li>Concepto:     La arquitectura Kappa propone un \u00fanico camino para el procesamiento de datos: todo se procesa como un flujo de datos en tiempo real.</li> <li>Ventajas:<ul> <li>Simplifica la arquitectura al eliminar la necesidad de mantener dos sistemas paralelos.</li> <li>Permite re-procesar datos hist\u00f3ricos utilizando el mismo mecanismo que los datos en vivo.</li> </ul> </li> <li>Desaf\u00edos:<ul> <li>Puede requerir sistemas de almacenamiento y procesamiento muy eficientes para manejar tanto datos hist\u00f3ricos como en tiempo real.</li> </ul> </li> </ul>"},{"location":"04%20Analisis%20Avanzado%20y%20Automatizacion/04-2%20An%C3%A1lisis%20de%20Datos%20en%20Tiempo%20Real%20con%20Microsoft%20Fabric/#componentes-clave-en-una-arquitectura-de-analisis-en-tiempo-real","title":"\ud83d\udd39Componentes Clave en una Arquitectura de An\u00e1lisis en Tiempo Real","text":"<p>Para dise\u00f1ar una arquitectura de an\u00e1lisis en tiempo real, se deben considerar los siguientes componentes:</p> <ul> <li> <p>Ingesta de Datos:     Herramientas y mecanismos que permiten capturar datos en movimiento desde diversas fuentes (sensores, logs, aplicaciones, etc.). Ejemplos: Apache Kafka, Event Hubs.</p> </li> <li> <p>Procesamiento en Streaming:     Motores que procesan los datos conforme llegan, permitiendo realizar transformaciones, filtrados y agregaciones en tiempo real. Ejemplos: Apache Spark Streaming, Flink.</p> </li> <li> <p>Almacenamiento en Tiempo Real:     Bases de datos o almacenes de datos dise\u00f1ados para almacenar grandes vol\u00famenes de datos de manera eficiente y con baja latencia. Ejemplos: bases de datos NoSQL, almacenes de datos en memoria.</p> </li> <li> <p>Visualizaci\u00f3n y Dashboards:     Herramientas de BI que permiten crear dashboards en tiempo real para monitorizar los datos. Ejemplos: Power BI, Tableau.</p> </li> <li> <p>Consultas y An\u00e1lisis:     Lenguajes y herramientas de consulta (como KQL \u2013 Kusto Query Language) que permiten analizar r\u00e1pidamente la informaci\u00f3n.</p> </li> <li> <p>Integraci\u00f3n y Orquestaci\u00f3n:     Sistemas que permiten coordinar y conectar todos los componentes anteriores, asegurando un flujo continuo de datos desde la fuente hasta la visualizaci\u00f3n.</p> </li> </ul>"},{"location":"04%20Analisis%20Avanzado%20y%20Automatizacion/04-2%20An%C3%A1lisis%20de%20Datos%20en%20Tiempo%20Real%20con%20Microsoft%20Fabric/#3-la-arquitectura-de-analitica-en-tiempo-real-en-microsoft-fabric","title":"3. La Arquitectura de Anal\u00edtica en Tiempo Real en Microsoft Fabric","text":"<p>Microsoft Fabric ha sido dise\u00f1ado para unificar diversas cargas de trabajo de an\u00e1lisis de datos en una \u00fanica plataforma SaaS. En el contexto de la anal\u00edtica en tiempo real, Fabric ofrece una serie de componentes y funcionalidades que simplifican la ingesta, el procesamiento y la visualizaci\u00f3n de datos en movimiento.</p>"},{"location":"04%20Analisis%20Avanzado%20y%20Automatizacion/04-2%20An%C3%A1lisis%20de%20Datos%20en%20Tiempo%20Real%20con%20Microsoft%20Fabric/#microsoft-fabric-una-plataforma-unificada","title":"\ud83d\udd39Microsoft Fabric: Una Plataforma Unificada","text":"<p>Microsoft Fabric integra m\u00faltiples experiencias de an\u00e1lisis de datos, como Data Engineering, Data Warehouse, Data Science, y Real-Time Intelligence, dentro de una \u00fanica soluci\u00f3n basada en la nube. Algunas de sus ventajas son:</p> <ul> <li>Integraci\u00f3n Nativa:     No es necesario combinar soluciones de diferentes proveedores, ya que Fabric integra de manera nativa los procesos de ingesta, transformaci\u00f3n y visualizaci\u00f3n.</li> <li>Unificaci\u00f3n de Datos:     Todos los datos, ya sean estructurados, semiestructurados o en texto, se almacenan en un \u00fanico repositorio centralizado llamado OneLake, lo que elimina los silos y facilita la colaboraci\u00f3n.</li> <li>Simplicidad y Escalabilidad:     Fabric simplifica la complejidad inherente a la gesti\u00f3n de infraestructura, permitiendo que los usuarios se concentren en el an\u00e1lisis y la toma de decisiones.</li> </ul>"},{"location":"04%20Analisis%20Avanzado%20y%20Automatizacion/04-2%20An%C3%A1lisis%20de%20Datos%20en%20Tiempo%20Real%20con%20Microsoft%20Fabric/#componentes-de-analitica-en-tiempo-real-en-fabric","title":"\ud83d\udd39Componentes de Anal\u00edtica en Tiempo Real en Fabric","text":"<p>Dentro de Microsoft Fabric, la anal\u00edtica en tiempo real se compone de varios elementos que trabajan conjuntamente para transformar datos en movimiento en insights accionables. A continuaci\u00f3n, se describen los componentes clave:</p>"},{"location":"04%20Analisis%20Avanzado%20y%20Automatizacion/04-2%20An%C3%A1lisis%20de%20Datos%20en%20Tiempo%20Real%20con%20Microsoft%20Fabric/#eventstream","title":"EventStream","text":"<ul> <li>Descripci\u00f3n:     Es el componente encargado de gestionar la ingesta de datos en streaming. Permite capturar eventos de diversas fuentes y llevarlos al sistema de an\u00e1lisis.</li> <li>Caracter\u00edsticas:<ul> <li>Ingesta Autom\u00e1tica: La ingesta de datos en streaming est\u00e1 habilitada por defecto, lo que facilita la integraci\u00f3n con or\u00edgenes de eventos tanto internos como externos.</li> <li>Indexaci\u00f3n por Defecto: Todos los datos ingeridos se indexan autom\u00e1ticamente, permitiendo consultas r\u00e1pidas y eficientes.</li> <li>Particionado de Datos: Los datos se particionan por tiempo y hash, lo que optimiza el rendimiento de las consultas.</li> </ul> </li> <li>Beneficios:<ul> <li>Reducci\u00f3n de la Complejidad: El manejo autom\u00e1tico de la ingesta y el particionado simplifica la arquitectura.</li> <li>Escalabilidad: Capacidad para manejar grandes vol\u00famenes de datos sin degradar el rendimiento.</li> </ul> </li> <li>Emoticono: \u00a1Captura cada evento en el momento preciso! \ud83d\udce1</li> </ul>"},{"location":"04%20Analisis%20Avanzado%20y%20Automatizacion/04-2%20An%C3%A1lisis%20de%20Datos%20en%20Tiempo%20Real%20con%20Microsoft%20Fabric/#eventhouse","title":"EventHouse","text":"<ul> <li>Descripci\u00f3n:     Aunque el t\u00e9rmino \u201ceventhouse\u201d puede no ser tan conocido fuera del ecosistema Fabric, en este contexto se refiere al almacenamiento y la gesti\u00f3n l\u00f3gica de los eventos procesados.</li> <li>Caracter\u00edsticas:<ul> <li>Almacenamiento L\u00f3gico: Permite organizar y estructurar los eventos para facilitar su an\u00e1lisis.</li> <li>Integraci\u00f3n con otros Componentes: Funciona de manera complementaria con EventStream, ofreciendo una copia l\u00f3gica de los datos para an\u00e1lisis adicionales.</li> </ul> </li> <li>Beneficios:<ul> <li>Optimizaci\u00f3n del Procesamiento: Facilita la transformaci\u00f3n y el filtrado de los datos en tiempo real.</li> </ul> </li> </ul>"},{"location":"04%20Analisis%20Avanzado%20y%20Automatizacion/04-2%20An%C3%A1lisis%20de%20Datos%20en%20Tiempo%20Real%20con%20Microsoft%20Fabric/#real-time-hub","title":"Real-Time Hub","text":"<ul> <li>Descripci\u00f3n:     Es la interfaz centralizada en Microsoft Fabric para gestionar y consumir datos en tiempo real.</li> <li>Caracter\u00edsticas:<ul> <li>Integraci\u00f3n Completa: Est\u00e1 integrado con todos los componentes de ingesta y procesamiento, lo que permite a los usuarios descubrir, gestionar y consumir datos en movimiento.</li> <li>Mirroring a OneLake: Los datos procesados en tiempo real pueden replicarse autom\u00e1ticamente en OneLake, facilitando su reutilizaci\u00f3n en otros escenarios de an\u00e1lisis.</li> <li>Transformaciones In-Place: Permite realizar transformaciones directamente sobre los datos sin necesidad de moverlos a otro repositorio.</li> </ul> </li> <li>Beneficios:<ul> <li>Acceso R\u00e1pido: Facilita la consulta inmediata de los datos para generar insights al instante.</li> <li>Interoperabilidad: Los datos del Real-Time Hub pueden ser consumidos por otros componentes de Fabric, como dashboards y herramientas de visualizaci\u00f3n.</li> </ul> </li> </ul>"},{"location":"04%20Analisis%20Avanzado%20y%20Automatizacion/04-2%20An%C3%A1lisis%20de%20Datos%20en%20Tiempo%20Real%20con%20Microsoft%20Fabric/#dashboards-en-tiempo-real-y-power-bi","title":"Dashboards en Tiempo Real y Power BI","text":"<ul> <li>Descripci\u00f3n:     Microsoft Fabric se integra de forma nativa con Power BI para ofrecer dashboards en tiempo real.</li> <li>Caracter\u00edsticas:<ul> <li>Visualizaci\u00f3n Din\u00e1mica: Permite crear reportes y dashboards que se actualizan de forma continua, mostrando informaci\u00f3n en tiempo real.</li> <li>Interactividad: Los usuarios pueden interactuar con los datos, aplicar filtros y profundizar en los detalles sin necesidad de recargar la p\u00e1gina.</li> <li>Integraci\u00f3n con Or\u00edgenes de Eventos: Gracias a la integraci\u00f3n nativa con EventStream y el Real-Time Hub, los dashboards reflejan los datos m\u00e1s recientes.</li> </ul> </li> <li>Beneficios:<ul> <li>Decisiones Instant\u00e1neas: Los l\u00edderes empresariales pueden tomar decisiones basadas en datos actualizados al minuto.</li> <li>Mayor Transparencia: Facilita el seguimiento de KPIs y m\u00e9tricas cr\u00edticas en tiempo real.</li> </ul> </li> </ul>"},{"location":"04%20Analisis%20Avanzado%20y%20Automatizacion/04-2%20An%C3%A1lisis%20de%20Datos%20en%20Tiempo%20Real%20con%20Microsoft%20Fabric/#kql-kusto-query-language","title":"KQL (Kusto Query Language)","text":"<ul> <li>Descripci\u00f3n:     KQL es un lenguaje de consulta optimizado para el an\u00e1lisis de grandes vol\u00famenes de datos en tiempo real.</li> <li>Caracter\u00edsticas:<ul> <li>Consultas R\u00e1pidas y Eficientes: Permite realizar b\u00fasquedas y an\u00e1lisis complejos con baja latencia.</li> <li>Flexibilidad: Soporta consultas sobre datos estructurados, semiestructurados y en texto.</li> <li>Integraci\u00f3n Nativa: Se integra de forma natural con los datos almacenados en el Real-Time Hub y OneLake.</li> </ul> </li> <li>Beneficios:<ul> <li>Profundidad Anal\u00edtica: Los usuarios pueden extraer insights complejos y detallados de los datos en tiempo real.</li> <li>Interactividad: Facilita la creaci\u00f3n de an\u00e1lisis ad hoc que pueden ajustar las respuestas seg\u00fan las necesidades inmediatas.</li> </ul> </li> </ul>"},{"location":"04%20Analisis%20Avanzado%20y%20Automatizacion/04-2%20An%C3%A1lisis%20de%20Datos%20en%20Tiempo%20Real%20con%20Microsoft%20Fabric/#integracion-y-flujo-de-datos-en-fabric","title":"\ud83d\udd39Integraci\u00f3n y Flujo de Datos en Fabric","text":"<p>En Microsoft Fabric, el flujo de datos para la anal\u00edtica en tiempo real se puede resumir en los siguientes pasos:</p> <ol> <li> <p>Captura de Datos en Tiempo Real:</p> <ul> <li>Los datos se capturan mediante EventStream a partir de diversas fuentes (por ejemplo, dispositivos IoT, logs de aplicaciones, cambios en bases de datos, etc.).</li> <li>La ingesta se realiza de forma autom\u00e1tica y est\u00e1 optimizada para manejar grandes vol\u00famenes sin intervenci\u00f3n manual.</li> <li>Emoticono: \u00a1Cada dato cuenta desde el primer instante! \u23f1\ufe0f</li> <li> <p>Procesamiento y Transformaci\u00f3n:</p> </li> <li> <p>Una vez capturados, los datos se procesan en streaming. Esto incluye transformaciones in-place, filtrados, agregaciones y particionamiento (por tiempo y hash).</p> </li> <li>Este procesamiento se lleva a cabo en el Real-Time Hub, lo que permite que los datos est\u00e9n listos para ser consultados casi de inmediato.</li> </ul> </li> <li> <p>Almacenamiento y Mirroring:</p> <ul> <li>Los datos procesados se almacenan l\u00f3gicamente en el EventHouse y, de forma opcional, se replican (mirroring) en OneLake para integrarlos con otras cargas de trabajo.</li> <li>Este mecanismo de replicaci\u00f3n asegura que los datos en tiempo real puedan ser consultados y analizados desde m\u00faltiples perspectivas, sin duplicaci\u00f3n innecesaria.</li> </ul> </li> <li> <p>Consulta y Visualizaci\u00f3n:</p> <ul> <li>Mediante KQL y otras herramientas de consulta, los usuarios pueden analizar los datos en tiempo real directamente desde el Real-Time Hub.</li> <li>Los resultados se pueden visualizar en dashboards interactivos a trav\u00e9s de Power BI, permitiendo un seguimiento continuo de KPIs y m\u00e9tricas cr\u00edticas.</li> </ul> </li> </ol> <pre><code>graph TD\n    subgraph Fuentes de Datos\n        A[Fuente de Datos 1]\n        B[Fuente de Datos 2]\n        C[Fuente de Datos 3]\n    end\n    A --&gt;|Env\u00eda datos| D[EventStream]\n    B --&gt; D\n    C --&gt; D\n    D --&gt;|Procesa datos| E[Real-Time Hub]\n    E --&gt;|Replica datos| F[OneLake]\n    F --&gt;|Visualiza| G[Dashboards Power BI]\n</code></pre>"},{"location":"04%20Analisis%20Avanzado%20y%20Automatizacion/04-2%20An%C3%A1lisis%20de%20Datos%20en%20Tiempo%20Real%20con%20Microsoft%20Fabric/#4-conclusiones-y-perspectivas-futuras","title":"4. Conclusiones y Perspectivas Futuras","text":""},{"location":"04%20Analisis%20Avanzado%20y%20Automatizacion/04-2%20An%C3%A1lisis%20de%20Datos%20en%20Tiempo%20Real%20con%20Microsoft%20Fabric/#resumen-de-lo-aprendido","title":"\ud83d\udd39Resumen de lo Aprendido","text":"<p>En este material hemos explorado en profundidad la anal\u00edtica en tiempo real y su aplicaci\u00f3n dentro del ecosistema de Microsoft Fabric. Los puntos clave incluyen:</p> <ul> <li>Definici\u00f3n e importancia:     La anal\u00edtica en tiempo real permite convertir datos en movimiento en insights inmediatos, fundamentales para la toma de decisiones en entornos din\u00e1micos.</li> <li>Modelos arquitect\u00f3nicos:     Se han comparado arquitecturas tradicionales basadas en procesamiento batch con arquitecturas en tiempo real (Lambda y Kappa), destacando las ventajas y desaf\u00edos de cada una.</li> <li>Componentes en Microsoft Fabric:<ul> <li>EventStream: Para la ingesta autom\u00e1tica y la indexaci\u00f3n de datos en streaming.</li> <li>EventHouse: Para la organizaci\u00f3n l\u00f3gica de los eventos.</li> <li>Real-Time Hub: Como interfaz central para gestionar y consumir datos en tiempo real, con mirroring a OneLake.</li> <li>Dashboards en Tiempo Real y Power BI: Que permiten visualizar y analizar datos al instante.</li> <li>KQL: Para realizar consultas r\u00e1pidas y complejas en los datos.</li> </ul> </li> <li>Casos de uso:     Se han presentado ejemplos concretos en industrias tan diversas como retail, finanzas, manufactura, telecomunicaciones y salud, ilustrando c\u00f3mo la anal\u00edtica en tiempo real aporta ventajas competitivas.</li> </ul>"},{"location":"04%20Analisis%20Avanzado%20y%20Automatizacion/04-2%20An%C3%A1lisis%20de%20Datos%20en%20Tiempo%20Real%20con%20Microsoft%20Fabric/#reflexiones-finales","title":"\ud83d\udd39Reflexiones Finales","text":"<p>\u2705 Fabric permite an\u00e1lisis en tiempo real con EventStream, EventHouse y Real-Time Dashboards. \u2705 La integraci\u00f3n con Power BI y Power Automate facilita la toma de decisiones en tiempo real. \u2705 El uso de KQL y SQL Analytics permite consultas r\u00e1pidas sobre eventos en movimiento.</p> <p>Laboratorio de Ejemplo: Announcing RTI End-to-End Sample | Microsoft Fabric Blog | Microsoft Fabric</p>"},{"location":"04%20Analisis%20Avanzado%20y%20Automatizacion/04-3%20Introducci%C3%B3n%20a%20Data%20Activator/","title":"04-03 Introducci\u00f3n a Data Activator","text":""},{"location":"04%20Analisis%20Avanzado%20y%20Automatizacion/04-3%20Introducci%C3%B3n%20a%20Data%20Activator/#1-automatizacion-de-procesos-basada-en-eventos-dentro-de-fabric","title":"1. Automatizaci\u00f3n de procesos basada en eventos dentro de Fabric","text":""},{"location":"04%20Analisis%20Avanzado%20y%20Automatizacion/04-3%20Introducci%C3%B3n%20a%20Data%20Activator/#que-es-data-activator","title":"\ud83d\udd39 \u00bfQu\u00e9 es Data Activator?","text":"<p>Data Activator en Microsoft Fabric es un servicio no-code que permite la automatizaci\u00f3n de procesos mediante la detecci\u00f3n de eventos en tiempo real. Su objetivo es proporcionar una respuesta r\u00e1pida a patrones de datos cambiantes, permitiendo la activaci\u00f3n autom\u00e1tica de acciones empresariales\u200b</p> <p>\ud83d\udccc Principales capacidades de Data Activator: \u2705 Monitorizaci\u00f3n de eventos en tiempo real. \u2705 Definici\u00f3n de reglas y condiciones sin c\u00f3digo. \u2705 Automatizaci\u00f3n de respuestas mediante integraci\u00f3n con otras herramientas.</p> <p>\ud83d\udd39 Ejemplo de uso: Una empresa de manufactura puede usar Data Activator para monitorear sensores IoT en una l\u00ednea de producci\u00f3n. Si la temperatura supera un umbral cr\u00edtico, se genera una alerta y se detiene autom\u00e1ticamente la m\u00e1quina afectada\u200b</p>"},{"location":"04%20Analisis%20Avanzado%20y%20Automatizacion/04-3%20Introducci%C3%B3n%20a%20Data%20Activator/#como-funciona-data-activator-en-fabric","title":"\ud83d\udd39 C\u00f3mo funciona Data Activator en Fabric","text":"<p>\ud83d\udccc Proceso de automatizaci\u00f3n basado en eventos: 1\ufe0f\u20e3 Detecci\u00f3n: Captura datos desde EventStream, Power BI o KQL Querysets. 2\ufe0f\u20e3 Definici\u00f3n de reglas: Se establecen condiciones para activar eventos. 3\ufe0f\u20e3 Acci\u00f3n: Se desencadenan respuestas autom\u00e1ticas (notificaciones, alertas, flujos de trabajo en Power Automate)\u200b</p> <p>\ud83d\udd39 Ejemplo: Si un KPI financiero en Power BI cae por debajo de un umbral, Data Activator puede enviar un correo a la direcci\u00f3n financiera para revisar el presupuesto\u200b</p>"},{"location":"04%20Analisis%20Avanzado%20y%20Automatizacion/04-3%20Introducci%C3%B3n%20a%20Data%20Activator/#2-integracion-con-power-automate-para-desencadenar-acciones","title":"2. Integraci\u00f3n con Power Automate para desencadenar acciones","text":""},{"location":"04%20Analisis%20Avanzado%20y%20Automatizacion/04-3%20Introducci%C3%B3n%20a%20Data%20Activator/#automatizacion-con-power-automate","title":"\ud83d\udd39 Automatizaci\u00f3n con Power Automate","text":"<p>Data Activator se puede integrar con Power Automate para ejecutar flujos de trabajo empresariales cuando se detectan ciertos eventos\u200b</p> <p>\ud83d\udccc Ejemplo de integraci\u00f3n: 1\ufe0f\u20e3 Data Activator detecta una anomal\u00eda en un conjunto de datos en Fabric. 2\ufe0f\u20e3 Se activa un flujo de trabajo en Power Automate. 3\ufe0f\u20e3 El flujo de trabajo env\u00eda una notificaci\u00f3n a Teams o crea una tarea en Microsoft Planner.</p> <p>\ud83d\udd39 Ejemplo de c\u00f3digo para integraci\u00f3n con Power Automate:</p> <p><code>{   \"trigger\": \"data_activator_event\",   \"action\": {     \"type\": \"send_notification\",     \"destination\": \"Microsoft Teams\",     \"message\": \"Se ha detectado una anomal\u00eda en el sistema financiero.\"   } }</code></p> <p>\ud83d\udccc Beneficio: Automatiza procesos cr\u00edticos sin intervenci\u00f3n manual\u200b</p>"},{"location":"04%20Analisis%20Avanzado%20y%20Automatizacion/04-3%20Introducci%C3%B3n%20a%20Data%20Activator/#3-ejemplo-practico-notificacion-automatica-ante-anomalias-en-los-datos","title":"3. Ejemplo pr\u00e1ctico: Notificaci\u00f3n autom\u00e1tica ante anomal\u00edas en los datos","text":"<p>\ud83d\udccc Objetivo: Configurar un sistema de detecci\u00f3n autom\u00e1tica de anomal\u00edas en un flujo de datos en tiempo real y generar notificaciones en Microsoft Teams cuando se detecten eventos fuera de lo normal</p>"},{"location":"04%20Analisis%20Avanzado%20y%20Automatizacion/04-3%20Introducci%C3%B3n%20a%20Data%20Activator/#pasos-del-ejercicio","title":"Pasos del ejercicio:","text":"<p>1\ufe0f\u20e3 Configurar un EventStream en Microsoft Fabric. 2\ufe0f\u20e3 Definir una condici\u00f3n de anomal\u00eda en Data Activator:</p> <ul> <li>Se establece una regla para detectar valores at\u00edpicos en una serie temporal de transacciones financieras.     3\ufe0f\u20e3 Integraci\u00f3n con Power Automate:</li> <li>Se crea un flujo que env\u00eda una alerta a Teams cuando Data Activator detecta una anomal\u00eda.</li> </ul> <p>\ud83d\udd39 Ejemplo de configuraci\u00f3n de Data Activator:</p> <p><code>SELECT * FROM transacciones_financieras WHERE monto &gt; 10000 AND cliente_riesgo = 'alto'</code></p> <p>\ud83d\udccc Beneficio: Permite una respuesta inmediata a fraudes o errores en transacciones sin necesidad de intervenci\u00f3n manual\u200b</p>"},{"location":"04%20Analisis%20Avanzado%20y%20Automatizacion/04-3%20Introducci%C3%B3n%20a%20Data%20Activator/#4-conclusion-y-preguntas-clave","title":"4. Conclusi\u00f3n y Preguntas Clave","text":"<p>\u2705 Data Activator permite la automatizaci\u00f3n de acciones basadas en eventos en tiempo real. \u2705 Su integraci\u00f3n con Power Automate facilita la respuesta r\u00e1pida a cambios en los datos. \u2705 Los negocios pueden reducir riesgos y mejorar la eficiencia operativa con reglas automatizadas.</p>"},{"location":"04%20Analisis%20Avanzado%20y%20Automatizacion/04-3%20Introducci%C3%B3n%20a%20Data%20Activator/#preguntas-para-reflexion-y-discusion","title":"Preguntas para reflexi\u00f3n y discusi\u00f3n:","text":"<p>1\ufe0f\u20e3 \u00bfCu\u00e1les son los beneficios de automatizar alertas en comparaci\u00f3n con monitoreo manual? 2\ufe0f\u20e3 \u00bfC\u00f3mo se pueden optimizar reglas en Data Activator para reducir falsas alarmas? 3\ufe0f\u20e3 \u00bfQu\u00e9 otros casos de uso podr\u00edan beneficiarse de Data Activator en su organizaci\u00f3n?</p>"},{"location":"05%20Fabric%20Databases/05-1%20Fabric%20Databases/","title":"Fabric Databases","text":""},{"location":"05%20Fabric%20Databases/05-1%20Fabric%20Databases/#1-introduccion","title":"1. Introducci\u00f3n","text":"<p>El aumento de la cantidad de datos disponibles y la creciente demanda de an\u00e1lisis en tiempo real han llevado a la integraci\u00f3n de m\u00faltiples servicios en una \u00fanica plataforma. Microsoft Fabric agrupa herramientas de ingenier\u00eda de datos, inteligencia en tiempo real, data warehousing, data science y business intelligence en un entorno SaaS unificado. En este contexto, la introducci\u00f3n de una base de datos transaccional en Fabric \u2013 denominada Fabric Databases \u2013 responde a las siguientes necesidades:</p> <ul> <li>Gesti\u00f3n de cargas operativas (OLTP): Permite almacenar y gestionar transacciones operativas en tiempo real, esenciales para aplicaciones empresariales que requieren integridad y coherencia en los datos.</li> <li>Integraci\u00f3n nativa con el ecosistema Fabric: Al estar basado en la tecnolog\u00eda de Azure SQL Database, se integra perfectamente con otros componentes como OneLake, Data Factory, Data Warehouse y herramientas de an\u00e1lisis.</li> <li>Simplificaci\u00f3n de la arquitectura: Con Fabric Databases se evita la dispersi\u00f3n de servicios de m\u00faltiples proveedores, centralizando la gesti\u00f3n de datos transaccionales en un solo entorno.</li> </ul>"},{"location":"05%20Fabric%20Databases/05-1%20Fabric%20Databases/#2-caracteristicas-de-fabric-databases","title":"2. Caracter\u00edsticas de Fabric Databases","text":""},{"location":"05%20Fabric%20Databases/05-1%20Fabric%20Databases/#arquitectura-y-tecnologia-subyacente","title":"\ud83d\udd39Arquitectura y Tecnolog\u00eda Subyacente","text":"<p>Fabric Databases se basa en el mismo SQL Database Engine que Azure SQL Database. Esta base de datos transaccional est\u00e1 dise\u00f1ada para ser:</p> <ul> <li>F\u00e1cil de configurar y administrar: Su implementaci\u00f3n en el entorno Fabric permite a los desarrolladores centrarse en la creaci\u00f3n de soluciones sin preocuparse por la infraestructura subyacente.</li> <li>Integrada con OneLake: Los datos operacionales se replican de forma continua en OneLake, lo que permite el an\u00e1lisis en tiempo real y la integraci\u00f3n con otros servicios anal\u00edticos de Fabric.</li> <li>Optimizada para OLTP: Est\u00e1 dise\u00f1ada para soportar un alto volumen de transacciones concurrentes, garantizando consistencia y baja latencia.</li> </ul>"},{"location":"05%20Fabric%20Databases/05-1%20Fabric%20Databases/#transaccionalidad-y-replicacion","title":"\ud83d\udd39Transaccionalidad y Replicaci\u00f3n","text":"<p>Las Fabric Databases est\u00e1n especialmente optimizadas para operaciones transaccionales. Entre sus ventajas se destacan:</p> <ul> <li>Soporte para transacciones ACID: Garantiza la atomicidad, consistencia, aislamiento y durabilidad en cada operaci\u00f3n.</li> <li>Replicaci\u00f3n en tiempo real: Mediante la tecnolog\u00eda de mirroring, los datos se replican casi en tiempo real hacia OneLake, lo que posibilita escenarios de an\u00e1lisis sin duplicar la carga en la base de datos transaccional. Por eso disponemos de un endpoint anal\u00edtico de SQL y un modelo sem\u00e1ntico en modo Direct Lake. </li> <li>Integraci\u00f3n con servicios de an\u00e1lisis: Gracias a la replicaci\u00f3n, los datos pueden ser consultados tanto mediante SQL como con herramientas de Apache Spark y Power BI, facilitando el cruce de informaci\u00f3n entre entornos operativos y anal\u00edticos.</li> </ul>"},{"location":"05%20Fabric%20Databases/05-1%20Fabric%20Databases/#integracion-y-ecosistema","title":"\ud83d\udd39Integraci\u00f3n y Ecosistema","text":"<p>Dentro de Microsoft Fabric, las Fabric Databases permiten:</p> <ul> <li>Conectividad nativa con otras cargas de trabajo: Como Data Engineering, Data Warehouse y Real-Time Intelligence.</li> <li>Uso compartido de datos: Los usuarios pueden compartir y acceder a la base de datos a trav\u00e9s de la funcionalidad de Graph API, que facilita la gesti\u00f3n y el descubrimiento de datos en el entorno Fabric.</li> <li>Soporte para operaciones avanzadas: Se incorpora soporte para operaciones vectoriales, fundamentales para soluciones de IA generativa y la implementaci\u00f3n del patr\u00f3n RAG (Retrieval-Augmented Generation).</li> </ul> <p>\ud83d\udccc Nota: La integraci\u00f3n con OneLake garantiza que siempre se trabaje con una \u00fanica copia de los datos, eliminando la duplicidad y facilitando la gobernanza.</p>"},{"location":"05%20Fabric%20Databases/05-1%20Fabric%20Databases/#3-guia-de-decision-cuando-utilizar-fabric-databases-o-azure-sql-database","title":"3. Gu\u00eda de Decisi\u00f3n: \u00bfCu\u00e1ndo Utilizar Fabric Databases o Azure SQL Database?","text":"<p>Elegir entre Fabric Databases y Azure SQL Database depende de varios factores que deben alinearse con las necesidades del negocio y la arquitectura del sistema. A continuaci\u00f3n, se presenta una gu\u00eda de decisi\u00f3n detallada:</p>"},{"location":"05%20Fabric%20Databases/05-1%20Fabric%20Databases/#factores-a-considerar","title":"\ud83d\udd39Factores a Considerar","text":"<ul> <li> <p>Integraci\u00f3n con Microsoft Fabric:</p> <ul> <li>Fabric Databases: Son ideales cuando se desea una integraci\u00f3n total con el ecosistema Fabric. Se beneficia de la replicaci\u00f3n autom\u00e1tica hacia OneLake, facilitando escenarios de an\u00e1lisis h\u00edbrido.</li> <li>Azure SQL Database: Puede ser la opci\u00f3n si la organizaci\u00f3n ya tiene inversiones previas en Azure y desea aprovechar funcionalidades avanzadas de PaaS sin necesidad de integrarse a un entorno unificado como Fabric.</li> <li> <p>Tipo de Carga de Trabajo:</p> </li> <li> <p>Transaccional (OLTP): Para operaciones de alta concurrencia, donde se requiere un control estricto de las transacciones, Fabric Databases ofrece un entorno optimizado.</p> </li> <li>Anal\u00edtico (OLAP): Si la prioridad es el an\u00e1lisis y la generaci\u00f3n de reportes en entornos de BI, es posible que se opt\u00e9 por un data warehouse o lakehouse, aprovechando la replicaci\u00f3n de datos desde la base operativa.</li> <li> <p>Costo y Escalabilidad:</p> </li> <li> <p>Fabric Databases: Al integrarse en el modelo SaaS de Fabric, se obtiene una administraci\u00f3n centralizada y escalabilidad autom\u00e1tica. Esto puede resultar en un menor costo total de propiedad al evitar la administraci\u00f3n de m\u00faltiples servicios.</p> </li> <li>Azure SQL Database: Puede ofrecer configuraciones personalizadas a nivel de recursos, pero requiere mayor administraci\u00f3n y puede ser m\u00e1s costosa en escenarios h\u00edbridos donde se necesite replicar y analizar datos.</li> <li> <p>Funcionalidades Espec\u00edficas:</p> </li> <li> <p>Soporte para operaciones vectoriales:</p> <ul> <li>Fabric Databases est\u00e1 dise\u00f1ado para soportar operaciones con vectores, lo que es fundamental para el desarrollo de soluciones de IA generativa y la implementaci\u00f3n del patr\u00f3n RAG.</li> </ul> </li> <li>Acceso y gesti\u00f3n mediante Graph API:<ul> <li>Esta caracter\u00edstica, integrada en Fabric Databases, facilita la administraci\u00f3n de la base de datos de forma program\u00e1tica y la integraci\u00f3n con otros servicios de Microsoft Fabric.</li> </ul> </li> </ul> </li> </ul> <p>Escenario 1: Una organizaci\u00f3n que desea consolidar sus operaciones transaccionales y anal\u00edticas en un \u00fanico entorno, aprovechando la replicaci\u00f3n autom\u00e1tica a OneLake y la integraci\u00f3n nativa con Power BI, deber\u00eda optar por Fabric Databases.</p> <p>Escenario 2: Una empresa con una infraestructura consolidada en Azure, que requiere funcionalidades espec\u00edficas de Azure SQL Database y no necesita la integraci\u00f3n total con el ecosistema Fabric, podr\u00eda seguir utilizando Azure SQL Database para sus cargas transaccionales.</p>"},{"location":"05%20Fabric%20Databases/05-1%20Fabric%20Databases/#decision-basada-en-escenarios","title":"\ud83d\udd39Decisi\u00f3n Basada en Escenarios","text":""},{"location":"05%20Fabric%20Databases/05-1%20Fabric%20Databases/#resumen-de-la-guia-de-decision","title":"\ud83d\udd39Resumen de la Gu\u00eda de Decisi\u00f3n","text":"<ul> <li> <p>Utilizar Fabric Databases cuando:</p> <ul> <li>Se requiera integraci\u00f3n completa con Microsoft Fabric.</li> <li>Se busque replicaci\u00f3n autom\u00e1tica de datos a OneLake para an\u00e1lisis en tiempo real.</li> <li>Se necesiten operaciones vectoriales para soluciones de IA generativa y patr\u00f3n RAG.</li> <li>Se desee una administraci\u00f3n simplificada en un entorno SaaS.</li> <li> <p>Utilizar Azure SQL Database cuando:</p> </li> <li> <p>La organizaci\u00f3n ya cuenta con una infraestructura consolidada en Azure.</p> </li> <li>Se requieren funcionalidades avanzadas o personalizadas no ofrecidas en Fabric.</li> <li>No se planea aprovechar la integraci\u00f3n nativa con otros servicios de Fabric.</li> </ul> </li> </ul> <p>\ud83d\udd0d Tip: Realiza una evaluaci\u00f3n detallada de las cargas de trabajo y los requisitos de integraci\u00f3n antes de tomar una decisi\u00f3n, considerando tanto aspectos t\u00e9cnicos como estrat\u00e9gicos.</p>"},{"location":"05%20Fabric%20Databases/05-1%20Fabric%20Databases/#4-seguridad-en-fabric-databases","title":"4. Seguridad en Fabric Databases","text":"<p>La seguridad es uno de los pilares fundamentales en el dise\u00f1o y operaci\u00f3n de Fabric Databases. La protecci\u00f3n de datos sensibles y la gobernanza de la informaci\u00f3n se logran a trav\u00e9s de m\u00faltiples capas y herramientas integradas.</p>"},{"location":"05%20Fabric%20Databases/05-1%20Fabric%20Databases/#autenticacion-y-autorizacion","title":"\ud83d\udd39Autenticaci\u00f3n y Autorizaci\u00f3n","text":"<ul> <li> <p>Microsoft Entra (Azure AD):     Todas las operaciones de acceso a Fabric Databases se realizan mediante la autenticaci\u00f3n de Microsoft Entra. Esto asegura que \u00fanicamente usuarios, grupos o aplicaciones autorizadas puedan interactuar con la base de datos.</p> </li> <li> <p>Control de Acceso Basado en Roles (RBAC):     Se definen roles espec\u00edficos para los diferentes perfiles de usuario (administradores, desarrolladores, analistas), lo que permite un control granular sobre qui\u00e9n puede leer, escribir o modificar datos.</p> </li> <li> <p>Pol\u00edticas de Seguridad:     Se implementan pol\u00edticas de encriptaci\u00f3n en reposo y en tr\u00e1nsito, garantizando que los datos est\u00e9n protegidos contra accesos no autorizados.</p> </li> </ul>"},{"location":"05%20Fabric%20Databases/05-1%20Fabric%20Databases/#acceso-a-traves-de-graph-api","title":"\ud83d\udd39Acceso a Trav\u00e9s de Graph API","text":"<p>El acceso program\u00e1tico a Fabric Databases se facilita mediante el uso de Graph API, lo que ofrece beneficios como:</p> <ul> <li> <p>Gesti\u00f3n Centralizada:     Permite a los administradores gestionar permisos, auditar accesos y realizar configuraciones de seguridad de forma centralizada.</p> </li> <li> <p>Integraci\u00f3n con Herramientas Externas:     Con Graph API, es posible integrar la gesti\u00f3n de la base de datos con otros sistemas empresariales o plataformas de monitoreo, automatizando procesos de seguridad y respuesta ante incidentes.</p> </li> <li> <p>Consultas y Operaciones Automatizadas:     Las operaciones de lectura, escritura y actualizaci\u00f3n pueden ser automatizadas, lo que agiliza procesos como la generaci\u00f3n de reportes o la actualizaci\u00f3n de pol\u00edticas de seguridad.</p> </li> </ul>"},{"location":"05%20Fabric%20Databases/05-1%20Fabric%20Databases/#medidas-adicionales-de-seguridad","title":"\ud83d\udd39Medidas Adicionales de Seguridad","text":"<ul> <li> <p>Auditor\u00eda y Monitoreo de Accesos:     Se implementan registros de auditor\u00eda y monitoreo continuo a trav\u00e9s de dashboards de rendimiento que permiten detectar patrones sospechosos y actuar proactivamente.</p> </li> <li> <p>Protecci\u00f3n de Datos Sensibles:     Funcionalidades como el enmascaramiento de datos din\u00e1mico, seguridad a nivel de filas (RLS) y objetos (OLS) aseguran que los usuarios solo puedan acceder a la informaci\u00f3n que les corresponde.</p> </li> </ul>"},{"location":"05%20Fabric%20Databases/05-1%20Fabric%20Databases/#5-monitorizacion-de-fabric-databases","title":"5. Monitorizaci\u00f3n de Fabric Databases","text":"<p>La monitorizaci\u00f3n es clave para asegurar el rendimiento y la estabilidad de las operaciones transaccionales. Fabric Databases integra diversas herramientas y m\u00e9tricas que facilitan el seguimiento del estado del sistema.</p>"},{"location":"05%20Fabric%20Databases/05-1%20Fabric%20Databases/#herramientas-de-monitorizacion-integradas","title":"\ud83d\udd39Herramientas de Monitorizaci\u00f3n Integradas","text":"<ul> <li> <p>Performance Dashboard:     Una herramienta nativa que muestra indicadores clave como el uso de CPU, la latencia de consultas y el volumen de transacciones. Esto permite identificar cuellos de botella y optimizar recursos.</p> </li> <li> <p>Dynamic Management Views (DMVs):     Las DMVs ofrecen una visi\u00f3n detallada de las operaciones de la base de datos, permitiendo a los administradores ejecutar consultas para detectar incidencias en el flujo de transacciones y cambios en el rendimiento.</p> </li> <li> <p>Alertas y Notificaciones:     Se pueden configurar alertas para recibir notificaciones autom\u00e1ticas en caso de que se superen ciertos umbrales, facilitando una respuesta r\u00e1pida ante problemas cr\u00edticos.</p> </li> </ul>"},{"location":"05%20Fabric%20Databases/05-1%20Fabric%20Databases/#integracion-con-power-bi-y-otras-herramientas","title":"\ud83d\udd39Integraci\u00f3n con Power BI y Otras Herramientas","text":"<ul> <li> <p>Dashboards Personalizados:     Utilizando Power BI, es posible crear dashboards que integren datos de monitorizaci\u00f3n, facilitando la visualizaci\u00f3n en tiempo real de la salud del sistema.</p> </li> <li> <p>Reportes Programados:     La generaci\u00f3n de reportes peri\u00f3dicos ayuda a mantener un historial de rendimiento, lo que permite analizar tendencias y planificar mejoras a largo plazo.</p> </li> </ul>"},{"location":"05%20Fabric%20Databases/05-1%20Fabric%20Databases/#6-conclusion","title":"6. Conclusi\u00f3n","text":"<p>La incorporaci\u00f3n de Fabric Databases en Microsoft Fabric representa un avance significativo en la forma en que las organizaciones gestionan sus operaciones transaccionales y anal\u00edticas. Al integrar capacidades de seguridad, monitorizaci\u00f3n, acceso program\u00e1tico mediante Graph API y soporte a operaciones vectoriales para soluciones de IA generativa, Fabric Databases no solo optimiza la eficiencia operativa, sino que tambi\u00e9n abre nuevas oportunidades para innovar en la inteligencia artificial y en la toma de decisiones basada en datos. La decisi\u00f3n entre utilizar Fabric Databases o Azure SQL Database depender\u00e1 de la infraestructura existente, la necesidad de integraci\u00f3n y los objetivos estrat\u00e9gicos de cada organizaci\u00f3n.</p>"},{"location":"05%20Fabric%20Databases/05-1%20Fabric%20Databases/#reflexiones-finales","title":"\ud83d\udd39Reflexiones Finales","text":"<p>\u2705 Fabric permite permite implementar el concepto de transalytics a trav\u00e9s de un motor SQL incluido en el producto. \u2705 DIsponemos de integraci\u00f3n directa con Onelake y por lo tanto con Direct Lake para la anal\u00edtica de los datos disponibles en esas base de datos. \u2705 Se nos habilitan nuevas opciones para el desarrollo de soluciones y aplicaciones basadas en datos</p>"},{"location":"05%20Fabric%20Databases/05-2%20Escenarios%20de%20Fabric%20Databases/","title":"Escenarios de uso de Fabric Databases","text":"<p>M\u00e1s all\u00e1 de las previsiones de las grandes consultoras, que dicen que para 2028, las plataformas de datos ser\u00e1n unificadas (transaccional y anal\u00edtica) y que ser\u00e1n gobernadas por la IA, esta posibilidad de disponer de base de datos SQL en Fabric, abre la puerta a diferentes escenarios, en ese entorno transalytics. </p>"},{"location":"05%20Fabric%20Databases/05-2%20Escenarios%20de%20Fabric%20Databases/#1-escenarios","title":"1. Escenarios","text":"<p>Entre los diferentes escenarios concretos que podr\u00edamos implementar en MIcrosoft Fabric, podr\u00edamos destacar:\u00a0</p> <ul> <li>Gestionar datos maestros para soluciones anal\u00edticas, de un modo completamente integrado y sencillo, en lugar de tener que cargar maestros en Excel o similares\u00a0 </li> <li>Disponibilizar soluciones de ETL-inverso, en las que posibilitamos la modificaci\u00f3n de datos anal\u00edticos y podemos concentrar esos datos en endpoints SQL.</li> <li>Desarrollo de aplicaciones departamentales, orientadas a procesos con un gran componente anal\u00edtico, y no muchos requerimientos</li> <li>Ahora que las bases de datos SQL incorporan soporte de vectores, podr\u00edamos utilizar una Fabric Database como almacen vectorial para implementar soluciones de IA Generativa que utilicen el patr\u00f3n RAG.</li> </ul> <p>Realmente, se trata de una integraci\u00f3n que puede abrir la puerta a otros muchos escenarios y simplificar la gesti\u00f3n de datos en compa\u00f1\u00edas de todos los tama\u00f1os.</p>"},{"location":"05%20Fabric%20Databases/05-2%20Escenarios%20de%20Fabric%20Databases/#2-soporte-a-vectores-para-soluciones-de-ia-generativa-y-patron-rag","title":"2. Soporte a Vectores para Soluciones de IA Generativa y Patr\u00f3n RAG","text":"<p>La creciente demanda de soluciones de inteligencia artificial generativa ha llevado a la integraci\u00f3n de capacidades vectoriales en Fabric Databases. Esto es crucial para escenarios en los que se requieren b\u00fasquedas sem\u00e1nticas y la combinaci\u00f3n de recuperaci\u00f3n de informaci\u00f3n con generaci\u00f3n de contenido.</p>"},{"location":"05%20Fabric%20Databases/05-2%20Escenarios%20de%20Fabric%20Databases/#que-son-las-operaciones-vectoriales","title":"Qu\u00e9 Son las Operaciones Vectoriales?","text":"<ul> <li> <p>Definici\u00f3n:     Las operaciones vectoriales permiten trabajar con representaciones num\u00e9ricas (vectores) que codifican informaci\u00f3n sem\u00e1ntica de textos, im\u00e1genes u otros datos. Estas representaciones son fundamentales para comparar similitudes, realizar b\u00fasquedas sem\u00e1nticas y alimentar modelos de IA.</p> </li> <li> <p>Aplicaciones en IA Generativa:     Permiten desarrollar aplicaciones que, por ejemplo, puedan responder preguntas de forma inteligente, generar res\u00famenes o realizar recomendaciones personalizadas utilizando modelos de lenguaje.</p> </li> </ul>"},{"location":"05%20Fabric%20Databases/05-2%20Escenarios%20de%20Fabric%20Databases/#implementacion-del-patron-rag-retrieval-augmented-generation","title":"Implementaci\u00f3n del Patr\u00f3n RAG (Retrieval-Augmented Generation)","text":"<p>El patr\u00f3n RAG combina t\u00e9cnicas de recuperaci\u00f3n de informaci\u00f3n con modelos generativos para mejorar la precisi\u00f3n y relevancia de las respuestas. Su implementaci\u00f3n en Fabric Databases se puede resumir en los siguientes pasos:</p> <ol> <li>Indexaci\u00f3n y Almacenamiento de Vectores:     Los datos textuales o de otros tipos se procesan para generar vectores que se almacenan junto con los registros transaccionales.    </li> <li>Consulta Sem\u00e1ntica:     Cuando se realiza una consulta, el sistema busca en el \u00edndice de vectores los registros que sean sem\u00e1nticamente similares al input del usuario.    </li> <li>Generaci\u00f3n de Respuestas:     Los resultados de la recuperaci\u00f3n se combinan con un modelo generativo (por ejemplo, basado en GPT) para producir una respuesta enriquecida, integrando tanto la informaci\u00f3n recuperada como nuevos elementos generados.</li> </ol>"},{"location":"05%20Fabric%20Databases/05-2%20Escenarios%20de%20Fabric%20Databases/#beneficios-para-soluciones-de-ia-generativa","title":"Beneficios para Soluciones de IA Generativa","text":"<ul> <li> <p>B\u00fasquedas Sem\u00e1nticas Mejoradas:     Al utilizar operaciones vectoriales, se pueden realizar b\u00fasquedas que van m\u00e1s all\u00e1 de la coincidencia exacta de palabras, permitiendo una mayor precisi\u00f3n en la recuperaci\u00f3n de datos relevantes.</p> </li> <li> <p>Integraci\u00f3n Fluida con Modelos de Lenguaje:     Fabric Databases, al soportar operaciones vectoriales, facilita la integraci\u00f3n con servicios de Azure OpenAI y otros modelos de IA, lo que permite desarrollar aplicaciones de IA generativa m\u00e1s robustas.</p> </li> <li> <p>Implementaci\u00f3n del Patr\u00f3n RAG:     Con la capacidad de combinar la recuperaci\u00f3n de informaci\u00f3n con la generaci\u00f3n de texto, las soluciones pueden ofrecer respuestas contextualizadas y de alta calidad, mejorando la experiencia del usuario.</p> </li> </ul>"},{"location":"05%20Fabric%20Databases/05-2%20Escenarios%20de%20Fabric%20Databases/#3caso-de-uso-implementacion-de-fabric-databases-en-un-entorno-real","title":"3.Caso de Uso: Implementaci\u00f3n de Fabric Databases en un Entorno Real","text":"<p>Para ilustrar c\u00f3mo se pueden utilizar las Fabric Databases, consideremos un caso de uso en el que una cadena de tiendas minoristas necesita gestionar sus operaciones transaccionales y, al mismo tiempo, disponer de an\u00e1lisis en tiempo real para tomar decisiones estrat\u00e9gicas.</p>"},{"location":"05%20Fabric%20Databases/05-2%20Escenarios%20de%20Fabric%20Databases/#descripcion-del-caso","title":"Descripci\u00f3n del Caso","text":"<p>Contexto: Una cadena de tiendas, denominada RetailX, desea optimizar la gesti\u00f3n de ventas, inventario y atenci\u00f3n al cliente. La organizaci\u00f3n necesita una base de datos transaccional robusta que soporte m\u00faltiples operaciones concurrentes \u2013 desde el procesamiento de ventas en caja hasta la actualizaci\u00f3n del inventario en tiempo real.</p> <p>Requerimientos principales:</p> <ul> <li>Alta Concurrencia:     Miles de transacciones simult\u00e1neas durante los picos de ventas.    </li> <li>Integraci\u00f3n Anal\u00edtica:     Replicaci\u00f3n autom\u00e1tica de datos para permitir an\u00e1lisis en tiempo real mediante Power BI y Apache Spark.    </li> <li>Seguridad y Gobernanza:     Control de acceso granular y protecci\u00f3n de datos sensibles (por ejemplo, informaci\u00f3n de clientes).    </li> <li>Soporte para IA:     Posibilidad de implementar soluciones de IA generativa, como chatbots o recomendaciones personalizadas, utilizando operaciones vectoriales y el patr\u00f3n RAG.</li> </ul>"},{"location":"05%20Fabric%20Databases/05-2%20Escenarios%20de%20Fabric%20Databases/#arquitectura-de-la-solucion","title":"Arquitectura de la Soluci\u00f3n","text":"<p>La soluci\u00f3n se compone de los siguientes elementos:</p> <ul> <li>Fabric Database Transaccional:     Se utiliza para gestionar todas las operaciones OLTP de RetailX. Las transacciones de venta, actualizaciones de inventario y registros de clientes se almacenan en esta base de datos.    </li> <li>Replicaci\u00f3n a OneLake:     Los datos se replican autom\u00e1ticamente a OneLake en formato Delta Parquet, permitiendo consultas anal\u00edticas en tiempo real sin afectar el rendimiento de la base operativa.    </li> <li>Integraci\u00f3n con Graph API:     Se habilita el acceso y la gesti\u00f3n program\u00e1tica de la base de datos mediante Graph API, facilitando la automatizaci\u00f3n de procesos y la integraci\u00f3n con otros sistemas empresariales.    </li> <li>Operaciones Vectoriales y Patr\u00f3n RAG:     Se implementa un m\u00f3dulo de IA que utiliza operaciones vectoriales para realizar b\u00fasquedas sem\u00e1nticas en el historial de transacciones y generar respuestas personalizadas a consultas de clientes. El patr\u00f3n RAG se utiliza para combinar resultados de recuperaci\u00f3n de informaci\u00f3n con generaci\u00f3n de texto, mejorando la precisi\u00f3n de las recomendaciones y respuestas autom\u00e1ticas.</li> </ul>"}]}