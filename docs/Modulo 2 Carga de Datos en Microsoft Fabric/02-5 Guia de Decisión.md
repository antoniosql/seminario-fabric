# **Guía de decisión entre Actividad de Copia de un Pipeline, Dataflow Gen2 y Apache Spark

|                                                  | **  <br>Actividad de copia de canalización**                                                            | **Flujo de datos Gen 2**                                                                                                      | Spark                                                                                                            |
| ------------------------------------------------ | ------------------------------------------------------------------------------------------------------- | ----------------------------------------------------------------------------------------------------------------------------- | ---------------------------------------------------------------------------------------------------------------- |
| Caso de uso                                      | Migración de Data Lake y almacenamiento de datos,  <br>ingesta de datos,  <br>transformación ligera     | Ingesta de datos,  <br>transformación de datos,  <br>limpieza y transformación de datos,  <br>generación de perfiles de datos | Ingesta de datos,  <br>transformación de datos,  <br>procesamiento de datos  <br>generación de perfiles de datos |
| rol de desarrollador principal                   | Ingeniero de datos,  <br>integrador de datos                                                            | Ingeniero de datos,  <br>integrador de datos,  <br>analista de negocios                                                       | Ingeniero de datos,  <br>científico de datos,  <br>desarrollador de datos                                        |
| Conjunto de aptitudes de desarrollador principal | ETL,  <br>SQL,  <br>JSON                                                                                | ETL,  <br>M,  <br>SQL                                                                                                         | Spark (Scala, Python, Spark SQL, R)                                                                              |
| Código escrito                                   | Sin código,  <br>código bajo                                                                            | Sin código,  <br>código bajo                                                                                                  | Código                                                                                                           |
| Volumen de datos                                 | Baja a alta                                                                                             | Baja a alta                                                                                                                   | Baja a alta                                                                                                      |
| Interfaz de desarrollo                           | Asistente,  <br>lienzo                                                                                  | Power Query                                                                                                                   | Cuaderno,  <br>Definición de trabajo de Spark                                                                    |
| Sources                                          | Más de 30 conectores                                                                                    | Más de 150 conectores                                                                                                         | Cientos de bibliotecas de Spark                                                                                  |
| Destinos                                         | +18 conectores                                                                                          | Lakehouse,  <br>-Azure SQL Database,  <br>Explorador de datos de Azure,  <br>Análisis de Azure Synapse                        | Cientos de bibliotecas de Spark                                                                                  |
| Complejidad de la transformación                 | Baja:  <br>ligero: conversión de tipos, mapeo de columnas, combinar/dividir archivos, aplanar jerarquía | Baja a alta:  <br>Más de 300 funciones de transformación                                                                      | Baja a alta:  <br>compatibilidad con Spark nativo y bibliotecas de código abierto                                |

Los siguiente escenarios están extraidos de la documentación oficial para entender en que caso es mejor utilizar cada una de las opciones:

## Escenario1

Leo, ingeniero de datos, debe ingerir un gran volumen de datos de sistemas externos, tanto locales como en la nube. Estos sistemas externos incluyen bases de datos, sistemas de archivos y API. Leo no quiere escribir y mantener código para cada conector o operación de movimiento de datos. Quiere seguir las prácticas recomendadas para los niveles de medallón (bronce, plata y oro). Leo no tiene ninguna experiencia con Spark, por lo que prefiere la interfaz de usuario de arrastrar y colocar tanto como sea posible, con una codificación mínima. Y también quiere procesar los datos según una programación.

El primer paso es obtener los datos sin procesar en el lago de datos del nivel bronce de los recursos de datos de Azure y varios orígenes de terceros (como Snowflake Web, REST, AWS S3, GCS, etc.). Quiere un lago de datos consolidado, de modo que todos los datos procedentes de distintos LOB, locales y orígenes en la nube residan en un único lugar. Leo revisa las opciones y selecciona la **actividad de copia de canalización** como opción adecuada para su copia binaria sin procesar. Este patrón se aplica tanto a la actualización de datos históricos como incrementales. Con la actividad de copia, Leo puede cargar datos Oro en un almacenamiento de datos sin código si surge la necesidad y las canalizaciones proporcionan una ingesta de datos a gran escala que puede mover datos a escala de petabyte. La actividad de copia es la mejor opción de código bajo y sin código para mover petabytes de datos a lago de datos y almacenamientos de variedades de orígenes, ya sea ad hoc o a través de una programación.
## Escenario2

Mary es ingeniero de datos con un profundo conocimiento de los múltiples requisitos de informes analíticos de LOB. Un equipo ascendente ha implementado correctamente una solución para migrar varios datos históricos e incrementales de LOB a un lago de datos común. Mary se ha encargado de limpiar los datos, aplicar lógicas de negocios y cargarlos en varios destinos (como Azure SQL DB, ADX y lakehouse) como preparación para sus respectivos equipos de informes.

Mary es un usuario experimentado de Power Query y el volumen de datos está en el rango bajo a medio para lograr el rendimiento deseado. Los flujos de datos proporcionan interfaces sin código o poco código para ingerir datos de cientos de orígenes de datos. Con los flujos de datos, puede transformar datos mediante más de 300 opciones de transformación de datos y escribir los resultados en varios destinos con una interfaz de usuario muy visual fácil de usar. Mary revisa las opciones y decide que tiene sentido usar **Dataflow Gen 2** como opción de transformación preferida.

## Escenario3

Adam es un ingeniero de datos que trabaja para una gran empresa minorista que usa una casa de lago para almacenar y analizar sus datos de clientes. Como parte de su trabajo, Adam es responsable de crear y mantener las canalizaciones de datos que extraen, transforman y cargan datos en el lago. Uno de los requisitos empresariales de la empresa es realizar análisis de revisión de clientes para obtener información sobre las experiencias de sus clientes y mejorar sus servicios.

Adam decide la mejor opción es usar **spark** para crear la lógica de extracción y transformación. Spark proporciona una plataforma informática distribuida que puede procesar grandes cantidades de datos en paralelo. Escribe una aplicación spark con Python o Scala, que lee datos estructurados, semiestructurados y no estructurados de OneLake para opiniones y comentarios de los clientes. La aplicación limpia, transforma y escribe datos en las tablas Delta del lago de datos. Los datos están listos para ser utilizados en análisis posteriores.