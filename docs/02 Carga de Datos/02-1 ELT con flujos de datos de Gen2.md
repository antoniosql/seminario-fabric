# **ELT con flujos de Generaci√≥n 2**
## **1. Creaci√≥n y configuraci√≥n de Dataflows Gen2**

### üîπ **¬øQu√© es Dataflow Gen2?**

Dataflows Gen2 en Microsoft Fabric es una soluci√≥n **low-code** para la ingesta y transformaci√≥n de datos, dise√±ada para simplificar el proceso ETL. Est√° basado en **Power Query**, lo que permite a los usuarios conectar, transformar y cargar datos desde diversas fuentes sin necesidad de c√≥digo. Los **Flujos de Datos de Gen2** son la evoluci√≥n de los Dataflows de Power BI, con mejoras significativas en rendimiento, compatibilidad con **OneLake** y escalabilidad a nivel empresarial.

üìå **Principales Beneficios de Dataflows Gen2:**

‚úÖ **Procesamiento en la nube:** No requieren infraestructura local.  
‚úÖ **Integraci√≥n con Power Query:** Se usa la misma interfaz de Power BI, pero m√°s potente.  
‚úÖ **Escalabilidad:** Permite procesar **grandes vol√∫menes de datos** en Fabric.  
‚úÖ **Compatibilidad con m√∫ltiples fuentes de datos.**  
‚úÖ **Automatizaci√≥n de cargas con Pipelines de Data Factory.**

### üîπ **Componentes clave de Dataflows Gen2**

- **Power Query**: El motor de transformaci√≥n de datos utilizado en Dataflows Gen2 es el mismo que en Power BI y Excel. Esto permite a los usuarios aprovechar su experiencia con Power Query para construir flujos de datos complejos sin necesidad de escribir c√≥digo. Power Query ofrece m√°s de 300 transformaciones de datos basadas en inteligencia artificial
- **Conectores**: Dataflows Gen2 ofrece una amplia gama de conectores a diversas fuentes de datos, incluyendo bases de datos SQL, archivos CSV, Excel, servicios en la nube, APIs y m√°s. Esto permite la extracci√≥n de datos de m√∫ltiples sistemas y la integraci√≥n en una √∫nica plataforma.
- **Destinos**: Dataflows Gen2 puede cargar datos transformados en varios destinos, incluyendo Azure Data Lake Storage Gen2, Lakehouses en Microsoft Fabric, y otros destinos como tablas de bases de datos
- **Orquestaci√≥n**: Aunque Dataflows Gen2 se centra en la transformaci√≥n de datos, tambi√©n puede integrarse con Data Pipelines de Microsoft Fabric para la orquestaci√≥n de flujos de datos. Esto permite construir procesos ETL complejos con gesti√≥n de dependencias y disparadores.
- **Staging**: Dataflows Gen2 utiliza un √°rea de almacenamiento temporal o "staging" para guardar los resultados intermedios de las transformaciones. Esto mejora el rendimiento del proceso, ya que las transformaciones se aplican en la nube y el resultado se carga en el destino final. .

### üîπ **Pasos para crear un Dataflow Gen2**

A continuaci√≥n, explicamos c√≥mo crear y configurar un **Flujo de Datos de Gen2** paso a paso.

#### **Acceder a Dataflows Gen2**

1. En **Microsoft Fabric**, ir al **espacio de trabajo** donde se desea crear el flujo.
2. Hacer clic en **Nuevo ‚Üí Flujo de Datos Gen2**.
3. Se abrir√° la interfaz de **Power Query**, donde se configuran las fuentes y transformaciones.

#### **Conectar una Fuente de Datos**

Los Flujos de Datos de Gen2 pueden conectarse a m√°s de **300 fuentes de datos**, como:  
‚úî **Azure SQL Database**  
‚úî **OneLake (Almacenamiento central en Fabric)**  
‚úî **Archivos CSV en Azure Blob Storage**  
‚úî **Google BigQuery, Amazon S3, SAP HANA, APIs REST**

üìå **Ejemplo Pr√°ctico:**  
Si queremos extraer datos de ventas desde una base de datos SQL en Azure:

`SELECT id_transaccion, cliente_id, monto, fecha_venta  FROM ventas WHERE fecha_venta >= '2024-01-01'`

üí° **Optimizaci√≥n:** Si la base de datos soporta **particionamiento**, aplicar **filtros en la consulta** para reducir el volumen de datos extra√≠dos.

#### **Aplicar Transformaciones en Power Query**

Una vez importados los datos, podemos realizar transformaciones como:  
‚úî **Eliminar duplicados**  
‚úî **Filtrar datos innecesarios**  
‚úî **Unir datos de m√∫ltiples fuentes**  
‚úî **Convertir formatos de fecha, n√∫mero y texto**  
‚úî **Crear columnas calculadas**

üìå **Ejemplo en Power Query:**

`= Table.AddColumn(#"Datos Importados", "Margen", each [Precio] - [Costo], type number)`

#### **Definir el Destino de Datos**

El **resultado del Dataflow Gen2** puede almacenarse en:  
‚úî **OneLake**  
‚úî **Azure SQL Database**  
‚úî **Un modelo sem√°ntico en Power BI**

Para definir el destino:

4. Seleccionar **OneLake** para almacenar los datos transformados.
5. Configurar **una actualizaci√≥n programada** para la carga de datos.


---

## **2. Conectores disponibles en Fabric y su uso en escenarios reales**

### üîπ **Tipos de conectores en Dataflows Gen2**

Los Dataflows Gen2 ofrecen una gran variedad de conectores para la ingesta de datos. Entre los m√°s utilizados est√°n‚Äã:

|**Tipo de Conector**|**Ejemplos de Fuentes**|**Casos de Uso**|
|---|---|---|
|**Bases de Datos**|SQL Server, PostgreSQL, SAP HANA|Extracci√≥n de datos transaccionales para an√°lisis.|
|**Servicios Cloud**|Azure Data Lake, AWS S3, Google BigQuery|Integraci√≥n de datos multicloud.|
|**Archivos**|Excel, CSV, JSON, Parquet|Transformaci√≥n y limpieza de datos no estructurados.|
|**SaaS y APIs**|Salesforce, Dynamics 365, SharePoint|Conexi√≥n con sistemas empresariales para reporting.|

### üîπ **Ejemplo de escenario real**

Una empresa de e-commerce necesita **consolidar datos de ventas desde SQL Server, Google Analytics y un ERP en Dynamics 365**.

1Ô∏è‚É£ **Fuente:** SQL Server, Google Analytics y Dynamics 365.  
2Ô∏è‚É£ **Proceso:** Se utiliza un **Dataflow Gen2** para transformar y consolidar la informaci√≥n.  
3Ô∏è‚É£ **Destino:** Los datos limpios se almacenan en **un Data Warehouse en Fabric** para reportes en Power BI.

üìå **Beneficio:** Automatizaci√≥n del proceso ETL y reducci√≥n del tiempo de consolidaci√≥n de datos en un **70%** ‚Äã.

---

## **3. Transformaciones avanzadas y optimizaci√≥n del rendimiento**

Para mejorar la calidad y estructura de los datos, se pueden aplicar:

‚úî **Agrupaciones y agregaciones**  
‚úî **Expresiones condicionales (IF, CASE)**  
‚úî **Tablas pivote y no pivote**  
‚úî **Normalizaci√≥n y desnormalizaci√≥n de datos**

üìå **Ejemplo:**  
Si queremos calcular el total de ventas por cliente en Power Query:

`= Table.Group(#"Datos Importados", {"cliente_id"}, {{"TotalVentas", each List.Sum([monto]), type number}})`

### üîπ**Optimizaci√≥n del Rendimiento**

Para mejorar el rendimiento en **Dataflows Gen2**, se recomienda:  
üìå **Reducir la cantidad de datos extra√≠dos** aplicando filtros desde la fuente.  
üìå **Usar formatos como Parquet o Delta Lake** en OneLake.  
üìå **Particionar grandes vol√∫menes de datos** para mejorar tiempos de procesamiento.


### üîπ **Transformaciones avanzadas en Power Query**

Power Query en Dataflows Gen2 permite realizar transformaciones avanzadas sin necesidad de programaci√≥n:

‚úÖ **Filtrado y limpieza de datos** ‚Üí Eliminaci√≥n de duplicados y correcci√≥n de valores nulos.  
‚úÖ **Creaci√≥n de columnas calculadas** ‚Üí Uso de expresiones en **M Query** para c√°lculos personalizados.  
‚úÖ **Uniones y combinaciones de datos** ‚Üí Integraci√≥n de m√∫ltiples fuentes en un solo dataset.  
‚úÖ **Pivot y unpivot** ‚Üí Transformaci√≥n de datos en distintos formatos para an√°lisis‚Äã.

### üîπ **Optimizaci√≥n del rendimiento en Dataflows Gen2**

üîπ **Fast Copy** ‚Üí Reduce el tiempo de ingesta de datos en un **50%** gracias a una optimizaci√≥n en la transferencia de datos‚Äã.  
üîπ **Incremental Refresh** ‚Üí Permite actualizar solo los datos nuevos en lugar de recargar todo el dataset‚Äã.  
üîπ **Staging optimizado** ‚Üí Los Dataflows Gen2 crean artefactos de staging en OneLake para acelerar el procesamiento‚Äã.  
üîπ **Column Binding en SAP HANA** ‚Üí Mejora la eficiencia en la consulta de datos en sistemas SAP‚Äã.

üìå **Ejemplo pr√°ctico:**  
Un equipo de analistas financieros necesita actualizar diariamente datos de ventas **sin recargar todo el dataset**. Se configura **Incremental Refresh**, logrando una reducci√≥n del 80% en el tiempo de carga‚Äã.

Estas opciones que veremos ahora en detalle, se configuran consulta por consulta:
![[Dataflow config consulta.png]]
#### **Refrescos Incrementales**

El **Refresco Incremental** permite que los Dataflows **solo carguen datos nuevos o modificados**, en lugar de recargar toda la fuente de datos.

üìå **Ejemplo Pr√°ctico:**  
Si tenemos una tabla de transacciones de ventas con m√°s de **100 millones de registros**, un Dataflow con Refresco Completo **recargar√° toda la tabla en cada ejecuci√≥n**, lo que es ineficiente.  
Con **Refresco Incremental**, solo se importar√°n las **ventas nuevas del √∫ltimo mes**.

Para activar el **Refresco Incremental en Dataflows Gen2**:

1. Seleccionar la **columna de fecha** que servir√° de referencia para el incremental.
2. Configurar el **rango de retenci√≥n** (ejemplo: solo los √∫ltimos 6 meses).
3. Publicar el Dataflow y activar el **modo incremental**.

Existe alguna limitaci√≥n, la m√°s importante relacionada con los destinos como podemos apreciar en esta imagen, porque no todos los destinos soportan actualizaciones incrementales. 
![[Dataflow Gen2 Incremental.png]]
Si queremos tener un destino lakehouse, y cargas incrementales, estas no podremos configurarlas en el Dataflow, sino que tendremos que cargar los datos en crudo, y posteriomente refinarlos en el lakehouse. Si el destino est√° soportado entonces tendremos que configurar la carga incremental, de este modo:
![[Dataflow Carga Incremental.png]]

- Primero configurados una columna por la que filtrar los datos. Esta configuraci√≥n es necesaria y especifica la columna que usan los flujos de datos para filtrar los datos. Esta columna debe ser una columna DateTime, Date o DateTimeZone. El flujo de datos usa esta columna para filtrar los datos y solo recupera los datos que han cambiado desde la √∫ltima actualizaci√≥n.
- Extraer datos del pasado. Esta configuraci√≥n es necesaria y especifica el retroceso en el tiempo que el flujo de datos debe extraer datos. Esta configuraci√≥n se usa para recuperar la carga de datos inicial. El flujo de datos recupera todos los datos del sistema de origen que se encuentran dentro del intervalo de tiempo especificado.
- Tama√±o del cubo. Esta configuraci√≥n es necesaria y especifica el tama√±o de los cubos que usa el flujo de datos para filtrar los datos. El flujo de datos divide los datos en cubos en funci√≥n de la columna DateTime. Cada cubo contiene los datos que cambiaron desde la √∫ltima actualizaci√≥n. El tama√±o del cubo determina la cantidad de datos que se procesan en cada iteraci√≥n. Un tama√±o de cubo m√°s peque√±o significa que el flujo de datos procesa menos datos en cada iteraci√≥n, pero tambi√©n significa que se requieren m√°s iteraciones para procesar todos los datos. Un tama√±o de cubo mayor significa que el flujo de datos procesa m√°s datos en cada iteraci√≥n, pero tambi√©n significa que se requieren menos iteraciones para procesar todos los datos.
- Valor m√°ximo de columna. Esta configuraci√≥n es necesaria y especifica la columna que usa el flujo de datos para determinar si los datos han cambiado. El flujo de datos compara el valor m√°ximo de esta columna con el valor m√°ximo de la actualizaci√≥n anterior. Si se cambia el valor m√°ximo, el flujo de datos recupera los datos que han cambiado desde la √∫ltima actualizaci√≥n. Si no se cambia el valor m√°ximo, el flujo de datos no recupera ning√∫n dato.
- Periodos Finalizados. Si seleccionamos esta opci√≥n se leer√°n √∫nicamente datos de periodos finalizados. 

Limitaciones:
- Solo se admiten destinos basados en SQL
- El n√∫mero m√°ximo de cubos es de 50 para una tabla y 150 para el dataflow completo
- Solo se admite el m√©todo replace en el destino

#### **Fast Copy**

**Fast Copy** es una funcionalidad que acelera la transferencia de datos en Fabric al utilizar **procesamiento en paralelo y compresi√≥n eficiente**.

üìå **Beneficios de Fast Copy:**  
‚úî **Hasta 10 veces m√°s r√°pido** que una copia normal.  
‚úî **Ideal para migraciones y cargas masivas de datos.**  
‚úî **Compatible con Dataflows Gen2 y Pipelines de Data Factory.**

Para habilitar **Fast Copy** en un Dataflow Gen2:

1. Seleccionar la fuente y destino de datos.
2. Activar la opci√≥n **"Usar Fast Copy"** en la configuraci√≥n del Dataflow.
3. Guardar y ejecutar el Dataflow.

Requisitos:
- Conectores y Transformaciones soportados
- Para ficheros solo est√°n soportados CSV y parquet almacenados en Azure Blob o ADLS de m√°s de 100Mb de tama√±o
- Para or√≠genes SQL tablas con m√°s de 5 millones de filas

---

## **4. Hands-On: Construcci√≥n de un Dataflow Gen2 desde cero**

### üîπ **Objetivo del ejercicio**

Construir un **Dataflow Gen2** que:

- Ingesta datos de una base de datos **SQL Server** en Azure en  **OneLake**.
- Realiza transformaciones de limpieza y c√°lculo.
- Almacena el resultado en un **Lakehouse en Fabric** para su uso en Power BI.

Los datos de conexi√≥n a la base de datos son:
- Servidor:  srvvernedev.database.windows.net 
- BBDD: SalesLT
- Usuario: pruebas
- Contrase√±a: @@Secret0
### üîπ **Pasos del ejercicio**

1Ô∏è‚É£ **Crear un Dataflow Gen2** en Microsoft Fabric.  
2Ô∏è‚É£ **A√±adir conectores** para SQL Server con los datos proporcionados

Agregar las tablas:
- SalesLT.Product
- SalesLT.ProductCategory
- SalesLT.Customer
- SalesLT.SalesOrderDEtail
- SalesLT.SalesOrderHeader

Aseg√∫rate de mostrar todas las opciones de perfilado desde las opciones:

![[Dataflow Perfiles de Datos.png]]
3Ô∏è‚É£ **Aplicar transformaciones** en Power Query:

- En la tabla SalesLT.Product
	- quitar las columnas "ThumbnailPhotoFileName", "ThumbNailPhoto"
	- Reemplazar los nulos de la columna Size por "NA"
	- Expande la tabla SalesLT.ProductCategoy para llevar el campo Name a la tabla de SalesLT.Product
- Renombra la tabla SalesLT.Product como Products
- Desmarca la opci√≥n de habilitar el almacenamiento temporal en la tablas en SAlesLT.ProductCategory
- En la tabla SalesLT.Customer
	- Quita las columnas "Suffix", "NameStyle", "MiddleName", "PasswordHash", "PasswordSalt"
	- Renombra la tabla como Customers
- En la tabla SalesLT.OrderDetail 
	- Expande la tablas SalesLT.SalesOrderHeader para traer los campos de detalle, trae todos los campos
	- Renombre la tabla como Sales
- Desmarca la opci√≥n de habilitar el almacenamiento temporal en las tablas en SalesLTSalesOrderHeader



    4Ô∏è‚É£ **Definir el destino en OneLake o Lakehouse**.  
    5Ô∏è‚É£ **Ejecutar el Dataflow y validar resultados**.

Abre el Modelo Sem√°ntico creado e intenta generar un informe sobre esos datos

üìå **Resultado esperado:** Los datos procesados estar√°n disponibles para an√°lisis en **Power BI** y otras herramientas dentro de Fabric‚Äã.

---

## **5. Conclusi√≥n y Preguntas Clave**

‚úÖ **Los Dataflows Gen2 permiten ingestar, transformar y automatizar procesos ETL en Fabric.**  
‚úÖ **El uso de conectores nativos facilita la integraci√≥n con diversas fuentes de datos.**  
‚úÖ **Las transformaciones avanzadas en Power Query mejoran la calidad y preparaci√≥n de los datos.**  
‚úÖ **Las optimizaciones como Fast Copy e Incremental Refresh mejoran el rendimiento de los flujos de datos.**

### **Preguntas de reflexi√≥n**

1Ô∏è‚É£ ¬øCu√°ndo es mejor usar Dataflows Gen2 en lugar de Pipelines en Data Factory?  
2Ô∏è‚É£ ¬øC√≥mo aprovechar Incremental Refresh para mejorar tiempos de procesamiento?  
3Ô∏è‚É£ ¬øC√≥mo elegir el mejor destino (OneLake, Lakehouse o Data Warehouse) seg√∫n el caso de uso?


