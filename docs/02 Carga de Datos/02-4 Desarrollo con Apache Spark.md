# **Desarrollo con Apache Spark**

**Apache Spark** es un motor de procesamiento distribuido de **c√≥digo abierto** dise√±ado para **procesamiento de datos a gran escala**. Se ejecuta en clusters y permite el an√°lisis de datos en **tiempo real y batch**, proporcionando APIs en **Python (PySpark), Scala, Java, R y SQL**.

‚úî **Alta velocidad** gracias a su ejecuci√≥n en memoria.  
‚úî **Escalabilidad** para procesar desde **megabytes hasta petabytes** de datos.  
‚úî **Compatibilidad con m√∫ltiples fuentes de datos**, como **OneLake, Data Warehouses, Data Lakes y bases de datos SQL**.

En **Microsoft Fabric**, Spark es una pieza clave del **Data Engineering** y se integra con otros servicios como **Data Factory, OneLake, Power BI y Data Science**.

--- 

## **1. ¬øC√≥mo se integra Apache Spark en Microsoft Fabric?

En **Microsoft Fabric**, Spark se ejecuta en un entorno **completamente administrado**, eliminando la necesidad de configurar clusters manualmente.

### **üîπ Componentes Clave**

‚úî **Clusters de Spark administrados**: Se asignan din√°micamente recursos seg√∫n la carga de trabajo.  
‚úî **Pools de Spark**: Grupos de recursos donde se ejecutan notebooks y trabajos de Spark. 
‚úî **Notebooks de Fabric**: Espacios interactivos para escribir c√≥digo en **PySpark, Scala y SQL**.  
‚úî **OneLake**: Almacenamiento unificado para datos procesados por Spark.  
‚úî **Integraci√≥n con Data Factory**: Automatizaci√≥n de cargas y ejecuci√≥n de pipelines.

üìå **Ejemplo:**  
Si una empresa quiere procesar **grandes vol√∫menes de datos de ventas**, Spark puede leer los datos desde **OneLake**, aplicar transformaciones con PySpark y guardar el resultado en un **Data Warehouse** para su an√°lisis en Power BI.

---

## **2. Conceptos Clave de Spark Compute**

### üîπ**Compute Engines**

Microsoft Fabric permite ejecutar Spark en un entorno administrado donde los **clusters** se crean y gestionan autom√°ticamente. Esto permite a los usuarios centrarse en el desarrollo sin preocuparse por la infraestructura subyacente.

### üîπ**Tipos de Jobs en Fabric Spark**

- **Batch Jobs:** Se utilizan para procesamiento de datos en grandes vol√∫menes.    
- **Streaming Jobs:** Permiten el procesamiento en tiempo real de datos en movimiento.    
- **Notebooks y Pipelines:** Usados para desarrollo interactivo y modelado de datos.    

---

## **3. Configuraci√≥n y Gesti√≥n de Pools de Spark**

### üîπ**Pools Predeterminados (Starter Pools)**

Los **Starter Pools** en Microsoft Fabric son grupos preconfigurados de recursos de Spark que permiten ejecutar trabajos de manera r√°pida sin necesidad de configuraci√≥n avanzada.

**Ventajas:**

- No requieren configuraci√≥n manual.    
- Son ideales para entornos de desarrollo y pruebas.    
- Ofrecen un rendimiento √≥ptimo para cargas de trabajo moderadas.    

### üîπ**Creaci√≥n de Pools Personalizados**

Para cargas de trabajo m√°s complejas, es posible **crear pools de Spark personalizados** con configuraciones espec√≠ficas.

Pasos para la configuraci√≥n:

1. Definir el tama√±o del pool de nodos.    
2. Seleccionar el tipo de instancias de Spark (RAM/CPU).    
3. Configurar l√≠mites de concurrencia y asignaci√≥n de memoria.    
4. Asociar el pool a una capacidad de Fabric.    

![Configuraci√≥n de Pools Spark](<imagenes/Configuracion Pools Spark.png>)

### üîπ**Facturaci√≥n y Administraci√≥n de Capacidad**

Microsoft Fabric usa un modelo de **cobro por uso**, donde los recursos de Spark consumen **unidades de capacidad (CU)**. Esta es la tabla con los valores predeterminados que tienen los pools de Spark dependiendo de la SKU. 

![Pools Spark por defecto SKU](<imagenes/Pools Spark por defecto SKU.png>)

- **Estrategias de optimizaci√≥n de costes:**    
    - Utilizar pools compartidos para trabajos recurrentes.        
    - Configurar l√≠mites de uso en trabajos Spark.        
    - Monitorear el consumo con herramientas de Fabric.        


Para un pool starter, estas son las tareas que se facturan y las que no. En el caso de un pool personalizado, simplemente desaparece el estado "Pool Idle". 

![Facturaci√≥n Pools Sparl](<imagenes/Facturacion Pools Spark.png>)

---

## **4. Gesti√≥n de Concurrencia y Encolado de Trabajos**

### üîπ**Modo de Alta Concurrencia**

El **modo de alta concurrencia** en Fabric permite ejecutar m√∫ltiples trabajos Spark simult√°neamente dentro de un mismo pool de recursos.

**Beneficios:**

- Mayor eficiencia en la ejecuci√≥n de tareas paralelas.    
- Menos sobrecarga en la creaci√≥n de clusters.    
- Uso m√°s eficiente de los recursos disponibles.    

### üîπ**Administraci√≥n de Admisi√≥n de Trabajos**

Fabric permite configurar pol√≠ticas de **admisi√≥n de trabajos** para controlar la cantidad de tareas en ejecuci√≥n simult√°nea y evitar sobrecargas.

**Opciones disponibles:**

- Asignar prioridades a diferentes tipos de trabajos.    
- Limitar el n√∫mero de trabajos simult√°neos.    
- Definir reglas de asignaci√≥n de recursos.    

### üîπ**Encolado de Trabajos**

Cuando los recursos est√°n ocupados, los nuevos trabajos ingresan a una **cola de espera** hasta que haya capacidad disponible.

**Buenas pr√°cticas:**

- Priorizar trabajos cr√≠ticos con etiquetas de urgencia.    
- Definir l√≠mites de tiempo de espera para evitar bloqueos.    
- Monitorear el estado de la cola para ajustes din√°micos.    

---

## **5. Optimizaci√≥n y Autoajuste en Spark**

### üîπ**T√©cnicas de Optimizaci√≥n**

Para mejorar el rendimiento de trabajos Spark en Fabric, se pueden aplicar las siguientes estrategias:

- **Particionamiento de Datos:** Para paralelizar el procesamiento.    
- **Broadcast Join:** Para evitar movimientos de datos innecesarios.    
- **Uso de Cache:** Para almacenar datos frecuentemente utilizados.    
- **Compresi√≥n de Datos:** Para reducir el tiempo de procesamiento.    

### üîπ**Uso de AutoTune**

Fabric ofrece **AutoTune**, una funcionalidad que ajusta autom√°ticamente los par√°metros de ejecuci√≥n de Spark para maximizar la eficiencia.

**¬øC√≥mo funciona?**

1. Analiza patrones de ejecuci√≥n de trabajos anteriores.    
2. Ajusta din√°micamente par√°metros como el n√∫mero de ejecutores y la asignaci√≥n de memoria.    
3. Optimiza la distribuci√≥n de tareas dentro del cluster.    

**Ventajas:**

- Reducci√≥n del tiempo de ejecuci√≥n.    
- Menos intervenci√≥n manual en la configuraci√≥n de Spark.    
- Uso m√°s eficiente de los recursos disponibles.

---

## **6. Uso de PySpark para la ingenier√≠a de datos**

### üîπ **¬øQu√© es PySpark y por qu√© usarlo en Fabric?**

PySpark es la implementaci√≥n de **Apache Spark en Python**, y es una herramienta clave para procesar grandes vol√∫menes de datos en paralelo dentro de Fabric.

üìå **Beneficios de PySpark en Fabric:**  
‚úÖ **Escalabilidad:** Ejecuta procesos de datos en cl√∫steres distribuidos.  
‚úÖ **Compatibilidad con Delta Lake:** Soporta almacenamiento en **OneLake** con formato optimizado.  
‚úÖ **Interoperabilidad con SQL y Power BI:** Permite consultas h√≠bridas con T-SQL‚Äã.

### üîπ **Ejemplo de c√≥digo PySpark en un Notebook de Fabric**


`from pyspark.sql import SparkSession  
# Crear sesi√≥n de Spark 
spark = SparkSession.builder.appName("FabricPySparkDemo").getOrCreate()  
# Cargar datos desde OneLake 
df = spark.read.format("delta").load("onelake://mi_empresa/datasets/ventas")  
# Transformaciones en PySpark 
df_filtrado = df.filter(df["ingresos"] > 10000)  # Mostrar resultados df_filtrado.show()`

üìå **Explicaci√≥n del c√≥digo:**  
1Ô∏è‚É£ Se inicia una sesi√≥n de **Spark en Fabric**.  
2Ô∏è‚É£ Se cargan datos en formato **Delta Lake desde OneLake**.  
3Ô∏è‚É£ Se aplican transformaciones con **PySpark (filtros y manipulaci√≥n de datos)**.


--- 

## **7. Introducci√≥n a los Notebooks en Microsoft Fabric**

### üîπ **¬øQu√© es un Notebook en Microsoft Fabric?**

Un **Notebook en Microsoft Fabric** es un entorno interactivo que permite escribir y ejecutar c√≥digo en m√∫ltiples lenguajes (**Python, Scala, SQL y .NET**). Est√° dise√±ado para **ingenieros de datos, cient√≠ficos de datos y analistas**, facilitando la exploraci√≥n y transformaci√≥n de datos‚Äã.

### üîπ **Caracter√≠sticas clave de los Notebooks en Fabric**

‚úÖ **Soporte multi-lenguaje:** Python (PySpark), Scala, SQL y .NET.  
‚úÖ **Integraci√≥n con OneLake:** Accede y manipula datos almacenados en Fabric.  
‚úÖ **Ejecuta Spark Jobs en paralelo:** Alta capacidad de c√≥mputo distribuido.  
‚úÖ **Versionado con Git:** Soporte para control de versiones e integraci√≥n con CI/CD.  
‚úÖ **Orquestaci√≥n con Pipelines:** Los notebooks pueden ejecutarse como parte de un flujo de datos‚Äã.

üìå **Ejemplo de uso:** Un equipo de an√°lisis de datos puede usar un Notebook en Fabric para limpiar y transformar datos financieros antes de almacenarlos en un Data Warehouse.


---

## **8. Optimizaci√≥n del rendimiento en Spark y mejores pr√°cticas**

### üîπ **Estrategias para mejorar el rendimiento de Apache Spark en Fabric**

üìå **Optimizaci√≥n del rendimiento con Spark en Fabric:**

‚úÖ **Uso de formato Delta Lake** ‚Üí Acelera la lectura/escritura y permite transacciones ACID‚Äã.  
‚úÖ **Paralelismo en tareas Spark** ‚Üí Ajustar el n√∫mero de particiones para evitar cuellos de botella.  
‚úÖ **Uso de cach√© en Spark** ‚Üí Permite mejorar tiempos de respuesta en datasets reutilizados.  
‚úÖ **Aplicaci√≥n de V-Order en Parquet** ‚Üí Optimiza el rendimiento de consultas SQL sobre datos en Fabric.

üìå **Ejemplo de optimizaci√≥n con cach√© en PySpark**

`df.cache()  # Mantiene el DataFrame en memoria para acelerar consultas df.count()  # Ejecuta una acci√≥n para materializar el cache`

üîπ **Estrategia de particionamiento recomendada:**

`df.repartition(10)  # Distribuir datos en 10 particiones para balanceo de carga`

‚úÖ **Monitorizaci√≥n de Spark Jobs en Fabric:** Fabric proporciona herramientas de mointorizaci√≥n para visualizar el rendimiento de cada tarea de Spark, permitiendo identificar cuellos de botella‚Äã.


---

## **9.Carga Incremental con SCD2 con PySpark**

En los escenarios de BI comunes donde disponemos de un Data Warehouses en SQL Server On-Premises o en la nube, nos encontr√°bamos con la caracter√≠stica IDENTITY que se asignaba a las claves subrogadas de las dimensiones. Ahora, con las tablas **Delta** nos encontramos con la **problem√°tica** de que las **IDENTITY desaparecen**, por lo que el versionado de las SCD2 tiene que ser redise√±ado.

Para ello, proponemos una **soluci√≥n** para emular el atributo IDENTITY almacenando en una variable el √∫ltimo valor de la clave subrogada cargada, para despu√©s con c√≥digo escrito en Spark SQL y la funci√≥n de ventana ROW\_NUMBER(), gestionar y mantener los √≠ndices autoincrem√©ntales en los elementos que entren nuevos o hayan cambiado de versi√≥n.

1) Partiendo de la siguiente dimensi√≥n que presenta el nombre ‚Äútabla\_delta‚Äù:

![tabla_delta](https://www.vernegroup.com/wp-content/uploads/2025/02/Imagen1.png "tabla_delta")

Con estas primeras l√≠neas de c√≥digo importamos las funciones del m√≥dulo pyspark.sql.functions, almacenamos la *pk* en la variable *key\_column*, guardamos los campos en una lista, la reorganizamos para dejar *fromDate* al final y convertimos esa lista en un *string* separado por comas.

![string](https://www.vernegroup.com/wp-content/uploads/2025/02/Captura-de-pantalla-2025-02-12-170322-1.png "string")

2) Suponiendo que en la variable *sdf* tenemos almacenado el *DataFrame* con los datos que han entrado en la √∫ltima ejecuci√≥n:

![datos-dataframe](https://www.vernegroup.com/wp-content/uploads/2025/02/Imagen2.png "datos-dataframe")

Con el siguiente c√≥digo le a√±adimos el campo *toDate* y *isCurrent*, adem√°s de a√±adir un hash con la funci√≥n *xxhash64*. Este hash puede estar compuesto por todos los elementos de la dimensi√≥n o √∫nicamente por los que nos interesen para versionar.

![hash](https://www.vernegroup.com/wp-content/uploads/2025/02/Captura-de-pantalla-2025-02-12-171012.png "hash")

1) Despu√©s, almacenamos en la variable *next\_surrogate\_key* el valor de la √∫ltima *sk* cargada en la tabla, y guardamos en una vista temporal el *DataFrame* *sdf*.

![vista-temporal-dataframe](https://www.vernegroup.com/wp-content/uploads/2025/02/Captura-de-pantalla-2025-02-12-171322.png "vista-temporal-dataframe")

2) Por √∫ltimo, a√±adimos las l√≠neas de Spark SQL encargadas de finalizar las versiones actuales de la dimensi√≥n y de insertar las nuevas.

![lineas-spark-sql](https://www.vernegroup.com/wp-content/uploads/2025/02/Captura-de-pantalla-2025-02-12-171643-1.png "lineas-spark-sql")

Tras ejecutar todo el c√≥digo el resultado de la dimensi√≥n ser√≠a el siguiente:

![codigo-resultado-dimensi√≥n](https://www.vernegroup.com/wp-content/uploads/2025/02/Imagen3-1.png "codigo-resultado-dimensi√≥n")

En el cual disponemos de **dos versiones** para los **id 2 y 6** y la **nueva inserci√≥n** para el **id 7**, cada una de ellas **con** **su** **propia clave subrogada** (*sk*).

---

## **10. Conclusi√≥n y Preguntas Clave**

‚úÖ **Los Notebooks en Microsoft Fabric son herramientas clave para la ingenier√≠a de datos con Apache Spark.**  
‚úÖ **PySpark permite la manipulaci√≥n y transformaci√≥n de datos a gran escala de manera eficiente.**  
‚úÖ **Optimizar el rendimiento con Delta Lake y t√©cnicas de caching mejora la eficiencia del procesamiento.**  
‚úÖ **Fabric facilita la integraci√≥n de Spark con almacenamiento en OneLake y an√°lisis en Power BI.**

### **Preguntas para reflexi√≥n y discusi√≥n**

1Ô∏è‚É£ ¬øC√≥mo elegir entre SQL, PySpark y .NET dentro de un Notebook en Fabric?  
2Ô∏è‚É£ ¬øCu√°ndo se debe usar **cache()** y **repartition()** en Spark para optimizar rendimiento?  
3Ô∏è‚É£ ¬øC√≥mo se pueden monitorear y depurar errores en Spark dentro de Microsoft Fabric?