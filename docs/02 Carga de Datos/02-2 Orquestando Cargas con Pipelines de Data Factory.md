# **Orquestando Cargas con Pipelines de Data Factory

## **1. Introducci√≥n a los Pipelines en Microsoft Fabric**

La **orquestaci√≥n de datos** es un proceso fundamental en cualquier ecosistema de an√°lisis de datos, permitiendo **automatizar, programar y gestionar flujos de trabajo ETL** (Extract, Transform, Load). Microsoft Fabric, a trav√©s de **Data Factory y sus Pipelines**, proporciona una soluci√≥n escalable y flexible para la **carga, transformaci√≥n y movimiento de datos en la nube**.

### **üìå ¬øPor qu√© es importante la Orquestaci√≥n en Fabric?**

‚úî **Automatizaci√≥n del movimiento de datos** entre diversas fuentes y destinos.  
‚úî **Gesti√≥n de dependencias** entre actividades de carga y transformaci√≥n.  
‚úî **Monitorizaci√≥n y control de ejecuci√≥n** con opciones de reintento y alertas.  
‚úî **Integraci√≥n con otros servicios** como Power BI, Dataflows Gen2 y OneLake.

### üîπ **¬øQu√© es un Pipeline en Microsoft Fabric?**

Un **Pipeline en Data Factory** dentro de Microsoft Fabric es un flujo de trabajo **automatizado** que permite la ingesta, transformaci√≥n y movimiento de datos a trav√©s de m√∫ltiples fuentes y destinos.

Un Espacio de Trabajo de Microsoft Fabric  puede tener una o m√°s canalizaciones. Una canalizaci√≥n es una agrupaci√≥n l√≥gica de actividades que juntas realizan una tarea. Por ejemplo, una canalizaci√≥n podr√≠a contener un conjunto de actividades que ingieren y limpian datos de registro y luego inician un flujo de datos para analizar los datos de registro. La canalizaci√≥n nos permite gestionar las actividades como un conjunto en lugar de cada una individualmente. 

**Caracter√≠sticas clave de los Pipelines en Fabric:**  
‚úÖ **Orquestaci√≥n de procesos ETL/ELT** con l√≥gica condicional y ejecuci√≥n programada.  
‚úÖ **Integraci√≥n con m√∫ltiples fuentes** como **Azure SQL, OneLake, AWS S3, Google Cloud** y m√°s.  
‚úÖ **Automatizaci√≥n de flujos de datos** entre Dataflows Gen2, Lakehouses y Data Warehouses.  
‚úÖ **Escalabilidad y paralelizaci√≥n**, permitiendo el procesamiento eficiente de grandes vol√∫menes de datos‚Äã.
### **üìå Principales caracter√≠sticas de los Pipelines en Fabric**

‚úî **Definici√≥n visual o basada en c√≥digo:** Se pueden construir desde una interfaz gr√°fica o con JSON.  
‚úî **Activaci√≥n basada en eventos:** Los pipelines pueden ejecutarse seg√∫n reglas de negocio o disparadores temporales.  
‚úî **Escalabilidad:** Soporta cargas de trabajo que van desde megabytes hasta petabytes de datos.  
‚úî **Integraci√≥n con m√∫ltiples fuentes:** SQL, OneLake, Azure Data Lake, APIs, entre otros.

üìå **Ejemplo de caso de uso:**  
Imagina que una empresa necesita **extraer datos de ventas desde un sistema SAP, transformarlos y almacenarlos en un Data Warehouse en Fabric**. Un **Pipeline de Data Factory** puede:  

1Ô∏è‚É£ **Ejecutar un proceso de extracci√≥n** desde SAP.  
2Ô∏è‚É£ **Transformar los datos con un Notebook en Spark**.  
3Ô∏è‚É£ **Cargar los datos en OneLake** para su an√°lisis en Power BI.
### üîπ **Principales Componentes de un Pipeline**

Un pipeline de Data Factory en Microsoft Fabric se compone de:

1Ô∏è‚É£ **Actividades:** Representan pasos dentro del pipeline, como copia de datos, transformaci√≥n o ejecuci√≥n de scripts.  
2Ô∏è‚É£ **Conjuntos de Datos:** Conexiones a fuentes y destinos de datos.  
3Ô∏è‚É£ **Flujos de Control:** Reglas de ejecuci√≥n como condicionales y bucles.  
4Ô∏è‚É£ **Eventos y Programadores:** Permiten ejecutar el pipeline en funci√≥n de triggers espec√≠ficos‚Äã.

#### **Actividades** 

Las actividades en una canalizaci√≥n definen acciones a realizar con los datos. Por ejemplo, podemos usar una actividad de copia para copiar datos de SQL Server a Azure Blob Storage. Luego, usaremos una actividad de Dataflow o una actividad de Notebook para procesar y transformar datos del almacenamiento de blobs a un grupo de Azure Synapse Analytics sobre el cual se crean soluciones de informes de inteligencia empresarial.

Microsoft Fabric tiene tres tipos de actividades: actividades de **movimiento de datos**, actividades de **transformaci√≥n de datos** y actividades de **control**.

##### **Actividades de movimiento de datos**
Realmente solo tenemos una actividad que es la actividad de copia de datos. Para copiar datos de un origen a un destino, el servicio que ejecuta la actividad de copia realiza estos pasos:

1. Lee datos desde un almac√©n de datos de origen.
2. Realiza procesos de serializaci√≥n y deserializaci√≥n, compresi√≥n y descompresi√≥n, asignaci√≥n de columnas, etc. Realiza estas operaciones en funci√≥n de la configuraci√≥n.
3. Escribe datos en el almac√©n de datos de destino.

##### **Actividades de transformaci√≥n**
Son las actividades que nos permiten realizar transformaciones sobre los datos. Son las siguientes:

| Actividad de transformaci√≥n de datos | Entorno de procesos                                          |
| ------------------------------------ | ------------------------------------------------------------ |
| Copia de datos                       | Administrador de proceso de Microsoft Fabric                 |
| Dataflow Gen2                        | Administrador de proceso de Microsoft Fabric                 |
| Eliminaci√≥n de datos                 | Administrador de proceso de Microsoft Fabric                 |
| Fabric Notebook                      | Cl√∫steres de Apache Spark administrados por Microsoft Fabric |
| Actividad de HDInsight               | Cl√∫steres de Apache Spark administrados por Microsoft Fabric |
| Definici√≥n de trabajos de Spark      | Cl√∫steres de Apache Spark administrados por Microsoft Fabric |
| Procedimiento almacenado             | Azure SQL, Azure Synapse Analytics o SQL Server              |
| Script de SQL                        | Azure SQL, Azure Synapse Analytics o SQL Server              |
##### **Actividades de Flujo de Control**

Son las actividades que nos permiten modificar el flujo de ejecuci√≥n de nuestro pipeline, tales como Bucles, actividades, condicionales, notificaciones, etc.. 
### üîπ **Ejemplo de un pipeline en Fabric**

Un pipeline t√≠pico puede extraer datos desde **un ERP en SQL Server**, transformarlos con un **Notebook en Spark**, y cargarlos en **un Data Warehouse en Fabric** para su consumo en Power BI‚Äã.


---

## **2. Actividad de Copia en Detalle**

### üîπ**Comportamiento de la actividad de copia**
En la actividad de copia de datos, podemos elegir entre los siguientes modos de carga de datos.
- **Modo de copia completa**: cada trabajo de copia que se ejecuta copia todos los datos del origen al destino a la vez.
- **Modo de copia incremental**: la ejecuci√≥n del trabajo inicial copia todos los datos y el trabajo posterior solo copia los cambios desde la √∫ltima ejecuci√≥n. Los datos modificados se anexan al almac√©n de destino. En el modo de copia incremental, deberemos de seleccionar una columna incremental para cada tabla para identificar los cambios. El trabajo de copia usa esta columna como marca de agua, comparando su valor con el mismo desde la √∫ltima ejecuci√≥n para copiar solo los datos nuevos o actualizados. La columna incremental debe ser una marca de tiempo o un INT incremental.

Tambi√©n podemos elegir c√≥mo se escriben los datos en el almac√©n de destino.

De manera predeterminada, el trabajo de Copia¬†**anexa**¬†datos al destino, de modo que no se pierda ning√∫n historial de cambios. Pero tambi√©n podemos ajustar el comportamiento de escritura a¬†**upsert**¬†o¬†**sobrescribir**.

- Cuando se copian datos en el almac√©n de almacenamiento: las nuevas filas de las tablas o archivos se copian en archivos nuevos en el destino. Si ya existe un archivo con el mismo nombre en el almac√©n de destino, se sobrescribir√°.
- Al copiar datos en una base de datos: las nuevas filas de las tablas o archivos se anexan a las tablas de destino. Podemos cambiar el comportamiento de escritura a upsert (en SQL DB o SQL Server) o sobrescribir (en tablas de Fabric Lakehouse).

### üîπ **Escalabilidad y Rendimiento de la Actividad de Copia**

Las canalizaciones de Data Factory ofrecen una arquitectura sin servidor que permite el paralelismo en distintos niveles. Esta arquitectura hace posible el desarrollo de canalizaciones que maximizan el rendimiento del movimiento de datos para nuestro entorno. Estas canalizaciones hacen un uso completo de los siguientes recursos:

- Ancho de banda de red entre los almacenes de datos de origen y destino
- Operaciones de entrada/salida por segundo (IOPS) y ancho de banda del almac√©n de datos de origen o destino

La copia puede escalarse en diferentes niveles:

- El¬†**flujo de control**¬†puede iniciar varias actividades de copia en paralelo, por ejemplo, mediante un¬†bucle ForEach.
- Una sola actividad de copia puede aprovechar los¬†**recursos de proceso escalables**.
    - Puede especificar la optimizaci√≥n de rendimiento inteligente como m√°ximo para cada actividad de copia y sin servidor.
- Una √∫nica actividad de copia lee y escribe en el almac√©n de datos¬†**mediante varios subprocesos**¬†en paralelo.

![Actividad Copia Configuraci√≥n](<imagenes/Actividad Copia Configuracion.png>)

#### **Copia Paralela**

Podemos establecer el valor de la propiedad "Grado de paralelismo de copia" en la configuraci√≥n de la actividad.  Esta propiedad se define como el n√∫mero m√°ximo de subprocesos dentro de la actividad de copia. Los subprocesos operan en paralelo. Estos subprocesos leen desde el origen o escriben en los almacenes de datos de destino.

La copia paralela es independiente de la configuraci√≥n de optimizaci√≥n de rendimiento inteligente. Para cada ejecuci√≥n de la actividad de copia, el servicio aplica din√°micamente la configuraci√≥n de copia paralela √≥ptima en funci√≥n del patr√≥n de datos y el par de destino de origen.

Para controlar la carga en las m√°quinas que hospedan los almacenes de datos o para optimizar el rendimiento de la copia, podemos modificar el valor predeterminado y especificar un valor para el grado de paralelismo. El valor debe ser un entero mayor o igual que 1. En tiempo de ejecuci√≥n, y para obtener el mejor rendimiento, la actividad de copia usa un valor inferior o igual al valor que ha establecido.

#### **Optimizaci√≥n Inteligente del rendimiento**

La optimizaci√≥n del rendimiento inteligente permite al servicio optimizar el rendimiento mediante la combinaci√≥n de los factores de asignaci√≥n de recursos de CPU, memoria y red y el coste esperado de ejecutar una √∫nica actividad de copia. Las opciones permitidas para habilitar una ejecuci√≥n de actividad de copia de forma inteligente son¬†**Autom√°tico, Est√°ndar, Equilibrado, M√°ximo**. Tambi√©n podemos especificar el valor¬†**entre 4 y 256**.

En la tabla siguiente se muestra el valor recomendado en diferentes escenarios de copia:

Expandir tabla

|Valor|Descripci√≥n|
|---|---|
|**Autom√°tico**|Permitir que el servicio aplique din√°micamente la optimizaci√≥n √≥ptima del rendimiento en funci√≥n del patr√≥n de datos y el par de destino de origen.|
|**Est√°ndar**|Permitir que el servicio aplique din√°micamente la optimizaci√≥n del rendimiento en los recursos de proceso est√°ndar en funci√≥n del patr√≥n de datos y el par de destino de origen.|
|**Equilibrada**|Permitir que el servicio aplique din√°micamente la optimizaci√≥n del rendimiento que equilibra el rendimiento y los recursos de proceso disponibles en funci√≥n del patr√≥n de datos y el par de destino de origen.|
|**M√°xima**|Permitir que el servicio aplique din√°micamente la optimizaci√≥n del rendimiento mediante el uso de los recursos de proceso m√°ximos disponibles en funci√≥n del patr√≥n de datos y el par de destino de origen.|

#### **Optimizaci√≥n al leer del origen**
La actividad de copia tambi√©n tiene configuraciones de optimizaci√≥n en la lectura de datos, que dependen de los or√≠genes de datos que utilicemos. Si nos fijamos en el caso concreto, por ejemplo de un origen SQL Server, vemos las siguientes opciones:

![PArtici√≥n lectura origen SQL pipeline](<imagenes/Partici√≥n lectura origen SQL pipeline.png>)

Al habilitar la copia con particiones, la actividad de copia ejecuta consultas en paralelo en el origen de SQL¬†Server para cargar los datos por particiones. El grado en paralelo se controla mediante el valor¬†`parallelCopies`¬†de la actividad de copia. Por ejemplo, si establecemos¬†`parallelCopies`¬†en cuatro, el servicio genera y ejecuta al mismo tiempo cuatro consultas de acuerdo con la configuraci√≥n y la opci√≥n de partici√≥n que hemos especificado, y cada consulta recupera una porci√≥n de datos de SQL¬†Server.

### üîπ **Carga Incremental**

La actividad de copia, a diferencia del dataflow, no est√° orientada a aplicar transformaciones, por lo que no tenemos una configuraci√≥n concreta para la carga incremental. Sin embargo, esto depende de los tipo de origen de datos que manejemos. Por ejemplo, en caso de los ficheros tenemos esta configuraci√≥n:

![Filtro modificaci√≥n ficheros pipelines](<imagenes/Filtro modificaci√≥n ficheros pipelines.png>)

Esta opci√≥n comprueba la fecha de modificaci√≥n de los ficheros, y si se encuentra en el intervalo especificado, los lee, y sino, los descarta. 

---


## **3. Automatizaci√≥n de cargas ELT y administraci√≥n de dependencias**

### üîπ **Automatizaci√≥n de cargas en Fabric Data Factory**

Los pipelines permiten configurar **flujos de trabajo completamente automatizados**, asegurando que los datos sean procesados sin intervenci√≥n manual.

üìå **M√©todos de automatizaci√≥n en Data Factory:**  
‚úÖ **Programaci√≥n basada en tiempo:** Ejecuci√≥n de pipelines a intervalos definidos.  
‚úÖ **Eventos y Triggers:** Disparo autom√°tico cuando nuevos datos est√°n disponibles.  
‚úÖ **Condicionales y Flujos de decisi√≥n:** Dependencias entre tareas dentro del pipeline.

En el caso concreto de los triggers y evento, estos se disparan creando una alerta sobre un eventstream de un Azure Blob Storage. Esta es la √∫nica configuraci√≥n soportada por el momento. 

![Triggers Pipelines](<imagenes/Triggers Pipelines.png>)

### üîπ **Administraci√≥n de dependencias en pipelines**

En escenarios donde las tareas deben ejecutarse en orden, es necesario definir **dependencias** entre actividades.

üìå **Tipos de dependencias en Data Factory:**

- **Encadenamiento de actividades:** Una actividad solo inicia si la anterior se completa con √©xito.
- **Ejecuci√≥n paralela:** Varias actividades pueden ejecutarse al mismo tiempo para optimizar rendimiento.
- **Reintentos autom√°ticos:** Si una tarea falla, se pueden configurar intentos adicionales‚Äã

Las dependencias entre las actividades del pipeline, se configuran a la hora de conectar las actividades.

![Dependencias de Actividades pipeline](<imagenes/Dependencias de Actividades pipeline.png>)

- Flecha gris, para decidir que actividad ejecutar cuando se omita la ejecuci√≥n de la actividad actual
- Verde, para cuando se ejecute correctamente
- Roja para cuando falle
- Azul, siempre que finalice

üîπ **Ejemplo de un flujo ELT automatizado**  

1Ô∏è‚É£ Un **Pipeline en Data Factory** extrae datos de una API y almacena en OneLake.  
2Ô∏è‚É£ Se activa un **Notebook en Spark** para transformar los datos.  
3Ô∏è‚É£ Se ejecuta una consulta SQL para cargar datos en un **Data Warehouse en Fabric**.  
4Ô∏è‚É£ **Power BI actualiza autom√°ticamente los dashboards** cuando los datos est√°n listos‚Äã


---

## **4. Monitorizaci√≥n de procesos de carga y troubleshooting**

### üîπ **Herramientas de monitorizaci√≥n en Fabric Data Factory**

Microsoft Fabric incluye herramientas avanzadas para monitorear la ejecuci√≥n de los pipelines y resolver errores r√°pidamente.

üìå **Opciones de monitorizaci√≥n disponibles:**  
‚úÖ **Fabric Monitor:** Proporciona logs en tiempo real y estad√≠sticas de ejecuci√≥n.  
‚úÖ **Registro de errores en ejecuci√≥n:** Muestra detalles de cada actividad y su estado.  
‚úÖ **Alertas y notificaciones:** Configuraci√≥n de alertas en caso de fallos‚Äãfabric-fundamentals.


### üîπ **Uso de Copilot para Troubleshooting**

Fabric Data Factory incluye **Copilot** (a partir de F64), una herramienta de IA que ayuda a identificar y solucionar problemas en pipelines‚Äã.

üìå **Ejemplo de troubleshooting con Copilot:**  
1Ô∏è‚É£ Se detecta una falla en la actividad de copia de datos.  
2Ô∏è‚É£ Copilot analiza los logs y proporciona una descripci√≥n del error.  
3Ô∏è‚É£ Se sugiere una soluci√≥n, como **ajustar el formato del archivo fuente o modificar permisos en la base de datos**‚Äãfabric-fundamentals.

üîπ **Ejemplo real de monitorizaci√≥n**  
Una empresa usa Fabric para integrar datos de distintos departamentos. Un pipeline programado diariamente **falla intermitentemente** debido a cambios en la estructura de los archivos de origen. **Fabric Monitor** y **Copilot** identifican el problema y sugieren una transformaci√≥n autom√°tica para corregirlo‚Äã.

---

## **5. Hands-On: Orquestando procesos**

### üîπ **Objetivo del ejercicio**

En este laboratorio, el objetivo es entender como orquestar realmente las ingestas de datos desde "el exterior" a Microsoft Fabric, generando las transformacions con Dataflows de Generaci√≥n 2, y orquestando el proceso con la actividad de Dataflows de las Pipelines. Utilizaremos el Dataflow generado en el ejercicio anterior

### üîπ **Pasos del ejercicio**

1.  Crea un elemento de tipo **Data pipeline**. 
2. En la opci√≥n de Comenzar con un lienzo en blanco, selecciona, Actividad de canalizaci√≥n y selecciona **Flujo de Datos**. 
3. En la pesta√±a de configuraci√≥n, selecciona el Dataflow generado en el ejercicio anterior
4. Agrega una actividad de **Teams** y con√©ctala cuando finalice correctamente la actividad de Flujo de Datos. 
5. Desde la pesta√±a de Configuraci√≥n inicia sesi√≥n en Teams
6. En Post-In selecciona donde quieres publicar. Tenemos dos opciones: Canal o Chat de Grupo
7. Comp√≥n un mensaje 
8. Ejecuta el pipeline. Deber√≠a de ejecutarse correctamente y llegarte el mensaje a Teams

![Mensaje en Teams de pipeline](<imagenes/Mensaje Teams pipeline.png>)

---

## **6. Conclusi√≥n y Preguntas Clave**

‚úÖ **Los Pipelines en Data Factory permiten automatizar flujos ETL y orquestar cargas de datos.**  
‚úÖ **Las dependencias entre actividades aseguran que las cargas de datos se ejecuten de forma ordenada.**  
‚úÖ **Las herramientas de monitoreo y troubleshooting facilitan la detecci√≥n y correcci√≥n de errores.**  
‚úÖ **El uso de Copilot permite optimizar el mantenimiento y resoluci√≥n de problemas en pipelines.**

### **Preguntas de reflexi√≥n**

9. ¬øCu√°les son las ventajas de usar triggers en vez de programaciones manuales en un pipeline?
10. ¬øC√≥mo se pueden optimizar los pipelines para manejar grandes vol√∫menes de datos?
11. ¬øC√≥mo se puede aprovechar Copilot para mejorar la depuraci√≥n de errores en pipelines?